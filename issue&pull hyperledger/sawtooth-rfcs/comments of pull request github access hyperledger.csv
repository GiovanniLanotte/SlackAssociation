pull request url html,pull request url,body,commit_id,create at,id,diff_hunk,position,update at,url,html url,user
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,Can we extend existing API and allow partial address?,ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-02-21 16:32:13,259009849,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input",10,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/259009849,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r259009849,arsulegai
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"We could.
I thought in order to prevent TP developers from deleting prefix by mistake, the right approach will be to add a new API for this instead of adding capabilities to existing delete API.
Do you think it will be better to allow delete with prefix by extending existing API?",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-11 09:34:36,264148474,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input",10,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/264148474,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r264148474,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"Yes, this can however be controlled by outputs field in TransactionHeader.",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-11 09:36:23,264149141,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input",10,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/264149141,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r264149141,arsulegai
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,form -> from,ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-12 18:04:56,264813995,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that ",,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/264813995,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r264813995,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,contextmanager -> context_manager,ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-12 18:08:38,264815615,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that ",,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/264815615,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r264815615,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"thank you, fixed",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-13 07:29:32,265000604,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that ",,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/265000604,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r265000604,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"I'm not really sure if it it's necessary (or desirable) to return the list of addresses. It has the same performance problem of the previously suggested RFC for getting all the set addresses out of state.  These addresses would need to be read under that prefix, and then the prefix can be deleted. 

Deleting the prefix is cheap. Knowning what is under the prefix is not. ",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-13 14:02:24,265141114,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that 
+	// were deleted
+	message TpStateDeleteResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;",,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/265141114,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r265141114,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"This is worded badly. In fact, I don't know that I would go so far as to even include prior art, since it is an extension of current Sawtooth - the Merkle-Radix trie - which that functionality made use of prior art. ",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-03-13 14:08:33,265143964,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that 
+	// were deleted
+	message TpStateDeleteResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+
+Addition to validator.proto:
+
+	// State delete prefix request from the transaction processor to the 
+	// validator/context_manager
+	TP_STATE_DELETE_PREFIX_REQUEST = 17;
+	// State delete prefix response from the validator/context_manager to the
+	// transaction processor
+	TP_STATE_DELETE_PREFIX_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+1. This will need to be implemented in every sawtooth SDK.
+2. Once implemented it will be difficult to deprecate because of our 
+   commitment to backward compatibility.
+3. This requires transaction processor developers to understand the risk of
+   calling delete by prefix and accidently deleting unintended addresses.
+   Delete by prefix should be used only when developer is sure prefix is 
+   unique like when having a pre-defined prefix and not in case there could
+   be hash collisions for the prefix.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to delete multiple 
+addresses transaction processor will first read the special address, then 
+delete each address in the list.
+This has a performance downside that each time transaction processor needs to 
+write a new address, it will need to modify the special address containing 
+list of all addresses. This address could be huge and it needs to be changed
+for almost every transaction, while the need to delete all the addresses in 
+the list is very rare.
+
+# Prior art
+[prior-art]: #prior-art
+
+Access address by prefix is a key capability of radix tree.",125,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/265143964,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r265143964,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,This message does not include the addresses that were deleted as stated above by the comment.,ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-04-03 12:50:07,271725110,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response from the context_manager/validator with the addresses
+	// that were deleted
+	message TpStateDeleteResponse {",77,2019-04-03 13:43:49,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271725110,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r271725110,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"thank you, fixed",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-04-03 13:45:10,271749567,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response from the context_manager/validator with the addresses
+	// that were deleted
+	message TpStateDeleteResponse {",77,2019-04-03 13:45:10,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271749567,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r271749567,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/39,https://github.com/hyperledger/sawtooth-rfcs/pull/39,"agree, removed the list",ea46e1371e7ded43425d0f065a2d1baae8dcbee6,2019-04-03 13:46:47,271750334,"@@ -0,0 +1,135 @@
+- Feature Name: delete_by_prefix
+- Start Date: 2019-2-20
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.deletePrefix(address_prefix) which will get an address prefix as input
+parameter and delete all the addresses in the prefix subtree.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for an efficient way to delete chunk of 
+addresses, for example all addresses that are allocated for a certain user.
+If each member has some unique address prefix, deleting all the addresses 
+belong to a member can become more efficient with delete by prefix.
+
+Since addresses are stored in a radix tree, deleting many addresses by one 
+prefix is much faster than deleting the same amount of addresses one by one.
+Furthermore, delete by prefix doesn't require transaction processor to know 
+which addresses exist. This will remove the need to store huge list of 
+existing addresses in a special address that will need to be modified by every
+transaction that writes a new address.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+A context API to delete address already exist for transaction processors.
+The existing delete API accepts only full 35 bytes addresses.
+This RFC exposes to transaction processor a new capability of deleting by 
+address prefix. When delete by prefix is called, all addresses under the 
+prefix subtree will be deleted.
+
+Main usage for this feature is the need to support some 'clean transaction'
+that will remove all addresses with some communality, for example all 
+addresses that belong to some member.
+The proposed solution is to have address mapping logic that allocates some
+unique pre-defined address prefix per member. The transaction processor logic
+will know to calculate the address suffix based on transaction payload data
+and execute any transaction.
+In order to execute the clean transaction, the transaction processor will
+call context.deletePrefix with the member prefix.
+
+The existing solution is to keep track of all used addresses, this will
+require read-modify-write of a special address for every new address that is
+used. This solution could work but will slow down executing of every regular 
+transactions while the clean transaction is rare.
+
+No change is recommended to the existing methods, so the implementation of this
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: deletePrefix that delete a subtree of addresses 
+starting from a given address prefix. 
+This API should be similar to regular delete address API but can get as input
+prefix of address and not just full 35 bytes address.
+Calling this API with full address should behave exactly like calling existing
+delete API.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+	
+	// A request from the handler/tp to delete state prefix entries at an 
+	// collection of addresses prefix subtree
+	message TpStateDeletePrefixRequest {
+		string context_id = 1;
+		repeated string addresses_prefix = 2;
+	}
+
+	// A response form the contextmanager/validator with the addresses that 
+	// were deleted
+	message TpStateDeleteResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;",,2019-04-03 13:46:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271750334,https://github.com/hyperledger/sawtooth-rfcs/pull/39#discussion_r271750334,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"The other SDKs appear with the Sawtooth core documentation. This has the advantage of being a lot easier to find and access if with the other docs.
The reason the other SDKs were split out was for sawtooth-core build issues and extra time required.",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-15 00:44:19,257069351,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,
+  the Sawtooth `.proto` files and Swift protogen scripts are included directly
+  in the XO client example application. Further work must be done to
+  implement the generation of the protobuf messages as part of the Swift SDK.
+
+### Other issues
+
+- The documentation for the Swift SDK uses the same templates as the other
+  SDKs. The templates were copied from the sawtooth-core repository. This is
+  not an ideal solution, as there is an extra overhead to maintaining several
+  versions of the same documentation across several repositories.
+
+- Ideally, the documentation for the Swift SDK should be published under the
+  Application Developer's Guide section in the Sawtooth documentation,
+  together with the documentation for the other SDKs. It is not clear what the
+  best strategy is to publish the Swift SDK documentation, given that the
+  source files and code base are in an different repository than the Sawtooth
+  documentation.",202,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257069351,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257069351,danintel
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,This sentence could use more context as to why Grid expanded the possibility ,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 16:33:43,257762903,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257762903,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257762903,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"""Swift is an language"" -> ""Swift is a language""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 16:37:44,257764321,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which ",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257764321,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257764321,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"""Using Swift makes is possible"" -> ""Using Swift makes it possible""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 16:38:24,257764548,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257764548,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257764548,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"The docs should be contained in the swift sdk repository as they need to be maintained in-sync with the source code. We are working on a mechanism to combine docs during publishing, since this isn't the only instance this issue exists.",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 20:30:13,257819846,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,
+  the Sawtooth `.proto` files and Swift protogen scripts are included directly
+  in the XO client example application. Further work must be done to
+  implement the generation of the protobuf messages as part of the Swift SDK.
+
+### Other issues
+
+- The documentation for the Swift SDK uses the same templates as the other
+  SDKs. The templates were copied from the sawtooth-core repository. This is
+  not an ideal solution, as there is an extra overhead to maintaining several
+  versions of the same documentation across several repositories.
+
+- Ideally, the documentation for the Swift SDK should be published under the
+  Application Developer's Guide section in the Sawtooth documentation,
+  together with the documentation for the other SDKs. It is not clear what the
+  best strategy is to publish the Swift SDK documentation, given that the
+  source files and code base are in an different repository than the Sawtooth
+  documentation.",202,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257819846,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257819846,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"Is ""Sawtooth Signing"" (with a space) correct?  It's written as `SawtoothSigning` on line 28.  

If the space is supposed to be there, I'd change ""Signing"" to"" signing"" (lower-case s) to indicate that this is not the actual module name.  And to reduce the Use of Uneccessary Capital Letters. :)",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:33:31,257831923,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257831923,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257831923,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"(Nitpick)  Other RFCs ~being~ begin with a complete sentence - ""This RFC describes/proposes ...""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:35:17,257832210,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257832210,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257832210,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"Line 12 said ""sign transactions"".  It's not clear why txns is used above but bytes is used here.",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:38:40,257832820,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257832820,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257832820,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"(Nitpick) Please use lower-case for ""transaction processors"" to be consistent with the Sawtooth core docs.",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:39:38,257832992,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257832992,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257832992,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"Missing ""the"" (before SawtoothSigning)",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:42:04,257833393,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257833393,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257833393,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"(Nitpick) I'd change ""minimal learning friction"" to a simpler phrase, such as ""with minimal effort"", or reword as ""will be able to learn the Swift SDK quickly"". Or something like that. 

(I had to google ""learning friction"" because I'm not up on current UX jargon.)",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:46:22,257834311,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning",136,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257834311,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257834311,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"Or you could rewrite as ""Swift has simpler syntax and is easier to learn, which...""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:48:06,257834576,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which ",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257834576,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257834576,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"s/To use/Using/
Add add ""a"" before ""dynamic framework""

     ""Using SawtoothSigning as a dynamic framework also allows ...""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:50:41,257835018,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257835018,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257835018,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"(Nitpick) I found it confusing that the items to be updated are after the explanation of using a Cartfile and the including/loading examples.  
- I suggest reorganizing the first bullet so that the items that must be updated are first, followed by the details and examples.
- I would remove ""If a new repository ... is created to host the Swift SDK,"" because the summary already says that a new repo ""should be created"".

For example: 

Once the new Swift SDK repository exists, the existing Cartfile in the XO example needs to be updated to the new path of the SDK. The documentation also needs instructions on how to import the SawtoothSigning framework from the new repository.

To use Carthage to import the SawtoothSigning framework into an iOS project, it is necessary to include a Cartfile, which describes the project’s dependencies to Carthage. GitHub repositories are specified with the github keyword. For example, ...",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 21:59:48,257836823,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+",180,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257836823,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257836823,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,s/Carfile/Cartfile,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 22:00:21,257836925,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257836925,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257836925,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"(Nitpick) A bit confusing, grammatically - it implies that we are not yet stabilized, rather than the protos.  I suggest moving/rewording ""before stabilization"" to later in the sentence, so that there's no hint of a dangling modifier.  How about: 

""We might want to include them temporarily until the Swift SDK is stabilized.""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 22:03:04,257837374,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257837374,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257837374,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"I agree with both @danintel and @vaporos: The core Sawtooth doc should make it easy to find all SDK docs, and a future mechanism for ""combined docs"" is highly desirable. But that's outside the scope of this RFC.  

For the Swift SDK, the following Sawtooth core topics could easily be updated to include info about and a link to the Swift SDK docs. 
- Available SDKs: https://sawtooth.hyperledger.org/docs/core/nightly/master/app_developers_guide/sdk_table.html
- Using the Sawtooth SDKs: 
https://sawtooth.hyperledger.org/docs/core/nightly/master/app_developers_guide/using_the_sdks.html",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-18 22:21:57,257840458,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,
+  the Sawtooth `.proto` files and Swift protogen scripts are included directly
+  in the XO client example application. Further work must be done to
+  implement the generation of the protobuf messages as part of the Swift SDK.
+
+### Other issues
+
+- The documentation for the Swift SDK uses the same templates as the other
+  SDKs. The templates were copied from the sawtooth-core repository. This is
+  not an ideal solution, as there is an extra overhead to maintaining several
+  versions of the same documentation across several repositories.
+
+- Ideally, the documentation for the Swift SDK should be published under the
+  Application Developer's Guide section in the Sawtooth documentation,
+  together with the documentation for the other SDKs. It is not clear what the
+  best strategy is to publish the Swift SDK documentation, given that the
+  source files and code base are in an different repository than the Sawtooth
+  documentation.",202,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/257840458,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r257840458,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,Updated with a better description,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:33:50,258172769,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258172769,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258172769,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,Updated summary,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:34:43,258173127,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258173127,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258173127,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"Changed it to ""sign transactions""",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:34:56,258173222,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258173222,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258173222,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,Fixed,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:35:20,258173362,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258173362,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258173362,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,Updated this description,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:37:46,258174236,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,",,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258174236,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258174236,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,I decided to just remove that example. It is not necessary to detail that process for the RFC.,5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 18:45:14,258176982,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+",180,2019-02-19 23:27:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258176982,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258176982,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/37,https://github.com/hyperledger/sawtooth-rfcs/pull/37,"I shortened the description of this issue to the core problem. It is largely out of scope for the RFC, but still important to mention as it will affect the proposed repository later on.",5c5ba43ad6ee131c529523077cbbbe7a53b4f5a5,2019-02-19 23:28:29,258275346,"@@ -0,0 +1,214 @@
+- Feature Name: sawtooth_sdk_swift
+- Start Date: 2019-02-14
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Develop a Sawtooth SDK for Swift to facilitate development for client side
+Cocoa/Cocoa Touch applications. It is modeled after existing SDKs,
+implementing a Sawtooth Signing module that provides methods to generate
+private/public key pairs and sign transactions. A separate repository should
+be created to host the Swift SDK.
+
+# Motivation
+[motivation]: #motivation
+
+As the interest in Sawtooth grows, so does the demand for native mobile
+applications that can create and submit valid transactions. Additionally, the
+introduction of Hyperledger Grid has expanded the possibility for mobile use
+cases for the Sawtooth platform. The creation of a Swift SDK will facilitate
+the development of native applications for iOS and fulfill a need from the
+community.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Swift SDK works similarly to existing SDKs. It provides a SawtoothSigning
+module that supports generating private/public key pairs and signing bytes.
+This module facilitates signing transactions in a way that is compatible with
+Sawtooth requirements.
+
+The Swift SDK proposed here does not currently implement a module to assist
+with the creation of Transaction Processors, as the initial purpose is to
+support front-end application development.
+
+### Capabilities
+
+- Generate private/public key pairs
+- Sign transactions
+- Verify signatures
+
+###### Example
+```
+import SawtoothSigning
+
+let context = Secp256k1Context()
+let privateKey = context.newRandomPrivateKey()
+let signer = Signer(context: context, privateKey: privateKey)
+let signature = signer.sign(data: message_bytes)
+context.verify(signature: signature, data: message_bytes,
+publicKey: signer.getPublicKey())
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+An initial implementation of the SawtoothSigning framework has already been
+written. The code can be found at
+[bitwiseio/sawtooth-sdk-swift](https://github.com/bitwiseio/sawtooth-sdk-swift).
+The repository also contains documentation explaining how to use the
+SawtoothSigning framework and a working example of the framework in use by an
+iOS client for the
+[xo transaction family](https://sawtooth.hyperledger.org/docs/core/releases/latest/transaction_family_specifications/xo_transaction_family.html).
+
+### Dependencies
+
+#### SawtoothSigning
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+#### XO client example
+- SawtoothSigning
+- [SwiftProtobuf](https://github.com/apple/swift-protobuf/)
+- [secp256k1](https://github.com/Boilertalk/secp256k1.swift)
+
+### Modules and Methods
+
+#### SawtoothSigning
+- Secp256k1Context
+  - sign
+  - verify
+  - getPublicKey
+  - newRandomPrivateKey
+- CryptoFactory
+  - createContext
+- Secp256k1PrivateKey
+  - fromHex
+  - hex
+  - getBytes
+- Secp256k1PublicKey
+  - fromHex
+  - hex
+  - getBytes
+- Signer
+  - sign
+  - getPublicKey
+
+### Importing SawtoothSigning framework
+The Swift SDK's SawtoothSigning module can be imported to a Cocoa/Cocoa Touch
+project via [Carthage](https://github.com/Carthage/Carthage), a dependency
+manager for Cocoa applications.
+
+1. Install Carthage using
+[these installation instructions](https://github.com/Carthage/Carthage#installing-carthage).
+
+2. Create a Cartfile in the same directory as your `.xcodeproj` or
+`.xcworkspace`.
+
+3. In the Cartfile for your project, add:
+   ```
+   github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+   ```
+
+4. Run `carthage update`.
+
+   After the framework is downloaded and built, it can be found at `path/to/your/project/Carthage/Build/<platform>/SawtoothSigning.framework`
+
+5. Add the built .framework binaries to the ""Embedded Binaries"" and
+""LinkedFrameworks and Libraries"" sections in your Xcode project.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+There are no known drawbacks for this addition, besides the maintainence of an
+additional SDK. No existing features or repositories will be changed.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The Sawtooth Swift SDK is modeled after the existing SDKs, with similar 
+classes and methods. Developers who are already familiar with other Sawtooth
+SDKs and Swift will be able to use the Swift SDK with minimal learning
+friction.
+
+#### Objective-C SDK
+
+An SDK that supports Cocoa/Cocoa Touch Applications could also be written in
+Objective-C. In spite of Objective-C being an older language with many
+feature-rich APIs, Swift is a better choice for a couple of reasons:
+
+- Swift is an language that is easier to learn, with simpler syntax which 
+  makes code easier to read and write. 
+- Using Swift makes is possible to export the SawtoothSigning module as a
+  dynamic framework rather than a static library. Dynamic frameworks reduce
+  the launch time of apps and their memory footprints when compared to static
+  libraries. To use SawtoothSigning as dynamic framework also allows the users
+  to use Carthage to download and build the module directly from the GitHub
+  repository.
+
+#### Use existing functionality
+
+Without the Swift SDK, developers have two options to write an iOS-compatible 
+Sawtooth client:
+- Create a web app using the Javascript SDK
+- Implement custom methods to create and sign transactions natively
+
+The first solution does not work for a use case that requires a native
+application. The second solution raises the barrier of entry for developers
+with a background in mobile development who want to use Sawtooth. The creation
+of the Sawtooth Swift SDK, including documentation and examples, will make the
+Sawtooth platform more accessible to those coming from mobile development.
+
+# Prior art
+[prior-art]: #prior-art
+
+The implementation of the SawtoothSigning module for the Swift SDK is based
+on existing implementations for other languages, such as 
+[Java](https://github.com/hyperledger/sawtooth-sdk-java/tree/master/sawtooth-sdk-signing/src/main/java/sawtooth/sdk/signing) 
+and
+[Rust](https://github.com/hyperledger/sawtooth-sdk-rust/blob/master/src/signing/secp256k1.rs).
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+### To be implemented
+
+- To use Carthage to import the SawtoothSigning framework into an iOS project
+  it is necessary to include a Cartfile, which describes the project’s
+  dependencies to Carthage. GitHub repositories are specified with the github
+  keyword. For example, it is possible to load the framework from a branch:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ""master""
+  ```
+
+  Or from a release:
+  ```
+  github ""bitwiseio/sawtooth-sdk-swift"" ~> 0.1
+  ```
+
+  If a new repository within the Hyperledger organization is created to host
+  the Sawtooth Swift SDK, the existing Carfile in the XO example needs to be
+  updated for the new path of the SDK. The documentation also needs to be
+  updated to include instructions on how to import the SawtoothSigning
+  framework from the new hyperledger repository.
+
+- The Sawtooth protos are not included as part of the Swift SDK. Before
+  stabilization, we may want to include them as part of the SDK. Currently,
+  the Sawtooth `.proto` files and Swift protogen scripts are included directly
+  in the XO client example application. Further work must be done to
+  implement the generation of the protobuf messages as part of the Swift SDK.
+
+### Other issues
+
+- The documentation for the Swift SDK uses the same templates as the other
+  SDKs. The templates were copied from the sawtooth-core repository. This is
+  not an ideal solution, as there is an extra overhead to maintaining several
+  versions of the same documentation across several repositories.
+
+- Ideally, the documentation for the Swift SDK should be published under the
+  Application Developer's Guide section in the Sawtooth documentation,
+  together with the documentation for the other SDKs. It is not clear what the
+  best strategy is to publish the Swift SDK documentation, given that the
+  source files and code base are in an different repository than the Sawtooth
+  documentation.",202,2019-02-19 23:28:29,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/258275346,https://github.com/hyperledger/sawtooth-rfcs/pull/37#discussion_r258275346,dplumb94
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Do we allow the case of address_prefix being empty / none?
If so, the reference implementation lists all addresses with non-empty data at prefixes which are in input address list of transaction.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-03 03:48:54,244908597,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.",11,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/244908597,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r244908597,arsulegai
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"I think that better to follow logic as in get_state: if TP is requesting read of addresses that is not in 'input' address list then return failure. 
It is up to the transaction family developer to check return value of the api and to make sure that the 'input' address list is valid, since the input list can accept address prefix i don't see this as a change of behavior.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-03 06:57:54,244921267,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.",11,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/244921267,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r244921267,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,Thanks,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-03 08:20:19,244931042,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.",11,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/244931042,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r244931042,arsulegai
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,This section is very light on content. Please provide concrete examples of why this is needed. Please check the Guide-Level explanation section in the template for other information that should be in this section.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 14:07:30,248293677,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.",39,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248293677,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248293677,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Please explain the actual implementation suggested for the validator and the sdks in this section as well, besides just the links to the commits.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 14:10:16,248294707,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248294707,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248294707,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"This argument - that the address could be huge - is also a flaw of this approach generally, since the returned address list could be that large. This should be mentioned in Drawbacks.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 15:48:23,248336358,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to manipulate multiple 
+addresses transaction processor will first read the special address, then 
+read/write/delete each address in the list.
+This has a performance downside that each time transaction processor needs to 
+write a new address, it will need to modify the special address containing 
+list of all addresses. This address could be huge, when the need to actually ",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248336358,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248336358,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"A few other drawbacks should be mentioned here: a) this will perform very poorly when the address list is large; b) this will cause large I/O and CPU on the validator when the address list is large (it is potentially very expensive, unlike the rest of the TP API); c) once implemented it will be hard/difficult to deprecate because of our commitment to backward compatibility; d) this seems like an anti-pattern in TP design, and may encourage others to implement poorly designed TPs.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 15:54:19,248339086,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248339086,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248339086,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,Another alternative is to redesign the TP to not need the list of addresses at all.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 15:56:03,248339882,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives",139,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248339882,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248339882,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"There is a performance penalty for maintaining a list like this, but it is probably substantially faster than the approach contained in this RFC.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 15:58:41,248341031,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to manipulate multiple 
+addresses transaction processor will first read the special address, then 
+read/write/delete each address in the list.
+This has a performance downside that each time transaction processor needs to ",145,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248341031,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248341031,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Add here the CLIENT_* proto messages which enable this, for reference.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:00:19,248341732,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to manipulate multiple 
+addresses transaction processor will first read the special address, then 
+read/write/delete each address in the list.
+This has a performance downside that each time transaction processor needs to 
+write a new address, it will need to modify the special address containing 
+list of all addresses. This address could be huge, when the need to actually 
+access all the addresses in the list can be rare.
+
+# Prior art
+[prior-art]: #prior-art
+
+Read state by address prefix is available from sawtooth-rest-api, and is
+one of the key capability that comes from using the radix tree.",155,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248341732,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248341732,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"This is misleading, as it is not ""missing"" in the sense that we forgot to add it, but rather, it was excluded as uninteresting and not performant. Suggest ""This capability exists in the Sawtooth REST API, but is not currently available via the transaction processor protocol or SDKs (for reasons covered in 'Drawbacks' below).""",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:07:43,248344913,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248344913,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248344913,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"On the surface, this could be explained away by bad TP and state address scheme. It would help if this section had an illustrative example to show when we would take this approach vs. simply using a different addressing scheme.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:10:43,248346189,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.",29,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248346189,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248346189,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,This is not a well-formed sentence.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:12:08,248346813,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.",32,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248346813,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248346813,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"What if this request was for an address space with an exceptionally large number of addresses? Should they just be returned, or is there an upper limit, where the validator will send an error back with ""request denied"". If there should be an upper limit, how would we manage it?

Another approach may be a paging approach, where the request specifies the maximum number of addresses to return (with a max page size, perhaps configurable per-validator).",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:18:31,248349581,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;",91,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248349581,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248349581,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Why is this a list? If it remains a list, we should specify in this section how the validator should process the request.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:20:59,248350619,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;",91,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248350619,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248350619,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,This should probably return an iterator so not all addresses must be obtained at once.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:21:42,248350911,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input",10,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248350911,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248350911,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"This document should describe whether this occurs based on a comparison to the request, or whether it occurs if the list would return an address outside the allowed range. It should probably be the former.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 16:24:34,248352130,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;",101,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248352130,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248352130,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"I don't know how this is handled for the rest-api, but one salient difference between the situation for the rest-api and a transaction processor, is that only one node is impacted by the query overhead for serving the rest-api request whereas all nodes are impacted by a transaction using a similar query. If not implemented carefully, a significant performance cost could be exploited as a DOS attack on the network.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 17:04:32,248368987,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248368987,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248368987,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,@yoni-wolf you have more thoughts in here: https://jira.hyperledger.org/browse/STL-1375 like the example of deleting all leaves under a certain node. As a use case maybe that's deleting all accounts owned by a certain user?,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 17:06:22,248369709,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.",39,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248369709,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248369709,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,Agreed.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 20:46:06,248444053,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input",10,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248444053,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248444053,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,The optimization here is actually reduces the number of messages between the validator and a transaction processor in exchange for potentially larger messages.,fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-16 20:49:30,248445152,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction ",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/248445152,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r248445152,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Same as regular read, there is no limitation of data size in one address, it is up the the TP developer to make sure he doesn't write too much data to one address. 
Since only requesting address list without addresses data this won't get too big that easily.
No objection to adding paging and max, only that it is not done in regular read address, added it as to unresolved questions section.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-20 10:16:22,249272390,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;",91,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/249272390,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r249272390,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Added to drawbacks.
The main different is that the access to a huge set of data can be limited to only when actually need to change all the data. with 'old' approach of keeping an address that tracks existing addresses, TP needs to modify this address during almost every transaction.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-20 12:34:47,249276901,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This will need to be implemented in every sawtooth SDK and will increase 
+the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to manipulate multiple 
+addresses transaction processor will first read the special address, then 
+read/write/delete each address in the list.
+This has a performance downside that each time transaction processor needs to 
+write a new address, it will need to modify the special address containing 
+list of all addresses. This address could be huge, when the need to actually ",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/249276901,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r249276901,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"It is not the same type of problem, because read/write are relatively efficient. Returning a list of addresses from the state tree will not be efficient as it will require a large number of underlying database reads to accomplish.

Adding an on-chain setting for maximum size of address storage would probably be a good feature too.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-22 05:02:54,249642114,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Example of implementation can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;",91,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/249642114,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r249642114,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"Suggest ""The relevant protocol defined in client_state.proto is:""",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-22 05:05:13,249642394,"@@ -0,0 +1,206 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists in the Sawtooth REST API, but is not currently 
+available via the transaction processor protocol or SDKs (for reasons covered 
+in 'Drawbacks' below).
+
+An open question is how to handle a very large list (iterator/paging/max),
+this is not fully answered in this RFC, see 'unresolved-questions' below.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to reduce the number of transaction
+processor calls to modify large addresses when need to work on a big chunk of 
+addresses by making better use optimization that comes from using the radix 
+tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping and a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth REST API.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.
+
+The motivation came when looking for a way to improve transaction processor
+performance when deleting chunk of addresses.
+For example when need to delete all addresses owned by some user with some
+extra conditions based on transaction processor logic.
+The problem is that the transaction processor doesn't know which addresses
+exists in the merkle tree.
+Old approach will be to store a special address with details on all existing
+addresses belong to each user. This is not optimal since although the need to 
+delete is rare, this special address will need to be updated for almost any
+transaction. This address can get huge very fast, having a big performance 
+impact for almost every transaction. Bucketing to several addresses can help
+improve performance but not significantly.
+
+The approach requested by this RFC is to take advantage of the 'radix' part
+of the context radix merkle tree. When storing data use common prefix based
+on transaction processor logic, then when need to delete chunk of address, 
+get the list of existing addresses under a prefix and request delete for all 
+addresses in the list. The returned list of addresses might be huge but with
+this approach, only in the rare cases of delete, the transaction processor will
+need to handle this huge amount of addresses. 
+Can improve performance since need only list of existing addresses under a prefix
+and not the address data, keeping the returned value in a relative normal size.
+
+No change is recommended to the existing methods, so the implementation of this 
+RFC should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Add new API to context: list_addresses that returns list of existing addresses
+under given address prefix. 
+This API should be similar to regular get address data API but can get as input
+prefix of address and not just full 35 bytes address and return a list of 
+addresses without their data. Since the ability to read by prefix is already
+supported from the REST API, this is not a new concept for the validator.
+There is still a need to add a path from the transaction processor to the 
+validator for this new API and to support this API for each SDK.
+
+In the suggested implementation there are two proto changes:
+
+Addition to state_context.proto:
+
+
+	// A request from a handler/tp for list of addresses with valid data
+	message TpStateAddressesListRequest {
+		// The context id that references a context in the contextmanager
+		string context_id = 1;
+		repeated string addresses = 2;
+	}
+
+	// A response from the contextmanager/validator with a series of State 
+	// entries having valid data
+	message TpStateAddressesListResponse {
+		enum Status {
+			STATUS_UNSET = 0;
+			OK = 1;
+			AUTHORIZATION_ERROR = 2;
+		}
+
+		repeated string addresses = 1;
+		Status status = 2;
+	}
+	
+Addition to validator.proto:
+
+	// State list request to get all addresses having non empty data
+	TP_STATE_LIST_REQUEST = 17;
+	// State list response to get all addresses having non empty data
+	TP_STATE_LIST_RESPONSE = 18;
+	
+
+Example of implementation (without paging) can be found here: 
+Sawtooth-core + Python SDK changes: 
+https://github.com/arsulegai/sawtooth-core/tree/jira_1375 
+(commits: 4225dc7 and 2c3a223)
+Sawtooth-sdk-cxx changes: 
+https://github.com/arsulegai/sawtooth-sdk-cxx/tree/jira_1375 (commit: 23697e8)	
+	
+# Drawbacks
+[drawbacks]: #drawbacks
+
+1. This will need to be implemented in every sawtooth SDK.
+2. Once implemented it will be hard/difficult to deprecate because of our 
+   commitment to backward compatibility.
+3. Will slightly increase the complexity of the validator.
+4. Since list of addresses can be very large this could cause a large I/O 
+   and CPU impact on validator. In our case, this is partly solved by making 
+   the new API return only address list without address data (based on my 
+   testing, list of up to 1K of addresses didn't take longer than a read). 
+   And still, it is recommended when possible to add restrictions in 
+   transaction processor for when this API will be called (same way it is 
+   not recommended to store more than 1MB of data in one address or modify
+   large list of addresses in one call).
+   Paging mechanism like done in the REST-API might be needed here to fully 
+   solve this problem. 
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative could be in the transaction processor logic, store an address
+that contains list of existing addresses. When need to manipulate multiple 
+addresses transaction processor will first read the special address, then 
+read/write/delete each address in the list.
+This has a performance downside that each time transaction processor needs to 
+write a new address, it will need to modify the special address containing 
+list of all addresses. This address could be huge and it needs to be changed
+for almost every transaction, when the need to actually access all the
+addresses in the list is very be rare.
+
+# Prior art
+[prior-art]: #prior-art
+
+Read state by address prefix is available at REST API, and is
+one of the key capability that comes from using the radix tree.
+
+in client_state.proto:",,2019-01-28 07:31:39,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/249642394,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r249642394,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/34,https://github.com/hyperledger/sawtooth-rfcs/pull/34,"I think the main usage for this feature is this:
1) you allocate some prefix for a member or usage
2) the TP logic has some algorithm for translating data to sufix 
3) TP writes the data to the calculated prefix+sufix

now, it is very fast to find a certain piece of information, you just calculate it again and read that address, however, it is very hard and inefficient to clean it up! 

with 'read by prefix' TP can support a 'clean transaction' that will:
1) use this feature to acquire a list of all used addresses under that prefix
2) make a list of them and call the delete once, or call delete for each address there

how would it be implemented w/o 'read by prefix'? (assuming you did implement efficient lookup like this):

one way is to try and read every possible suffix (which might be a huge list and take forever), another way is to start keeping track of every suffix you used in another address, which you will have to read,modify,write for every sufix you add, which will slow you down. both are much worse i think.",fa778983c97a9ed023d4f5ca537e10fac874dea3,2019-01-28 08:10:13,251311188,"@@ -0,0 +1,111 @@
+- Feature Name: read_by_prefix
+- Start Date: 2018-12-19
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API to transaction context,
+context.listAddresses(address_prefix), which will get address prefix as input
+and return list of all existing addresses that starts with the prefix.
+
+This capability exists when reading state from rest-api but is missing when 
+transaction processor reads state.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation came when looking for a way to improve processing transaction 
+logic by making better use optimization that comes from using the radix tree. 
+
+Since addresses are stored in a radix tree, reading many addresses by 
+one prefix is much faster than reading the same amount of addresses one by one.
+Further more, read by prefix doesn't require transaction processor to know 
+which addresses exist, removing the need to store list of existing addresses 
+in a separate address.
+
+Having this capability allows a better and smarter address mapping this a 
+better transaction processor logic.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Get state by prefix is a feature available for reading via sawtooth rest-api.
+This RFC adds method to expose this capability of reading address prefix 
+to transaction processor.",39,2019-01-28 08:10:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/251311188,https://github.com/hyperledger/sawtooth-rfcs/pull/34#discussion_r251311188,lenzoron
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"I don't see the corresponding permissioning text below.
One of the main security considerations is how to prevent a bad actor from taking the validator offline. Attacks may include connecting the validator to offline or otherwise byzantine nodes. Other attacks may include the general type of exposure from adding any new remote call (input formatting etc).
The proposal adds remote access to what has so far required local system administrator access to the host. We must consider adversaries who are on the same local network. The system in most cases may be configured such that the component port is not advertised beyond the loopback, but there are valid reasons for the administrator to advertise the component port on the local network. This proposal should consider how the validator remains secure independent of the system's network configuration.",4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 14:58:12,237887810,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237887810,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237887810,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Have you also considered removing nodes? The security implications are a little more risky but about the same. The benefit of including a remove function would let us address the limits of maximum-peer-connectivity.,4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 15:13:24,237893293,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237893293,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237893293,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,I agree that it would be good to include removal functionality for discussion.,4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 15:29:07,237898721,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237898721,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237898721,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"In addition to IP addresses, this should also accept DNS names, which ZMQ will resolve: http://api.zeromq.org/2-1:zmq-tcp",4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 17:34:35,237942169,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the
+  Consul node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to
+add them without restarting the validator with a newer `--peers` parameter
+value. To add a new peer you can send the `ClientAddPeersRequest` to the
+`component` endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peers = ['tcp://192.168.0.100:8008', 'tcp://192.168.0.200:8008']
+add_peers_request = ClientAddPeersRequest(new_peers=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEERS_REQUEST,
+                     add_peers_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeersResponse()
+response.ParseFromString(response_serialized)
+```
+
+The response Protocol Buffers definition is the following:
+
+```protobuf
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:
+
+- Validates the format of peer URIs which has to be
+  `tcp://IP_ADDRESS:PORT_NUMBER`",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237942169,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237942169,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"If the response is sent back before connecting, there is no way to notify clients of authorization errors. Is not sending an error due to authorization an intentional decision? Or should the error enum be extended and a response sent only after peering completes?",4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 17:44:30,237944837,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the
+  Consul node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to
+add them without restarting the validator with a newer `--peers` parameter
+value. To add a new peer you can send the `ClientAddPeersRequest` to the
+`component` endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peers = ['tcp://192.168.0.100:8008', 'tcp://192.168.0.200:8008']
+add_peers_request = ClientAddPeersRequest(new_peers=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEERS_REQUEST,
+                     add_peers_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeersResponse()
+response.ParseFromString(response_serialized)
+```
+
+The response Protocol Buffers definition is the following:
+
+```protobuf
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:
+
+- Validates the format of peer URIs which has to be
+  `tcp://IP_ADDRESS:PORT_NUMBER`
+- If the validation was successful then the validator updates its peer list and
+  immediately returns the `OK` response. The new peers are connected _after_
+  that.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237944837,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237944837,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"My opinion would be to either have the response only be sent after the new connection is created, or expose another request type that allows the user to poll on the status of the new connection so they can detect connection failures such as authorization errors.",4f17b19f11b499784da81913094c6afc6f46530a,2018-11-30 17:47:19,237945707,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the
+  Consul node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to
+add them without restarting the validator with a newer `--peers` parameter
+value. To add a new peer you can send the `ClientAddPeersRequest` to the
+`component` endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peers = ['tcp://192.168.0.100:8008', 'tcp://192.168.0.200:8008']
+add_peers_request = ClientAddPeersRequest(new_peers=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEERS_REQUEST,
+                     add_peers_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeersResponse()
+response.ParseFromString(response_serialized)
+```
+
+The response Protocol Buffers definition is the following:
+
+```protobuf
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:
+
+- Validates the format of peer URIs which has to be
+  `tcp://IP_ADDRESS:PORT_NUMBER`
+- If the validation was successful then the validator updates its peer list and
+  immediately returns the `OK` response. The new peers are connected _after_
+  that.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+- The proposed solution does not provide any information on the status of new
+  peers. It just returns immediately if a request does not break the conditions
+  specified in the previous chapter.
+- It does not specify any connection retry policies leaving it to the existing
+  peering implementation.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+Alternatives were not considered and judging from multiple examples this is the
+state-of-the-art solution.
+
+# Prior art
+[prior-art]: #prior-art
+
+- [`addnode` in Bitcoin JSON RPC](https://bitcoincore.org/en/doc/0.16.0/rpc/network/addnode/)
+- [`admin_addPeer` in Ethereum management API](https://github.com/ethereum/go-ethereum/wiki/Management-APIs#admin_addpeer)
+
+Those two allow adding new peers in their platforms. Interesting points:
+
+- Ethereum can return the connection status;
+- Bitcoin allows specifying the connection retry policy.
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+- During the pre-RFC discussion on the #sawtooth-core-dev channel, there was no
+  final solution on ""should it be included to the REST API or no?""
+- In the same discussion, there was a proposition to resolve security issues by
+  using this feature along with the permissioning module. If we do not add this
+  feature to the REST API and hence to the administrative utilities then I do
+  not see any point in permissioning because the described feature remains an
+  internal interface of the application. Even if we do then we can restrict the
+  access to that feature by using a proxy as suggested in the documentation.
+- Should the system notify its user about the statuses of new connections or
+  leave the status check to the end user?",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/237945707,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r237945707,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Okay, after this explanation I have a better understanding of the permissioning requirement. Will describe a model that was discussed on the chat.",4f17b19f11b499784da81913094c6afc6f46530a,2018-12-03 09:45:12,238198798,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238198798,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r238198798,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,I think that this is a good idea.,4f17b19f11b499784da81913094c6afc6f46530a,2018-12-03 09:45:32,238198918,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238198918,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r238198918,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"> Is not sending an error due to authorization an intentional decision

I missed it intentionally to have a discussion on how we should handle such errors.

In my opinion, it would be better to extend the enum in `ClientAddPeersResponse` but then I would like to discuss the implementation. The simplest approach I see here is to make the [`Gossip`](https://github.com/hyperledger/sawtooth-core/blob/ffad8e38d9c26b278b374bc5edaf776e5d0edcb2/validator/sawtooth_validator/gossip/gossip.py#L77) class send notifications about peer status changes. This will allow us to know if the connection attempt was ok or not ok, but will not give any specific information. To be more specific we will have to add handlers for corresponding peer messages that will notify us, for example, about authorization violations, rejected connections and so on. I think that the simplest approach should be acceptable now (at least in the system our company build).",4f17b19f11b499784da81913094c6afc6f46530a,2018-12-03 10:13:03,238208466,"@@ -0,0 +1,188 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC proposes to implement the possibility to add new peers in runtime
+(through the component validator endpoint) when a node is working in the
+`static` peering mode. Along with that it also adds the corresponding extensions
+to the off-chain permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the
+  Consul node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to
+add them without restarting the validator with a newer `--peers` parameter
+value. To add a new peer you can send the `ClientAddPeersRequest` to the
+`component` endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peers = ['tcp://192.168.0.100:8008', 'tcp://192.168.0.200:8008']
+add_peers_request = ClientAddPeersRequest(new_peers=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEERS_REQUEST,
+                     add_peers_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeersResponse()
+response.ParseFromString(response_serialized)
+```
+
+The response Protocol Buffers definition is the following:
+
+```protobuf
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+
+message ClientAddPeersResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 3;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 4;
+    }
+    Status status = 1;
+    repeated string invalid_uris = 2;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:
+
+- Validates the format of peer URIs which has to be
+  `tcp://IP_ADDRESS:PORT_NUMBER`
+- If the validation was successful then the validator updates its peer list and
+  immediately returns the `OK` response. The new peers are connected _after_
+  that.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+- The proposed solution does not provide any information on the status of new
+  peers. It just returns immediately if a request does not break the conditions
+  specified in the previous chapter.
+- It does not specify any connection retry policies leaving it to the existing
+  peering implementation.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+Alternatives were not considered and judging from multiple examples this is the
+state-of-the-art solution.
+
+# Prior art
+[prior-art]: #prior-art
+
+- [`addnode` in Bitcoin JSON RPC](https://bitcoincore.org/en/doc/0.16.0/rpc/network/addnode/)
+- [`admin_addPeer` in Ethereum management API](https://github.com/ethereum/go-ethereum/wiki/Management-APIs#admin_addpeer)
+
+Those two allow adding new peers in their platforms. Interesting points:
+
+- Ethereum can return the connection status;
+- Bitcoin allows specifying the connection retry policy.
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+- During the pre-RFC discussion on the #sawtooth-core-dev channel, there was no
+  final solution on ""should it be included to the REST API or no?""
+- In the same discussion, there was a proposition to resolve security issues by
+  using this feature along with the permissioning module. If we do not add this
+  feature to the REST API and hence to the administrative utilities then I do
+  not see any point in permissioning because the described feature remains an
+  internal interface of the application. Even if we do then we can restrict the
+  access to that feature by using a proxy as suggested in the documentation.
+- Should the system notify its user about the statuses of new connections or
+  leave the status check to the end user?",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238208466,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r238208466,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Please add remove as well to this sentence,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-04 14:38:21,245314340,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a",21,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245314340,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245314340,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Can you explain why you have AUTHORIZATION_VIOLATION here along with ADMIN_AUTHORIZATION_ERROR, but only ADMIN_AUTHORIZATION_ERROR in ClientRemovePeerResponse?",4f17b19f11b499784da81913094c6afc6f46530a,2019-01-04 14:47:01,245316769,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;",87,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245316769,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245316769,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,This should be 133,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-04 14:48:05,245317091,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245317091,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245317091,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,This should be 134,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-04 14:48:15,245317139,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245317139,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245317139,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Please also add what the validator would do to remove peers,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-04 14:49:19,245317574,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245317574,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245317574,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,`AUTHORIZATION_VIOLATION` is the error thrown during the process of establishing a new connection from validator to another and is definitely irrelevant in `ClientRemovePeerResponse`.,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 12:01:41,245971444,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;",87,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245971444,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245971444,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Done.,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 12:03:30,245971874,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245971874,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245971874,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Done.,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 12:03:38,245971914,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;",,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245971914,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245971914,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Done.,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 12:03:44,245971931,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a",21,2019-01-08 12:08:31,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245971931,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245971931,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Added that, and there is one question: what to do if we go below the minimum peer connectivity limit here? I have two options in my mind:

- Just allow to do that, but it my bring the node functionality down;
- Another option is to throw an error here in case we reached the minimum limit.

Wondering which of those two will be a better solution.",4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 12:12:05,245973875,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:",,2019-01-08 12:12:05,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/245973875,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r245973875,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Ah i see,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 19:42:32,246127029,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;",87,2019-01-08 19:42:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/246127029,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r246127029,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Isnt this proposal only for static peering? minimum peer connectivity is used for dynamic peering. ,4f17b19f11b499784da81913094c6afc6f46530a,2019-01-08 19:48:44,246128933,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:",,2019-01-08 19:48:45,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/246128933,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r246128933,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"My bad, sorry",4f17b19f11b499784da81913094c6afc6f46530a,2019-01-09 16:07:44,246441301,"@@ -0,0 +1,210 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 131;
+        CLIENT_PEERS_REMOVE_RESPONSE = 132;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint. When the validator
+receives a new request for adding peers it:",,2019-01-09 16:07:45,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/246441301,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r246441301,eugene-babichenko
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Recommend: ""This RFC proposes functionality to add and remove static peer connections while a validator node is running.""",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:36:02,270998291,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component",8,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/270998291,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r270998291,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Recommend: ""This RFC also adds the corresponding extensions to the off-chain permssioning model.""",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:36:58,270998656,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain",10,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/270998656,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r270998656,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,add comma after 'network' and just pick either he or she for the gender,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:37:38,270998896,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has",16,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/270998896,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r270998896,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Recommend: ""Restarting the process adds substantial complexity to infrastructure automation, and incurs system downtime.""",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:39:30,270999559,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the",18,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/270999559,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r270999559,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add a comma after problem,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:39:48,270999676,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a",21,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/270999676,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r270999676,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Recommend: ""When an administrator adds new peers to the network, she likely wants to connect to them without needing to restart existing validators with an updated `--peers` parameter value.""",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:41:36,271000358,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add",36,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271000358,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271000358,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Recommend: ""To add a new peer, the administrator can send..."" and replace 'your' with 'their' later in the sentence.",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:43:11,271000893,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`",38,2019-04-01 18:48:56,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271000893,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271000893,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Remove I/we language,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:44:33,271001377,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:",67,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001377,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001377,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Remove I/we language,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:44:50,271001474,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:",116,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001474,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001474,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after successful,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:45:06,271001561,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a",143,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001561,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001561,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after successful,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:45:53,271001840,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.",144,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001840,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001840,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after happens,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:46:10,271001938,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers",150,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001938,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001938,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after Otherwise,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:46:19,271001983,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.",145,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271001983,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271001983,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Add comma after ""provided to the validator""",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:46:41,271002105,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator",153,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002105,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002105,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after connectivity,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:46:58,271002207,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and",154,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002207,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002207,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after connected,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:47:21,271002353,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+### Removing peers
+
+When the validator receives a new request for removing peers it does the
+following:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If a peer is connected the validator removes it. Otherwise the",165,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002353,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002353,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after specified,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:47:50,271002509,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+### Removing peers
+
+When the validator receives a new request for removing peers it does the
+following:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If a peer is connected the validator removes it. Otherwise the
+  `PEER_NOT_FOUND` error is thrown.
+
+## Permissioning
+
+The proposition is to add the `admin` role to off-chain permissioning that will
+restrict access to requests that can be malicious. The workflow for the
+validation is the following:
+
+- If the `admin` role is not specified then the permissioning module will use",174,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002509,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002509,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after specified,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:48:02,271002575,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+### Removing peers
+
+When the validator receives a new request for removing peers it does the
+following:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If a peer is connected the validator removes it. Otherwise the
+  `PEER_NOT_FOUND` error is thrown.
+
+## Permissioning
+
+The proposition is to add the `admin` role to off-chain permissioning that will
+restrict access to requests that can be malicious. The workflow for the
+validation is the following:
+
+- If the `admin` role is not specified then the permissioning module will use
+  the `default` policy.
+- If the `default` policy is not specified then the validation of the",176,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002575,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002575,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,Add comma after satisfied,4f17b19f11b499784da81913094c6afc6f46530a,2019-04-01 18:48:19,271002668,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+### Removing peers
+
+When the validator receives a new request for removing peers it does the
+following:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If a peer is connected the validator removes it. Otherwise the
+  `PEER_NOT_FOUND` error is thrown.
+
+## Permissioning
+
+The proposition is to add the `admin` role to off-chain permissioning that will
+restrict access to requests that can be malicious. The workflow for the
+validation is the following:
+
+- If the `admin` role is not specified then the permissioning module will use
+  the `default` policy.
+- If the `default` policy is not specified then the validation of the
+  permissions is not performed and fields `admin_public_key` and `signature` can
+  be omitted.
+- If the `default` or the `admin` policy is specified, then the permission
+  verifier checks:
+  - If the `admin_public_key` is allowed.
+  - If the `signature` is correct.
+  - If one of the above conditions is not satisfied then the",183,2019-04-01 18:48:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271002668,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271002668,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/32,https://github.com/hyperledger/sawtooth-rfcs/pull/32,"Can you expand on this? In particular, it is not clear what is being signed.",4f17b19f11b499784da81913094c6afc6f46530a,2019-04-02 22:19:33,271517684,"@@ -0,0 +1,222 @@
+- Feature Name: `add_peers_in_runtime`
+- Start Date: 2018-11-12
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary This RFC proposes to implement the possibility to add new
+peers and remove the existing connections in runtime (through the component
+validator endpoint) when a node is working in the `static` peering mode. Along
+with that it also adds the corresponding extensions to the off-chain
+permissioning model.
+
+# Motivation
+[motivation]: #motivation
+
+When an administrator adds a new node to an existing Sawtooth network he/she has
+to restart a node with new peering settings. This makes any automation
+significantly harder to write than if we had the possibility to add peers in the
+runtime and also decreases the uptime.
+
+To resolve this problem our team proposes to add a method to add new peers to a
+running node and to remove peers from a running node.
+
+An example use case is using Sawtooth along with a service discovery system like
+[Consul](https://www.consul.io):
+
+- A Consul node is set up along with the Sawtooth node;
+- A middleware continuously fetches the changes of the peer list from the Consul
+  node;
+- The middleware adds peers to the Sawtooth node via the validator component
+  endpoint.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+When an administrator adds new peers to the network you very likely want to add
+them without restarting the validator with a newer `--peers` parameter value. To
+add a new peer you can send the `ClientAddPeersRequest` to the `component`
+endpoint of your Sawtooth validator.
+
+The Protocol Buffers definition for this message is the following:
+
+```protobuf
+message ClientAddPeersRequest {
+    repeated string peers = 1;
+}
+```
+
+Simple Python example using `sawtooth_sdk.messaging.stream`:
+
+```python
+new_peer = 'tcp://192.168.0.100:8008'
+add_peer_request = ClientAddPeerRequest(peer_uri=new_peers)
+future = stream.send(Message.CLIENT_ADD_PEER_REQUEST,
+                     add_peer_request.SerializeToString())
+response_serialized = future.result().content
+response = ClientAddPeerResponse()
+response.ParseFromString(response_serialized)
+```
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Protocol Buffers definitions
+[protobuf]: #protobuf
+
+I propose to add them to `protos/client_peers.proto`:
+
+```protobuf
+message ClientAddPeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientAddPeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // One or more of peer URIs were malformed. List of malformed URIs is in
+        // the `invalid_uris` field of this response.
+        INVALID_PEER_URI = 4;
+        MAXIMUM_PEERS_CONNECTIVITY_REACHED = 5;
+        AUTHORIZATION_VIOLATION = 6;
+        CONNECTION_DROPPED = 7;
+    }
+    Status status = 1;
+}
+
+message ClientRemovePeerRequest {
+    repeated string peer_uri = 1;
+    string admin_public_key = 2;
+    // The signature of `peer_uri`
+    string signature = 3;
+}
+
+message ClientRemovePeerResponse {
+    enum Status {
+        STATUS_UNSET = 0;
+        OK = 1;
+        INTERNAL_ERROR = 2;
+        ADMIN_AUTHORIZATION_ERROR = 3;
+        // The requested peer do not exist
+        PEER_NOT_FOUND = 4;
+    }
+    Status status = 1;
+}
+```
+
+The rationale behind the `invalid_uris` is to be more precise about what is
+wrong and to ease the debugging process for developers.
+
+We should also add new message types to `protos/validator.proto`:
+
+```protobuf
+message Message {
+
+    enum MessageType {
+        // ...
+        CLIENT_PEERS_ADD_REQUEST = 131;
+        CLIENT_PEERS_ADD_RESPONSE = 132;
+        CLIENT_PEERS_REMOVE_REQUEST = 133;
+        CLIENT_PEERS_REMOVE_RESPONSE = 134;
+        // ...
+    }
+    // ...
+}
+```
+
+## How are the requests processed by the validator
+[request-processing]: #request-processing
+
+The requests are received on the `component` endpoint.
+
+### Adding peers
+
+When the validator receives a new request for adding peers it:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If the validation was successful then the validator tries to connect to a
+  provided peer. If the connection was successful it returns the `OK` status.
+  Otherwise the corresponding error status is returned.
+
+Edge cases:
+
+- The peer address format was wrong in one or more of the provided peers. If
+  that happens then the request fails without adding any new peers to the peers
+  list and returns the `INVALID_PEER_URI` status along with the list of faulty
+  peer URIs.
+- If the `--maximum-peer-connectivity` parameter was provided to the validator
+  then the validator checks if it has reached the maximum peer connectivity and
+  fails with an error if so. The validator also fails if it cannot add _all_ of
+  the peers provided in a request without breaking the provided
+  `maximum-peer-connectivity`.
+
+### Removing peers
+
+When the validator receives a new request for removing peers it does the
+following:
+
+- Validates the format of peer URI which has to be `tcp://ADDRESS:PORT_NUMBER`;
+- If a peer is connected the validator removes it. Otherwise the
+  `PEER_NOT_FOUND` error is thrown.
+
+## Permissioning
+
+The proposition is to add the `admin` role to off-chain permissioning that will
+restrict access to requests that can be malicious. The workflow for the
+validation is the following:
+
+- If the `admin` role is not specified then the permissioning module will use
+  the `default` policy.
+- If the `default` policy is not specified then the validation of the
+  permissions is not performed and fields `admin_public_key` and `signature` can
+  be omitted.
+- If the `default` or the `admin` policy is specified, then the permission
+  verifier checks:
+  - If the `admin_public_key` is allowed.
+  - If the `signature` is correct.",182,2019-04-02 22:19:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/271517684,https://github.com/hyperledger/sawtooth-rfcs/pull/32#discussion_r271517684,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,The second problem is addressed in the original Castro paper. Is this proposal intended to implement that solution or provide an different solution (see section 4.4 of Castro)? ,768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-02-26 00:16:41,260076197,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).",13,2019-02-26 00:21:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/260076197,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r260076197,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"My read of the Castro paper is that the backup proposing a view change will be the leader (primary) of the new view. Which node would be elected in this case? Would it be deterministic, and if so does that predictability present a new and meaningful risk? ",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-02-26 00:21:04,260077371,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).
+
+# Motivation
+[motivation]: #motivation
+
+This RFC proposes a solution to the ""unfair ordering"" problem for Sawtooth
+PBFT. In general, fair ordering means that a Byzantine node can not order
+transactions in a way that unfairly benefits that node. This problem is
+important in voting-type algorithms with a long-term leader, such as many PBFT
+variants. A malicious leader can, depending on the protocol, participate in the
+protocol perfectly but manipulate the ordering to its benefit by:
+
+1. Manipulating the order of a given set of transactions
+2. Generating and inserting new transactions into the ordering
+3. Withholding submitted transactions from the ordering
+
+One thing that makes this problem hard is that, without application-specific
+knowledge from the transactions, it is difficult to tell if an ordering
+actually benefits the leader or if ""random noise"" caused the ordering to be
+substantially different than some expected or fair ordering.
+
+The current implementation of PBFT does not prevent a leader from unfairly
+ordering transactions. This is a problem when a leader node has an incentive to
+do so. This RFC aims to mitigate this problem in the interest of making PBFT
+more resilient to bad actors.
+
+This RFC also proposes a mitigating solution to the ""silent leader"" problem.
+Sawtooth PBFT checks for leader liveness by starting a timer when a leader
+proposes a new block and endorses it with a pre-prepare message. If the timer
+expires before the new block is committed, the leader is suspected of being
+faulty and a view change is initiated. However, if a leader never proposes a
+block or doesn't endorse a block with a pre-prepare message, no timer is
+started. This means that a faulty leader can ""remain silent"" and stall the
+network without the other nodes being able to determine whether the silence is
+because no new batches are arriving, or the leader is intentionally ignoring
+them.
+
+                PrePreparing  <----------- Finished
+    (start timer) |                           ^
+                  v                           | (stop timer)
+                Preparing -------------> Committing
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In order to mitigate the effects of the ""unfair ordering"" and ""silent leader""
+problems, Sawtooth PBFT will perform view changes in two new cases.
+
+In the first case, regular view changes are ""forced"" based on the number of
+committed blocks. This mitigates the ""unfair ordering"" problem by giving every
+node a chance to be unfair for a period of blocks. This is inspired by how the
+Tendermint consensus algorithm handles the problem and how lottery-style
+algorithms handle the problem, where a new leader can be elected for every
+block.
+
+In the second case, nodes propose a standard view change when the network is
+idle for too long (the primary does not produce a block and a pre-prepare
+message). This mitigates the ""silent leader"" problem by limiting the amount of
+time a faulty leader can stall the network.
+
+The setting `sawtooth.consensus.pbft.forced_view_change_period` determines how
+often, measured in blocks, a view change should be forced. After transitioning
+from the Finished to PrePreparing phase, a node will check whether a view change
+should be forced and, if so, immediately change views. Specifically, the node",76,2019-02-26 00:21:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/260077371,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r260077371,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"Section 4.4 of the paper (""View Changes"") does not actually solve this problem. ""View
changes are triggered by timeouts that prevent backups from waiting indefinitely for requests to execute."" This timeout only starts once a request is received (in our case a block) and makes sure that it will execute at some point. It doesn't handle the case where no request (block) ever gets received.",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-02-26 14:47:44,260317863,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).",13,2019-02-26 14:47:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/260317863,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r260317863,ltseeley
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"That is incorrect; the new primary is deterministically calculated based on the ordering of the nodes (in the `sawtooth.consensus.pbft.peers` setting) and the view number. If you have _n_ nodes that are numbered _0 .. n - 1_, node _p_ will be the primary as determined by the formula: _p = v mod n_, where _v_ is the view number.",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-02-26 14:50:15,260319185,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).
+
+# Motivation
+[motivation]: #motivation
+
+This RFC proposes a solution to the ""unfair ordering"" problem for Sawtooth
+PBFT. In general, fair ordering means that a Byzantine node can not order
+transactions in a way that unfairly benefits that node. This problem is
+important in voting-type algorithms with a long-term leader, such as many PBFT
+variants. A malicious leader can, depending on the protocol, participate in the
+protocol perfectly but manipulate the ordering to its benefit by:
+
+1. Manipulating the order of a given set of transactions
+2. Generating and inserting new transactions into the ordering
+3. Withholding submitted transactions from the ordering
+
+One thing that makes this problem hard is that, without application-specific
+knowledge from the transactions, it is difficult to tell if an ordering
+actually benefits the leader or if ""random noise"" caused the ordering to be
+substantially different than some expected or fair ordering.
+
+The current implementation of PBFT does not prevent a leader from unfairly
+ordering transactions. This is a problem when a leader node has an incentive to
+do so. This RFC aims to mitigate this problem in the interest of making PBFT
+more resilient to bad actors.
+
+This RFC also proposes a mitigating solution to the ""silent leader"" problem.
+Sawtooth PBFT checks for leader liveness by starting a timer when a leader
+proposes a new block and endorses it with a pre-prepare message. If the timer
+expires before the new block is committed, the leader is suspected of being
+faulty and a view change is initiated. However, if a leader never proposes a
+block or doesn't endorse a block with a pre-prepare message, no timer is
+started. This means that a faulty leader can ""remain silent"" and stall the
+network without the other nodes being able to determine whether the silence is
+because no new batches are arriving, or the leader is intentionally ignoring
+them.
+
+                PrePreparing  <----------- Finished
+    (start timer) |                           ^
+                  v                           | (stop timer)
+                Preparing -------------> Committing
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In order to mitigate the effects of the ""unfair ordering"" and ""silent leader""
+problems, Sawtooth PBFT will perform view changes in two new cases.
+
+In the first case, regular view changes are ""forced"" based on the number of
+committed blocks. This mitigates the ""unfair ordering"" problem by giving every
+node a chance to be unfair for a period of blocks. This is inspired by how the
+Tendermint consensus algorithm handles the problem and how lottery-style
+algorithms handle the problem, where a new leader can be elected for every
+block.
+
+In the second case, nodes propose a standard view change when the network is
+idle for too long (the primary does not produce a block and a pre-prepare
+message). This mitigates the ""silent leader"" problem by limiting the amount of
+time a faulty leader can stall the network.
+
+The setting `sawtooth.consensus.pbft.forced_view_change_period` determines how
+often, measured in blocks, a view change should be forced. After transitioning
+from the Finished to PrePreparing phase, a node will check whether a view change
+should be forced and, if so, immediately change views. Specifically, the node",76,2019-02-26 14:50:15,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/260319185,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r260319185,ltseeley
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"So basically, when you force a view change (increment the view by 1), it will just be the next node in the on-chain list that becomes the primary.",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-02-26 14:51:24,260319768,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).
+
+# Motivation
+[motivation]: #motivation
+
+This RFC proposes a solution to the ""unfair ordering"" problem for Sawtooth
+PBFT. In general, fair ordering means that a Byzantine node can not order
+transactions in a way that unfairly benefits that node. This problem is
+important in voting-type algorithms with a long-term leader, such as many PBFT
+variants. A malicious leader can, depending on the protocol, participate in the
+protocol perfectly but manipulate the ordering to its benefit by:
+
+1. Manipulating the order of a given set of transactions
+2. Generating and inserting new transactions into the ordering
+3. Withholding submitted transactions from the ordering
+
+One thing that makes this problem hard is that, without application-specific
+knowledge from the transactions, it is difficult to tell if an ordering
+actually benefits the leader or if ""random noise"" caused the ordering to be
+substantially different than some expected or fair ordering.
+
+The current implementation of PBFT does not prevent a leader from unfairly
+ordering transactions. This is a problem when a leader node has an incentive to
+do so. This RFC aims to mitigate this problem in the interest of making PBFT
+more resilient to bad actors.
+
+This RFC also proposes a mitigating solution to the ""silent leader"" problem.
+Sawtooth PBFT checks for leader liveness by starting a timer when a leader
+proposes a new block and endorses it with a pre-prepare message. If the timer
+expires before the new block is committed, the leader is suspected of being
+faulty and a view change is initiated. However, if a leader never proposes a
+block or doesn't endorse a block with a pre-prepare message, no timer is
+started. This means that a faulty leader can ""remain silent"" and stall the
+network without the other nodes being able to determine whether the silence is
+because no new batches are arriving, or the leader is intentionally ignoring
+them.
+
+                PrePreparing  <----------- Finished
+    (start timer) |                           ^
+                  v                           | (stop timer)
+                Preparing -------------> Committing
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In order to mitigate the effects of the ""unfair ordering"" and ""silent leader""
+problems, Sawtooth PBFT will perform view changes in two new cases.
+
+In the first case, regular view changes are ""forced"" based on the number of
+committed blocks. This mitigates the ""unfair ordering"" problem by giving every
+node a chance to be unfair for a period of blocks. This is inspired by how the
+Tendermint consensus algorithm handles the problem and how lottery-style
+algorithms handle the problem, where a new leader can be elected for every
+block.
+
+In the second case, nodes propose a standard view change when the network is
+idle for too long (the primary does not produce a block and a pre-prepare
+message). This mitigates the ""silent leader"" problem by limiting the amount of
+time a faulty leader can stall the network.
+
+The setting `sawtooth.consensus.pbft.forced_view_change_period` determines how
+often, measured in blocks, a view change should be forced. After transitioning
+from the Finished to PrePreparing phase, a node will check whether a view change
+should be forced and, if so, immediately change views. Specifically, the node",76,2019-02-26 14:51:24,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/260319768,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r260319768,ltseeley
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"Ok. So long as we are addressing the client's request (i.e. transactions by a transactor) I think that is the spirit of requests in the paper. The way I read the paper, the lead validator is aggregating requests into blocks and then transmitting that aggregation of requests (the block) in the subsequent stages (preprepare, prepare, commit). The reply stage should probably then disaggregate the block to message the original clients. ",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-03-20 02:23:16,267165464,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).",13,2019-03-20 02:23:16,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/267165464,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r267165464,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/29,https://github.com/hyperledger/sawtooth-rfcs/pull/29,"Yes, I was incorrect. The sequencing you describe is consistent with the paper.",768300ef41bddb1cbb8c8cc570e169b2409f9e17,2019-03-20 02:45:58,267168836,"@@ -0,0 +1,185 @@
+- Feature Name: pbft_regular_view_changes
+- Start Date: 2018-10-11
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes an extension to the PBFT implementation to mitigate the
+effect of compromised leader nodes. The RFC takes into account leaders unfairly
+ordering batches within blocks (the ""unfair ordering"" problem) and leaders
+intentionally not producing new blocks to stall the network (the ""silent
+leader"" problem).
+
+# Motivation
+[motivation]: #motivation
+
+This RFC proposes a solution to the ""unfair ordering"" problem for Sawtooth
+PBFT. In general, fair ordering means that a Byzantine node can not order
+transactions in a way that unfairly benefits that node. This problem is
+important in voting-type algorithms with a long-term leader, such as many PBFT
+variants. A malicious leader can, depending on the protocol, participate in the
+protocol perfectly but manipulate the ordering to its benefit by:
+
+1. Manipulating the order of a given set of transactions
+2. Generating and inserting new transactions into the ordering
+3. Withholding submitted transactions from the ordering
+
+One thing that makes this problem hard is that, without application-specific
+knowledge from the transactions, it is difficult to tell if an ordering
+actually benefits the leader or if ""random noise"" caused the ordering to be
+substantially different than some expected or fair ordering.
+
+The current implementation of PBFT does not prevent a leader from unfairly
+ordering transactions. This is a problem when a leader node has an incentive to
+do so. This RFC aims to mitigate this problem in the interest of making PBFT
+more resilient to bad actors.
+
+This RFC also proposes a mitigating solution to the ""silent leader"" problem.
+Sawtooth PBFT checks for leader liveness by starting a timer when a leader
+proposes a new block and endorses it with a pre-prepare message. If the timer
+expires before the new block is committed, the leader is suspected of being
+faulty and a view change is initiated. However, if a leader never proposes a
+block or doesn't endorse a block with a pre-prepare message, no timer is
+started. This means that a faulty leader can ""remain silent"" and stall the
+network without the other nodes being able to determine whether the silence is
+because no new batches are arriving, or the leader is intentionally ignoring
+them.
+
+                PrePreparing  <----------- Finished
+    (start timer) |                           ^
+                  v                           | (stop timer)
+                Preparing -------------> Committing
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In order to mitigate the effects of the ""unfair ordering"" and ""silent leader""
+problems, Sawtooth PBFT will perform view changes in two new cases.
+
+In the first case, regular view changes are ""forced"" based on the number of
+committed blocks. This mitigates the ""unfair ordering"" problem by giving every
+node a chance to be unfair for a period of blocks. This is inspired by how the
+Tendermint consensus algorithm handles the problem and how lottery-style
+algorithms handle the problem, where a new leader can be elected for every
+block.
+
+In the second case, nodes propose a standard view change when the network is
+idle for too long (the primary does not produce a block and a pre-prepare
+message). This mitigates the ""silent leader"" problem by limiting the amount of
+time a faulty leader can stall the network.
+
+The setting `sawtooth.consensus.pbft.forced_view_change_period` determines how
+often, measured in blocks, a view change should be forced. After transitioning
+from the Finished to PrePreparing phase, a node will check whether a view change
+should be forced and, if so, immediately change views. Specifically, the node",76,2019-03-20 02:45:59,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/267168836,https://github.com/hyperledger/sawtooth-rfcs/pull/29#discussion_r267168836,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"The description of why re-serialization is undesirable should be placed in the ""rationale and alternatives"" section. (It is not really an option, but we should keep the description of why.)",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 14:58:54,219198608,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219198608,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219198608,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"This paragraph should be reworded to explain the proposal without referencing the re-serializing ""option"".

There is a typo ""soultion"" should be ""solution"".",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 15:01:52,219199808,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219199808,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219199808,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Remove ""For this recommended solution"" here.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 15:02:35,219200078,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219200078,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219200078,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Because of the nature of this change (changing an API we have committed to backwards compatibility), this should be restated in more firm terms omitting language like ""possible"", since this is a concrete proposal.  Alternatives should be added to the ""rationale and alternatives"" section.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:23:03,219269162,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.",42,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219269162,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219269162,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"This is technically impossible, since the signature is generated from the payload's hash. You could store a hash of a portion of the payload with in the payload.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:25:33,219269941,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Most transaction processors do not need the raw header bytes.
+Enabling this feature may mean carrying extra data through the sawtooth-core
+internal pipeline.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be to include a signature within the payload itself. That",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219269941,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219269941,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,I think you mean 'validator' instead of 'sawtooth-core'.,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:27:09,219270539,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Most transaction processors do not need the raw header bytes.
+Enabling this feature may mean carrying extra data through the sawtooth-core
+internal pipeline.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be to include a signature within the payload itself. That
+has the downside of bloating the transactions with an additional redundant
+field.
+
+# Prior art [prior-art]: #prior-art
+
+This problem appears unique to the conjunction of the sawtooth architecture and
+the use of a trusted execution environment for the transaction handling logic.
+
+# Unresolved questions [unresolved]: #unresolved-questions
+
+Internal sawtooth-core management of the header is not prescribed in this RFC.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219270539,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219270539,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Actually, 0.8 had an API which sent the raw bytes, so that's the best prior art to point to. This API changed during 1.0 stabilization to send the transaction payload as it is now.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:28:53,219271058,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Most transaction processors do not need the raw header bytes.
+Enabling this feature may mean carrying extra data through the sawtooth-core
+internal pipeline.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be to include a signature within the payload itself. That
+has the downside of bloating the transactions with an additional redundant
+field.
+
+# Prior art [prior-art]: #prior-art
+
+This problem appears unique to the conjunction of the sawtooth architecture and
+the use of a trusted execution environment for the transaction handling logic.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219271058,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219271058,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Is the ""private ledger TP"" available to link to? If not, it should not be mentioned here as readers will not have the appropriate context.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:31:38,219271928,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219271928,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219271928,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"This does not explain why it is important to be able to verify the signature within the TEE.

It seems like this could have been solved entirely within the payload content.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:35:07,219273003,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219273003,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219273003,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Suggest instead ""Adding this feature will increase the stable API surface for the benefit of a single transaction processor implementation. (Though in theory it could be useful to other transaction processors in the future, we do not know of any plans.) The stable API is important because we have made a commitment of backward support, so this feature, if added, will need to be supported into the future.

This will also increase the complexity of the validator slightly.""",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:39:37,219274587,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+There are several ways to implement this api. First and most simple, txn object
+that arrives to TP in 'apply' method will have an api to re-serialize the
+transaction header. This solution could be problematic since it requires trust
+in protobuf that de-serializing and re-serializing back will produce same bytes.
+
+A more reliable soultion is for the validator to forward the serialized header
+bytes of the transaction object, now calling the new get_serialized_header()
+will provide the bytes without the need to re-serialize. For this recommended
+solution, there is also an option to add flag to TP registration in order to let
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Most transaction processors do not need the raw header bytes.
+Enabling this feature may mean carrying extra data through the sawtooth-core",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219274587,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219274587,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,We may want this to be get_raw_header().,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:40:20,219274840,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219274840,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219274840,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Change the feature name here to raw_txn_header to match the filename.,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:40:42,219275047,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219275047,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219275047,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Either in the guide or reference explanation, we should have an example of the API change (maybe by showing the lines required within apply() to get to the raw header. Ideally, this example would be given in Rust but Python would be okay as well.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-20 18:42:29,219275622,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.
+
+
+# Guide-level explanation",23,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219275622,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r219275622,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"payload is verified with its hash that is in the signed txn header, TP needs a way to detect that attacker didn't replace the txn payload from a different txn, only way to know that is if we check signature of header that includes the payload hash.
if you ment adding the txn header fields in the payload, this will make the payload size a lot bigger ",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-09-25 15:47:48,220249273,"@@ -0,0 +1,96 @@
+- Feature Name: send_serialize_txn
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_serialized_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature.  For example, in the private ledger TP (C++)
+that runs inside SGX enclave, we can't trust the signature verification done by
+sawtooth since it happens outside of SGX.  In order to verify the transaction
+signature, TP needs to get the serialized transaction header and not just all
+the fields.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/220249273,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r220249273,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"typo, capitalize",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-22 19:57:02,227115908,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+an alternative would be that txn object that arrives to TP in 'apply' method ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227115908,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227115908,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Please clean up the above paragraph. There are several typos and incorrect uses of punctuation, and the last sentence is a little confusing.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-22 20:01:24,227117492,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+an alternative would be that txn object that arrives to TP in 'apply' method 
+will have an api to re-serialize the transaction header. 
+This solution could be problematic since it requires trust in protobuf that 
+de-serializing and re-serializing back will produce same bytes.
+
+An alternative would be to sign the payload itself. That has the downside of 
+bloating the transactions with an additional redundant field from the header.
+
+# Prior art [prior-art]: #prior-art
+
+sawtooth 0.8 had had an API which sent the raw bytes, This API changed during
+1.0 stabilization to send the transaction payload as it is now
+Missing this API appears unique to the conjunction of the sawtooth
+architecture and the use of a trusted execution environment for the 
+transaction handling logic.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227117492,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227117492,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"typo, `()` appears twice",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-22 20:01:46,227117622,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227117622,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227117622,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Suggest changing to ""The transaction processor API in Hyperledger Sawtooth v0.8 sent the raw header bytes, however this API was changed during the 1.0 stabilization period to its current state. The requirement that raw header bytes be included in the transaction processor API seems to be unique to processors that use a trusted execution environment.""",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-22 21:22:56,227143314,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {
+    TransactionHeader header = 1;  // The transaction header
+    bytes payload = 2;  // The transaction payload
+    string signature = 3;  // The transaction header_signature
+    string context_id = 4; // The context_id for state requests.
+    bytes header_raw = 5;
+}
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+an alternative would be that txn object that arrives to TP in 'apply' method 
+will have an api to re-serialize the transaction header. 
+This solution could be problematic since it requires trust in protobuf that 
+de-serializing and re-serializing back will produce same bytes.
+
+An alternative would be to sign the payload itself. That has the downside of 
+bloating the transactions with an additional redundant field from the header.
+
+# Prior art [prior-art]: #prior-art
+
+sawtooth 0.8 had had an API which sent the raw bytes, This API changed during
+1.0 stabilization to send the transaction payload as it is now
+Missing this API appears unique to the conjunction of the sawtooth
+architecture and the use of a trusted execution environment for the 
+transaction handling logic.",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227143314,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227143314,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,The above is not formatted correctly when viewed and process_request_header_style is missing a field number,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-23 14:56:50,227432179,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227432179,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227432179,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,This is not formatted correctly when viewed. ,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-10-23 14:58:04,227432776,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header()(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+message TpRegisterRequest {
+    message TpProcessRequestHeaderStyle {
+        STYLE_UNSET,
+        EXPANDED,
+        RAW
+    }
+
+    string family = 1;
+    string version = 2;
+    repeated string namespaces = 4;
+    uint32 max_occupancy = 5;
+    TpProcessRequestHeaderStyle process_request_header_style;
+}
+With a new field within TpRegisterRequest:
+
+message TpProcessRequest {",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227432776,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r227432776,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Sentence fragment,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:11:37,238357401,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238357401,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238357401,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/so implementation/so the implementation/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:12:07,238357596,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238357596,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238357596,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"I suggest replacing this sentence with something like: ""Calling get_serialized_header() will return the serialized bytes of the transaction object.""",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:13:45,238358222,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358222,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358222,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/add flag/add a flag/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:13:54,238358271,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358271,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358271,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,/to TP registration/to the TP registration/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:14:12,238358374,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358374,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358374,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/to let TP/to let the TP/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:14:23,238358483,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358483,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358483,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Insert line feed after },be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:14:50,238358655,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }",56,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358655,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358655,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Insert line feed after },be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:15:00,238358709,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }",66,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238358709,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238358709,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/txn/transaction/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:17:01,238359478,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be that txn object that arrives to TP in 'apply' method ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238359478,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238359478,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/that arrives to TP in 'apply' method/which is provided to the TP's 'apply' method/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:17:51,238359770,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be that txn object that arrives to TP in 'apply' method ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238359770,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238359770,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/requires trust/requires placing trust/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:18:17,238359951,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be that txn object that arrives to TP in 'apply' method 
+will have an api to re-serialize the transaction header. 
+This solution could be problematic since it requires trust in protobuf that ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238359951,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238359951,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/produce same bytes/produce the same bytes/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:18:26,238360035,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be that txn object that arrives to TP in 'apply' method 
+will have an api to re-serialize the transaction header. 
+This solution could be problematic since it requires trust in protobuf that ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238360035,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238360035,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,s/api/API/,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:18:58,238360226,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the
+bytes without the need to re-serialize. 
+there is also an option to add flag to TP registration in order to let TP
+decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+An alternative would be that txn object that arrives to TP in 'apply' method 
+will have an api to re-serialize the transaction header. ",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238360226,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238360226,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Replace this sentence with something like ""This RFC proposes an additional API call, txn.get_raw_header(), which will return the transaction's header bytes.""",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:20:51,238360948,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238360948,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238360948,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Is it txn.get_serialized_header() or txn.get_raw_header() ?,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-03 17:21:16,238361069,"@@ -0,0 +1,103 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+Requesting to add API to the sdk - txn.get_raw_header(), that will return
+the transaction header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The validator to forward the serialized header bytes of the transaction
+object, now calling the new get_serialized_header() will provide the",,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/238361069,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r238361069,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"What is the behavior when this is not set?  For backwards compatibility, I would suggest that the default should be expanded.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-19 20:53:22,243071491,"@@ -0,0 +1,104 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API call, txn.get_raw_header(), which will 
+return the transaction's header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Calling get_raw_header() will return the serialized bytes of the transaction 
+object without the need to re-serialize.
+There is also an option to add a flag to the TP registration in order to let the 
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would",68,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/243071491,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r243071491,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,Would an additional alternative be to implement a `Signer` that executes in the TEE?,be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-19 20:54:23,243071760,"@@ -0,0 +1,104 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API call, txn.get_raw_header(), which will 
+return the transaction's header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Calling get_raw_header() will return the serialized bytes of the transaction 
+object without the need to re-serialize.
+There is also an option to add a flag to the TP registration in order to let the 
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives",85,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/243071760,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r243071760,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"You mean that sawtooth will validate the transaction in TEE?
Attacker could still hack ZMQ and send a different transaction to TP, a secure TP must validate any data that comes from outside TEE.
Only option i can think of is that validator and TP both run in TEE and communicate directly enclave to enclave (which also requires some kind of communication authentication)",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-20 11:13:40,243235558,"@@ -0,0 +1,104 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API call, txn.get_raw_header(), which will 
+return the transaction's header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Calling get_raw_header() will return the serialized bytes of the transaction 
+object without the need to re-serialize.
+There is also an option to add a flag to the TP registration in order to let the 
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would
+be filled in.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Adding this feature will increase the stable API surface for the benefit of a
+single transaction processor implementation. (Though in theory it could be 
+useful to other transaction processors in the future, we do not know of any
+plans.) The stable API is important because we have made a commitment of 
+backward support, so this feature, if added, will need to be supported into
+the future.
+
+This will also increase the complexity of the validator slightly.
+
+# Rationale and alternatives
+[alternatives]: #alternatives",85,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/243235558,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r243235558,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"Agree, update RFC to reflect this",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2018-12-20 11:19:04,243236838,"@@ -0,0 +1,104 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API call, txn.get_raw_header(), which will 
+return the transaction's header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Calling get_raw_header() will return the serialized bytes of the transaction 
+object without the need to re-serialize.
+There is also an option to add a flag to the TP registration in order to let the 
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,
+            EXPANDED,
+            RAW
+        }
+
+        string family = 1;
+        string version = 2;
+        repeated string namespaces = 4;
+        uint32 max_occupancy = 5;
+        TpProcessRequestHeaderStyle process_request_header_style = 6;
+    }
+
+With a new field within TpRegisterRequest:
+
+    message TpProcessRequest {
+        TransactionHeader header = 1;  // The transaction header
+        bytes payload = 2;  // The transaction payload
+        string signature = 3;  // The transaction header_signature
+        string context_id = 4; // The context_id for state requests.
+        bytes header_raw = 5;
+    }
+
+For EXPANDED, 'header' field would be filled in, for RAW, 'header_raw' would",68,2018-12-23 09:24:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/243236838,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r243236838,yoni-wolf
https://github.com/hyperledger/sawtooth-rfcs/pull/23,https://github.com/hyperledger/sawtooth-rfcs/pull/23,"[Question]: Default is EXPANDED, do we still need STYLE_UNSET? If so, in which case?
Apologies if I am missing something here and for late commenting on it.",be8be2f95cffbc23a0461d3ab3aa7956cacbbc87,2019-01-03 04:42:34,244911640,"@@ -0,0 +1,105 @@
+- Feature Name: raw_txn_header
+- Start Date: 2018-08-03
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an additional API call, txn.get_raw_header(), which will 
+return the transaction's header bytes.
+
+# Motivation 
+[motivation]: #motivation
+
+For security reasons transaction processor would like to verify the incoming
+transaction header, and signature, if TP that runs inside SGX enclave, we 
+can't trust the signature verification done by sawtooth since it happens
+outside of SGX (TEE).
+In order to verify the transaction signature, TP needs to get the serialized
+transaction header and not just all the fields.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC adds a new method to directly access the transaction header bytes.
+Transaction Processor developers will use this API when they choose to re-verify
+the signature inside of a transaction processor.
+
+No change is recommended to the existing methods, so the implementation of this RFC
+should not cause any breakage for pre-existing transaction processors.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Calling get_raw_header() will return the serialized bytes of the transaction 
+object without the need to re-serialize.
+There is also an option to add a flag to the TP registration in order to let the 
+TP decide if it requires this API.
+
+A possible implementation approach would be to modify TpRegisterRequest to
+indicate that the transaction processor desires header bytes.
+
+    message TpRegisterRequest {
+        message TpProcessRequestHeaderStyle {
+            STYLE_UNSET,",46,2019-01-03 04:42:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/244911640,https://github.com/hyperledger/sawtooth-rfcs/pull/23#discussion_r244911640,arsulegai
https://github.com/hyperledger/sawtooth-rfcs/pull/22,https://github.com/hyperledger/sawtooth-rfcs/pull/22,Can you expand on this? How do inputs/outputs default today? What implications are there in enabling the entire address space ... performance? ... security? How do the transaction input/output addresses interact or not with permissions?,d6262942877b1cb488c93bc5703ebf72c4abe72e,2018-10-23 23:47:12,227602854,"@@ -0,0 +1,347 @@
+- Feature Name: seth\_rpc\_extensions
+- Start Date: 2018-07-30
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+The Seth JSON-RPC API is an implementation of the Ethereum JSON-RPC API, used
+for cross-compatibility with existing Ethereum clients. The Ethereum JSON-RPC
+API lacks several areas of functionality that require the existing Seth client
+to talk directly to the Sawtooth REST API. This RFC proposes several JSON-RPC
+API extensions that will allow the CLI to become a thin client that builds on
+top of the `web3` library.
+
+# Motivation
+[motivation]: #motivation
+
+Effecting this RFC proposal will bring us in line with existing extensions to
+the Ethereum JSON-RPC API, allowing current Ethereum clients to work with Seth
+without modification.
+
+Moving the CLI to becoming a thin client will allow a simplified architecture
+that stores private keys in one place, and will drastically reduce code
+complexity. It will also eliminate the need for clients to understand how to
+talk to the Sawtooth REST API.
+
+An additional benefit will be that dogfooding our JSON-RPC API will ensure that
+the implementation is tested and up-to-date with other implementations.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Managing Accounts
+
+The [parity] project has faced the same issue of allowing account management
+via the [JSON-RPC API][personal-api] that it represents. This RFC proposes
+implementing a compatible API that presents these endpoints:
+
+ - personal_listAccounts
+ - personal_newAccount
+ - personal_unlockAccount
+ 
+Additionally, in order to facilitate import existing accounts, the API will
+present this method that [geth][import-raw-key] has implemented:
+
+ - personal_importRawKey
+
+[parity]: https://www.parity.io/
+[import-raw-key]: https://github.com/ethereum/go-ethereum/wiki/Management-APIs#personal_importrawkey
+[personal-api]: https://wiki.parity.io/JSONRPC-personal-module.html
+
+## Managing Permissions
+
+Two new endpoints will be added for general permissions management, and one
+existing endpoint will be extended to handle permissions. The new endpoints
+will be `seth_getPermissions` and `seth_setPermissions` and will allow general
+retrieval and setting of permissions.
+
+The existing `eth_sendTransaction` endpoint will be extended to allow a
+`permissions` argument.
+
+Finally, the proposed `personal_importRawKey` and `personal_newAccount`
+endpoints will accept a `permissions` argument as well.
+
+## Contract Chaining
+
+Sawtooth has the concept of a limited set of input and output address that a
+transaction is limited to reading from/writing to. That concept doesn't exist
+in Ethereum, and makes the concept of contract chaining (e.g. creating a
+contract from another contract) difficult. The `eth_sendTransaction` endpoint
+will be extended to allow an argument that sets the list of inputs and outputs
+to be the entire Sawtooth Seth address space, enabling general contract
+chaining.",,2018-10-29 14:36:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227602854,https://github.com/hyperledger/sawtooth-rfcs/pull/22#discussion_r227602854,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/22,https://github.com/hyperledger/sawtooth-rfcs/pull/22,"After talking with @aludvik, I think we can default to allowing contract chaining (i.e., transactions are simply limited to the seth address namespace). This is what Ethereum-based tools expect, and we can then tackle restricting the address space for contract chaining as a separate ticket. So, this section has been removed.",d6262942877b1cb488c93bc5703ebf72c4abe72e,2018-10-24 17:03:42,227877845,"@@ -0,0 +1,347 @@
+- Feature Name: seth\_rpc\_extensions
+- Start Date: 2018-07-30
+- RFC PR:
+- Sawtooth Issue:
+
+# Summary
+[summary]: #summary
+
+The Seth JSON-RPC API is an implementation of the Ethereum JSON-RPC API, used
+for cross-compatibility with existing Ethereum clients. The Ethereum JSON-RPC
+API lacks several areas of functionality that require the existing Seth client
+to talk directly to the Sawtooth REST API. This RFC proposes several JSON-RPC
+API extensions that will allow the CLI to become a thin client that builds on
+top of the `web3` library.
+
+# Motivation
+[motivation]: #motivation
+
+Effecting this RFC proposal will bring us in line with existing extensions to
+the Ethereum JSON-RPC API, allowing current Ethereum clients to work with Seth
+without modification.
+
+Moving the CLI to becoming a thin client will allow a simplified architecture
+that stores private keys in one place, and will drastically reduce code
+complexity. It will also eliminate the need for clients to understand how to
+talk to the Sawtooth REST API.
+
+An additional benefit will be that dogfooding our JSON-RPC API will ensure that
+the implementation is tested and up-to-date with other implementations.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Managing Accounts
+
+The [parity] project has faced the same issue of allowing account management
+via the [JSON-RPC API][personal-api] that it represents. This RFC proposes
+implementing a compatible API that presents these endpoints:
+
+ - personal_listAccounts
+ - personal_newAccount
+ - personal_unlockAccount
+ 
+Additionally, in order to facilitate import existing accounts, the API will
+present this method that [geth][import-raw-key] has implemented:
+
+ - personal_importRawKey
+
+[parity]: https://www.parity.io/
+[import-raw-key]: https://github.com/ethereum/go-ethereum/wiki/Management-APIs#personal_importrawkey
+[personal-api]: https://wiki.parity.io/JSONRPC-personal-module.html
+
+## Managing Permissions
+
+Two new endpoints will be added for general permissions management, and one
+existing endpoint will be extended to handle permissions. The new endpoints
+will be `seth_getPermissions` and `seth_setPermissions` and will allow general
+retrieval and setting of permissions.
+
+The existing `eth_sendTransaction` endpoint will be extended to allow a
+`permissions` argument.
+
+Finally, the proposed `personal_importRawKey` and `personal_newAccount`
+endpoints will accept a `permissions` argument as well.
+
+## Contract Chaining
+
+Sawtooth has the concept of a limited set of input and output address that a
+transaction is limited to reading from/writing to. That concept doesn't exist
+in Ethereum, and makes the concept of contract chaining (e.g. creating a
+contract from another contract) difficult. The `eth_sendTransaction` endpoint
+will be extended to allow an argument that sets the list of inputs and outputs
+to be the entire Sawtooth Seth address space, enabling general contract
+chaining.",,2018-10-29 14:36:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227877845,https://github.com/hyperledger/sawtooth-rfcs/pull/22#discussion_r227877845,knkski
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"Suggest a wording more like ""extends the original algorithm to new platforms""",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:32:00,204900854,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204900854,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204900854,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"Strictly speaking... it does give you BFT. The problem is that it is trivially easy to ""compromise"" nodes (so the 3f+1/2f+1 guarantees are easy to violate).",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:37:32,204902432,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204902432,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204902432,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,Suggest that you link to the full description of PoET v1.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:38:09,204902617,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:",107,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204902617,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204902617,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,There is no reason to compute the WaitTime in the enclave. Since the wait time is essentially meaningless to the enclave. All the other validators can do the conversion from the random number/duration into a time. That would allow you to just call Duration what it really is... a random number.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:39:11,204902941,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the",114,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204902941,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204902941,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,I would suggest that you describe how the wait time is computed from the random number. The computation is more or less the same as the computation from PoET v1.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:41:26,204903657,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network",125,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204903657,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204903657,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"This is pretty confusing. Duration is a random number. WaitTime is just a transformation of that random number based on a particular computation. In this case, the transformation ensures that d1 < d2 --> f(d1) < f(d2) where f is the conversion function.  That means that you could just talk about clocks or you could just talk about random numbers. But combining them makes this hard to read.",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:44:33,204904624,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204904624,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204904624,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,If you create the timer inside SGX you'll still need to verify that the parameters given were correct. Why compute WaitTime in the enclave? Just generate the random number and have each validator do the conversion.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:47:53,204905663,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` ",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204905663,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204905663,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"I think it would be clearer if you said ""a malicious actor can send the claim block earlier than the time"". ",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:49:13,204906074,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204906074,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204906074,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,see comments above. this information is redundant.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:49:55,204906329,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave",191,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204906329,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204906329,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"i'm confused. poet 1 doesn't store signup data, it stores the identity of the monotonic counter. we regenerate data on reboot because the protocol REQUIRES that the enclave re-register, not because there is no sealed storage. Sealed storage without a monotonic counter cannot prevent replay attacks (just copy an old version of sealed storage into place if you want to have multiple signups for the processor).",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:55:28,204908112,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A participant joins as a validator by downloading the PoET SGX enclave and a
+SPID certificate for the blockchain. The client side of the validator runs
+the following sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference between PoET 1.0 and PoET 2.0 is that PoET 2.0 does
+>not store the signup data in a sealed blob. Part of the reason for moving away 
+>from the sealed blob creation is the unavailability of the Monotonic counter 
+>(MTC) which provides certain security guarantees for sealed data stored to disk.
+>The implication of this change is that the signup data is lost each time the 
+>enclave is unloaded or the platform is restarted. The enclave has to register
+>afresh with a new set of keys each time it gets loaded.
+",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204908112,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204908112,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,This appears to be missing the two delays that are necessary. The first delay is the time between registration & use of the registration. The second is the delay between subsequent registrations for the processor.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 20:57:17,204908671,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A participant joins as a validator by downloading the PoET SGX enclave and a
+SPID certificate for the blockchain. The client side of the validator runs
+the following sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference between PoET 1.0 and PoET 2.0 is that PoET 2.0 does
+>not store the signup data in a sealed blob. Part of the reason for moving away 
+>from the sealed blob creation is the unavailability of the Monotonic counter 
+>(MTC) which provides certain security guarantees for sealed data stored to disk.
+>The implication of this change is that the signup data is lost each time the 
+>enclave is unloaded or the platform is restarted. The enclave has to register
+>afresh with a new set of keys each time it gets loaded.
+
+The server side of the validator runs the following sign-up procedure:
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up, ignore the join request, exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     ",373,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204908671,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204908671,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,Its probably worth receiving several blocks and computing an average over those blocks to ensure that you don't favor a low-latency neighbor or a cheating neighbor.,0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 21:02:46,204910220,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A participant joins as a validator by downloading the PoET SGX enclave and a
+SPID certificate for the blockchain. The client side of the validator runs
+the following sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference between PoET 1.0 and PoET 2.0 is that PoET 2.0 does
+>not store the signup data in a sealed blob. Part of the reason for moving away 
+>from the sealed blob creation is the unavailability of the Monotonic counter 
+>(MTC) which provides certain security guarantees for sealed data stored to disk.
+>The implication of this change is that the signup data is lost each time the 
+>enclave is unloaded or the platform is restarted. The enclave has to register
+>afresh with a new set of keys each time it gets loaded.
+
+The server side of the validator runs the following sign-up procedure:
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up, ignore the join request, exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     
+# Leader Election
+### Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the
+_Chain Clock_.
+
+The Wall Clock (`WC`) measures the time elapsed since the arrival of the
+'Sync Block', typically the first block on the chain. It is maintained by the
+validator on the 'untrusted' portion of the PoET code. The Chain Clock (`CC`)
+measures the cumulative `WaitTime` of all the blocks in the chain (each
+fork will have its own `CC`).
+
+Upon receiving the Sync Block, `WC` and `CC` are initialized to 0. 
+
+>Note 1: In practice, the WC may be calculated by recording the system time at 
+>the moment of the arrival of the Sync Block and subsequently subtracting this 
+>timestamp from the current time.
+",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204910220,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204910220,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,"it will be broadcast assuming that another, better claim is not received before the timer expires.",0ee016b78427c93548a20a603328597002b4c9b3,2018-07-24 21:04:40,204910785,"@@ -0,0 +1,566 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the IntelÂ® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an IntelÂ® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application’s address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: “Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation”. IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event, typically the first block arriving after a validator registers|
+|Chain Clock       | The cumulative wait times of all the blocks in the chain. Each fork has its own chain clock |
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the Wait Time of a block |
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a “trusted execution environment” to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof” algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a cumulative count of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator with the smallest `WaitCertificate.Duration` for a particular 
+	transaction block is elected the leader
+
++ One function, such as “CreateWaitCertificate”, takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as “CheckWaitCertificate”, verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase ""Proof of Elapsed Time"".
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a time and sets the timer OUTSIDE the SGX. A Claim Block is forwarded only
+after the timer has expired. The block is now accompanied by a `WaitCertificate` 
+from SGX containing the time that the block was expected to wait for.
+
+While this opens up the possibility of misuse by a malicious actor, the 
+community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A participant joins as a validator by downloading the PoET SGX enclave and a
+SPID certificate for the blockchain. The client side of the validator runs
+the following sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference between PoET 1.0 and PoET 2.0 is that PoET 2.0 does
+>not store the signup data in a sealed blob. Part of the reason for moving away 
+>from the sealed blob creation is the unavailability of the Monotonic counter 
+>(MTC) which provides certain security guarantees for sealed data stored to disk.
+>The implication of this change is that the signup data is lost each time the 
+>enclave is unloaded or the platform is restarted. The enclave has to register
+>afresh with a new set of keys each time it gets loaded.
+
+The server side of the validator runs the following sign-up procedure:
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up, ignore the join request, exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     
+# Leader Election
+### Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the
+_Chain Clock_.
+
+The Wall Clock (`WC`) measures the time elapsed since the arrival of the
+'Sync Block', typically the first block on the chain. It is maintained by the
+validator on the 'untrusted' portion of the PoET code. The Chain Clock (`CC`)
+measures the cumulative `WaitTime` of all the blocks in the chain (each
+fork will have its own `CC`).
+
+Upon receiving the Sync Block, `WC` and `CC` are initialized to 0. 
+
+>Note 1: In practice, the WC may be calculated by recording the system time at 
+>the moment of the arrival of the Sync Block and subsequently subtracting this 
+>timestamp from the current time.
+
+>Note 2: Notice that the CC is a function of the WaitTime, which is computed within
+>the enclave.
+
+### Block Publishing
+
+To publish a block in the election phase a validator runs the following procedure 
+on the client side:
+
+1. Start the PoET SGX enclave: ENC
+2. Assemble a Proposed Block (outside the enclave)
+3. Call `(waitCertificate, signature) = ENC.createWaitCertificate(blockDigest)`
+4. Wait `WaitCertificate.WaitTime` seconds
+5. Broadcast `(waitCertificate, signature, block, OPK, PPK)` over the
+   Gossip Network.
+
+   Here `block` is the transaction block identified by `blockDigest`.
+
+>Once a `WaitCertificate` is created, the enclave stores the `BlockNumber` and 
+>`WaitCertificate` in a table. The enclave only allows one `WaitCertificate` per
+>`BlockNumber`. Further, it uses the `LastBlockNumber` to ensure a `WaitCertificate` 
+> is created only for future blocks. This prevents an attacker from 'going back 
+>in time' to request a `WaitCertificate` for a past block.
+
+### Block Verification
+Upon receiving a Block from a peer over the Gossip Network, a server side
+validator first the block for eligibility before it can be allowed to
+participate in consensus.
+
+#### Block Eligibility Checks
+1. Verify the PPK and OPK belong to a registered validator by checking the
+   EndPoint registry.
+
+2. Verify the signature is valid using sender's PPK.
+
+3. Verify the PPK was used by sender to commit less than `K` blocks by checking 
+   EndPoint registry (otherwise sender needs to re-sign).
+
+4. Verify the `WaitCertificate.LocalMean` is correct by comparing
+   against `LocalMean` computed locally.
+
+5. Verify the waitCertificate.blockDigest is a valid ECDSA signature of the
+   SHA256 hash of block using OPK.
+
+6. Verify the sender has been winning elections according to the expected
+   distribution (see z-test documentation).
+
+7. Verify the sender signed up at least `c` committed blocks ago, i.e.,
+   respected the `c` block start-up delay.
+
+8. Verify that the block is not an early-arriving block by computing 
+`CC' = CC + WaitCertificate.WaitTime`, then doing `WC >= CC'`, where `CC` is the 
+existing Chain Clock of the fork being extended by the new block.
+
+>Implementation Note1: While checking for early arriving blocks, implementations
+>may choose to add a constant `E`, representing average delay in seconds due to 
+>network latency. The check will then be `WC >= CC' + E`
+
+>Implementation Note2: There may be cases where a block arrives earlier than 
+>its predecessors. In such cases, the block will need to be cached until its
+>predecessors arrive before it can be checked for Eligibility.
+
+A block passing all checks is considered 'Eligible' and proceeds to participate
+in consensus. It is also further broadcast over the Gossip network to
+neighboring peers.
+
+An early arriving block (where `WC < CC'`) is considered 'Ineligible'. The block
+is cached for `CC' - WC` seconds until it becomes 'Eligible'. It is then
+broadcast to neighboring peers over the Gossip Network.",,2018-08-25 09:25:32,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/204910785,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r204910785,cmickeyb
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,Not important that it's computed within the enclave. Only enclave trusted function is the RNG. If we compute WaitTime in the enclave it is for convenience / readability of having all the logic in one method. ,0ee016b78427c93548a20a603328597002b4c9b3,2018-08-31 14:49:14,214378801,"@@ -0,0 +1,582 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This RFC describes an extension to the PoET algorithm for supporting the Intel� 
+Software Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an Intel� Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application's address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: ""Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation"". IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event|
+|Chain Clock       | The sum of the wait times of all the blocks in the chain since the synchronization event. Each fork has its own chain clock. The Chain Clock is measured in seconds |
+|BaseTime	   | The timestamp of the synchronization event. BaseTime is used to subsequently compute Wall Clock by performing Wall Clock = CurrentTime - BaseTime|
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the WaitTime of a block |
+|WaitTime	   | Time in seconds for which the validator should wait before broadcasting the block. Also used in calculating the Chain Clock.
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a ""trusted execution environment"" to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof�? algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`. See 'CreateWaitCertificate' for details on WaitTime computation.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a sum of of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator publishing the block with the lowest CC and extending the longest
+		valid chain for a particular transaction block is elected the leader. The 
+		""Leader Election"" section describes this in greater detail.
+
++ One function, such as ""CreateWaitCertificate"", takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as ""CheckWaitCertificate"", verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase 'Proof of Elapsed Time'.
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a random 'duration', which is then used to set a timer OUTSIDE the SGX. 
+A Claim Block is forwarded only after the timer has expired. The block is now 
+accompanied by a `WaitCertificate` from SGX containing the 'duration' time that 
+the block was expected to wait for.
+
+While a malicious actor may send the claim block earlier than the assigned time, 
+the community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A new validator joins an existing network by downloading the PoET SGX enclave 
+and a SPID certificate for the blockchain. The validator then runs the following
+sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference from PoET 1.0 is that PoET 2.0 does not store any 
+>persistent data in a sealed blob. Part of the reason for moving away from the 
+>sealed blob creation is the unavailability of the Monotonic counter (MTC) which 
+>provides replay protection guarantees for sealed data stored to disk.
+
+Upon receiving a join request, validators already on the network run the following
+sign-up procedure:
+
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up within the last 'K' blocks, ignore 
+   the join request and exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     
+# Leader Election
+### Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the
+_Chain Clock_.
+
+The Wall Clock (`WC`) measures the time elapsed since the synchronization event.
+For the first validator, the synchronization event occurs when the genesis block
+is added as the first block in the chain.  For validators subsequently added to
+the network, the synchronization event occurs when the Synchronization block arrives 
+at the validator. See ""Bootstrapping new validator nodes"" for details. 
+The Wall Clock is maintained by the validator on the 'untrusted' portion of the
+PoET code. 
+
+The Chain Clock (`CC`) measures the cumulative `WaitTime` of all the blocks in 
+the chain (each fork will have its own `CC`).
+
+Upon receiving the Sync Block, `WC` and `CC` are initialized to 0. 
+
+>Note 1: In practice, the WC may be calculated by recording the system time
+>(`BaseTime`) at the moment of the synchronization event  and subsequently 
+>subtracting this timestamp from the current time (`WC = CurrentTime - BaseTime`). 
+
+>Note 2: Notice that the CC is a function of the WaitTime, which is computed within
+>the enclave.",397,2018-08-31 15:22:02,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/214378801,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r214378801,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,Verify wait time similarly,0ee016b78427c93548a20a603328597002b4c9b3,2018-08-31 14:51:10,214379395,"@@ -0,0 +1,582 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This RFC describes an extension to the PoET algorithm for supporting the Intel� 
+Software Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an Intel� Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application's address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: ""Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation"". IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event|
+|Chain Clock       | The sum of the wait times of all the blocks in the chain since the synchronization event. Each fork has its own chain clock. The Chain Clock is measured in seconds |
+|BaseTime	   | The timestamp of the synchronization event. BaseTime is used to subsequently compute Wall Clock by performing Wall Clock = CurrentTime - BaseTime|
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the WaitTime of a block |
+|WaitTime	   | Time in seconds for which the validator should wait before broadcasting the block. Also used in calculating the Chain Clock.
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a ""trusted execution environment"" to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof�? algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`. See 'CreateWaitCertificate' for details on WaitTime computation.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a sum of of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator publishing the block with the lowest CC and extending the longest
+		valid chain for a particular transaction block is elected the leader. The 
+		""Leader Election"" section describes this in greater detail.
+
++ One function, such as ""CreateWaitCertificate"", takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as ""CheckWaitCertificate"", verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase 'Proof of Elapsed Time'.
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a random 'duration', which is then used to set a timer OUTSIDE the SGX. 
+A Claim Block is forwarded only after the timer has expired. The block is now 
+accompanied by a `WaitCertificate` from SGX containing the 'duration' time that 
+the block was expected to wait for.
+
+While a malicious actor may send the claim block earlier than the assigned time, 
+the community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A new validator joins an existing network by downloading the PoET SGX enclave 
+and a SPID certificate for the blockchain. The validator then runs the following
+sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference from PoET 1.0 is that PoET 2.0 does not store any 
+>persistent data in a sealed blob. Part of the reason for moving away from the 
+>sealed blob creation is the unavailability of the Monotonic counter (MTC) which 
+>provides replay protection guarantees for sealed data stored to disk.
+
+Upon receiving a join request, validators already on the network run the following
+sign-up procedure:
+
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up within the last 'K' blocks, ignore 
+   the join request and exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     
+# Leader Election
+### Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the
+_Chain Clock_.
+
+The Wall Clock (`WC`) measures the time elapsed since the synchronization event.
+For the first validator, the synchronization event occurs when the genesis block
+is added as the first block in the chain.  For validators subsequently added to
+the network, the synchronization event occurs when the Synchronization block arrives 
+at the validator. See ""Bootstrapping new validator nodes"" for details. 
+The Wall Clock is maintained by the validator on the 'untrusted' portion of the
+PoET code. 
+
+The Chain Clock (`CC`) measures the cumulative `WaitTime` of all the blocks in 
+the chain (each fork will have its own `CC`).
+
+Upon receiving the Sync Block, `WC` and `CC` are initialized to 0. 
+
+>Note 1: In practice, the WC may be calculated by recording the system time
+>(`BaseTime`) at the moment of the synchronization event  and subsequently 
+>subtracting this timestamp from the current time (`WC = CurrentTime - BaseTime`). 
+
+>Note 2: Notice that the CC is a function of the WaitTime, which is computed within
+>the enclave.
+
+### Block Publishing
+
+To publish a block in the election phase a validator runs the following procedure: 
+
+1. Start the PoET SGX enclave: ENC
+2. Assemble a Proposed Block (outside the enclave)
+3. Call `(waitCertificate, signature) = ENC.createWaitCertificate(blockDigest)`
+4. Wait `WaitCertificate.WaitTime` seconds
+5. Broadcast `(waitCertificate, signature, block, OPK, PPK)` over the
+   Gossip Network.
+
+   Here `block` is the transaction block identified by `blockDigest`.
+
+>Once a `WaitCertificate` is created, the enclave stores the `BlockNumber` and 
+>`WaitCertificate` in a table. The enclave only allows one `WaitCertificate` per
+>`BlockNumber`. Further, it uses the `LastBlockNumber` to ensure a `WaitCertificate` 
+> is created only for future blocks. This prevents an attacker from 'going back 
+>in time' to request a `WaitCertificate` for a past block.
+
+### Block Verification
+Upon receiving a Block from a peer over the Gossip Network, a validator first 
+checks the block for eligibility before it can be allowed to participate in 
+consensus.
+
+#### Block Eligibility Checks
+1. Verify the PPK and OPK belong to a registered validator by checking the
+   EndPoint registry.
+
+2. Verify the signature is valid using sender's PPK.
+
+3. Verify the PPK was used by sender to commit less than `K` blocks by checking 
+   EndPoint registry (otherwise sender needs to re-sign).
+
+4. Verify the `WaitCertificate.LocalMean` is correct by comparing
+   against `LocalMean` computed locally.",433,2018-08-31 15:22:02,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/214379395,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r214379395,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/20,https://github.com/hyperledger/sawtooth-rfcs/pull/20,Some of the points in this section sound like they could be moved to/or repeated in the currently empty drawbacks section. ,0ee016b78427c93548a20a603328597002b4c9b3,2018-10-24 13:41:55,227793080,"@@ -0,0 +1,582 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This RFC describes an extension to the PoET algorithm for supporting the Intel� 
+Software Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an Intel� Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+| Term             | Definition |
+| ---------------- | ------------------------------------------- |
+|Enclave           | A protected area in an application's address space which provides confidentiality and integrity even in the presence of privileged malware. The term can also be used to refer to a specific enclave that has been initialized with a specific code and data.|
+|Basename          | A service provider base name. In our context the service provider entity is the distributed ledger network. Each distinct network should have its own Basename and Service Provider ID (see EPID and IAS specifications).|
+|EPID              | An anonymous credential system. See E. Brickell and Jiangtao Li: ""Enhanced Privacy ID from Bilinear Pairing for Hardware Authentication and Attestation"". IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust. 2010.|
+|EPID Pseudonym    | Pseudonym of an SGX platform used in linkable quotes.  It is part of the IAS attestation response according to IAS API specifications. It is computed as a function of the service Basename (validator network in our case) and the device's EPID private key.|
+|PPK, PSK          | PoET ECC public and private key created by the PoET enclave.|
+|IAS Report Key    | IAS public key used to sign attestation reports as specified in the current IAS API Guide.|
+|AEP               | Attestation evidence payload sent to IAS (see IAS API specifications). Contains JSON encodings of the quote and an optional nonce.|
+|AVR               | Attestation Verification Report, the response to a quote attestation request from the IAS. It is verified with the IAS Report Key. It contains a copy of the input AEP.|
+|`WaitCertId_{n}`  | The `n`-th or most recent WaitCertificate digest. We assume `n >= 0` represents the current number of blocks in the ledger. WaitCertId is a function of the contents of the Wait Certificate. For instance the SHA256 digest of the WaitCertificate ECDSA signature.|
+|OPK, OSK          | Originator ECDSA public and private key. These are the higher level ECDSA keys a validator uses to sign messages.|
+|OPKhash           | SHA256 digest of OPK|
+|blockDigest       | ECDSA signature with OSK of SHA256 digest of transaction block that the validator wants to commit.|
+|localMean         | Estimated wait time local mean.|
+|PoET\_MRENCLAVE   | Public MRENCLAVE (see SGX SDK documentation) value of valid PoET SGX enclave.|
+|`K`               | Number of blocks a validator can commit before having to sign-up with a fresh PPK.|
+|`c`               | The ""sign-up delay"", i.e., number of blocks a validator has to wait after sign-up before starting to participate in elections.|
+|MinDuration       | Minimum duration for a WaitTimer.|
+|Wall Clock        | The number of seconds elapsed since a synchronization event|
+|Chain Clock       | The sum of the wait times of all the blocks in the chain since the synchronization event. Each fork has its own chain clock. The Chain Clock is measured in seconds |
+|BaseTime	   | The timestamp of the synchronization event. BaseTime is used to subsequently compute Wall Clock by performing Wall Clock = CurrentTime - BaseTime|
+|Duration          | A 256 bit random number generated by the enclave and recorded in the WaitCertificate. The Duration influences the WaitTime of a block |
+|WaitTime	   | Time in seconds for which the validator should wait before broadcasting the block. Also used in calculating the Chain Clock.
+|BlockNumber       | A number indicative of the position of a block in the chain. It is also be known as the 'Block Depth'|
+|Proposed Block    | A block that is currently under construction and for whom a WaitCertificate has not yet been generated|
+|Claim Block       | A block whose construction is complete and for whom a WaitCertificate has been generated. A Claim Block announces a validator's candidature for the PoET Leader Election|
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Proof of Elapsed Time (PoET) Consensus method offers a solution to the
+Byzantine Generals Problem that utilizes a ""trusted execution environment"" to
+improve on the efficiency of present solutions such as Proof-of-Work. This
+specification defines a concrete implementation for SGX. The following
+presentation assumes the use of Intel SGX as the trusted execution environment. 
+
+At a high-level, PoET stochastically elects individual peers to execute requests
+at a given target rate. Individual peers sample an exponentially distributed
+random variable and wait for an amount of time dictated by the sample. The peer
+with the smallest sample wins the election. Cheating is prevented through the
+use of a trusted execution environment, identity verification and blacklisting
+based on asymmetric key cryptography, and an additional set of election
+policies.
+
+For the purpose of achieving distributed consensus efficiently,
+a good lottery function has several characteristics:
+
++ Fairness: The function should distribute leader election across the broadest 
+possible population of participants.
+
++ Investment: The cost of controlling the leader election process should be 
+proportional to the value gained from it.
+
++ Verification: It should be relatively simple for all participants to verify 
+that the leader was legitimately selected.
+
+PoET is designed to achieve these goals using new secure CPU instructions
+which are becoming widely available in consumer and enterprise processors.
+PoET uses these features to ensure the safety and randomness of the leader
+election process without requiring the costly investment of power and
+specialized hardware inherent in most “proof�? algorithms.
+
+Sawtooth includes an implementation which simulates the secure instructions.
+This should make it easier for the community to work with the software but
+also forgoes Byzantine fault tolerance.
+
+PoET 2.0 essentially works as follows:
+
++ Each validator requests a `WaitCertificate` from an enclave (a trusted 
+	function), corresponding to each block it wishes to publish. 
+
++ The `WaitCertificate` contains a `Duration` as well as a related `WaitTime`.
+	The `Duration` is a 256-bit random number generated using the secure
+	RNG available within the SGX. The `WaitTime` is derived from the
+	`Duration`. See 'CreateWaitCertificate' for details on WaitTime computation.
+
++ Every validator maintains two clocks, a WallClock (`WC`) - the
+	time elapsed since an initial synchronization event and a ChainClock
+        (`CC`) - a sum of of the wait times for all blocks in the
+        chain since the synchronization event
+	   
++ On the originating validator, the `WaitTime` is used to throttle broadcast of
+	claim blocks. Upon creating the `WaitCertificate`, the validator waits
+	until `WaitTime` seconds have elapsed before broadcasting the block over
+	the gossip network
+	
++ On peer validators, a block is eligible for consensus if the `CC` for its fork
+        does not exceed the validator's `WC`. If it does (e.g. for an
+        early-arriving block), the block is held back by `CC-WC` seconds until
+        it becomes eligible for consensus and broadcast over the gossip network
+  
++ The validator publishing the block with the lowest CC and extending the longest
+		valid chain for a particular transaction block is elected the leader. The 
+		""Leader Election"" section describes this in greater detail.
+
++ One function, such as ""CreateWaitCertificate"", takes a transaction block
+	 and creates a `WaitCertificate` that is guaranteed to have been created 
+	 by the enclave
+
++ Another function, such as ""CheckWaitCertificate"", verifies
+	that the `WaitCertificate` was created by the enclave
+
+The PoET leader election algorithm meets the criteria for a good lottery
+algorithm. It randomly distributes leadership election across the entire
+population of validators with distribution that is similar to what is
+provided by other lottery algorithms. The probability of election
+is proportional to the resources contributed (in this case, resources
+are general purpose processors with a trusted execution environment).
+An attestation of execution provides information for verifying that the
+certificate was created within the enclave. Further, the low cost of participation
+increases the likelihood that the population of validators will be large, increasing
+the robustness of the consensus algorithm.
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release
+a WaitCertificate, once the timer elapsed. To be considered a candidate for 
+consensus, a block (a.k.a Claim Block) would have to be accompanied by the 
+corresponding `WaitCertificate`. This meant that peers receiving a Claim Block 
+accompanied by the `WaitCertificate`, were assured that the block had not been 
+forwarded prematurely, hence the phrase 'Proof of Elapsed Time'.
+PoET 2.0, however, removes the reliance on internal timers and monotonic 
+counters within SGX. This necessitates a different approach for enforcing wait 
+times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it 
+with a random 'duration', which is then used to set a timer OUTSIDE the SGX. 
+A Claim Block is forwarded only after the timer has expired. The block is now 
+accompanied by a `WaitCertificate` from SGX containing the 'duration' time that 
+the block was expected to wait for.
+
+While a malicious actor may send the claim block earlier than the assigned time, 
+the community of validators is engaged to enforce the wait on the block. Blocks 
+arriving at peers earlier than suggested by their `WaitCertificate` are held back
+until the time is right to become eligible for consensus. The larger the size of
+the network, the more resistant the protocol is to attack by a malicious actor 
+or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail. We will also call out
+sections that are unchanged from PoET 1.0.
+
+## PoET enclave Data Structures
+The PoET enclave uses the following data structures:
+
+### WaitCertificate
+```
+WaitCertificate {
+    byte[32] Duration    # A random 256 bit number generated by SGX
+    double WaitTime      # The number of seconds to wait, as a function of the
+                         # Duration and the LocalMean
+    double LocalMean     # The computed local mean
+    byte[32] BlockID     # The BlockID passed in to the Enclave
+    byte[32] PrevBlockID # The BlockID of the previous block, as stored in the
+                         # previousWaitCertificate
+    uint32 BlockNumber   # The length of the chain
+    byte[32] TxnHash     # The hash of the transactions in the block
+    byte[] ValidatorID   # The ID of the current Validator
+    byte[64] Sign        # The signature of WaitCertificate computed over all
+                         # the fields using the PSK 
+  }
+```
+### Global state
+```
+uint32 LastBlockNumber              # The last blocknumber that the enclave has
+                                    # generated
+map<BlockNumber, WaitCertificate>   # Contains each wait certificate generated 
+                                    # by the enclave. Only one waitCertificate 
+                                    # will be allowed per block position across 
+                                    # all forks
+```
+## PoET enclave functions
+It exports the following functions:
+
+### generateSignUpData(OPKhash)
+
+**Returns**
+
+```
+    byte[64]  PPK
+    byte[432] report # SGX Report Data Structure
+```    
+**Parameters**
+
+```
+    byte[32] OPKhash # SHA256 digest of OPK
+```
+**Description**
+
+1. Generate fresh ECC key pair (PPK, PSK)
+2. Create SGX enclave report, store ``SHA256(OPKhash|PPK)`` in 
+``report_data`` field.
+3. Return (PPK, report).
+
+### createWaitCertificate(PreviousWaitCert, BlockDigest, ValidatorID)
+
+**Returns**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature # ECDSA PSK signature of waitCertificate
+```
+**Parameters**
+
+```
+    WaitCertificate PreviousWaitCert #The wait certificate of the current chain 
+                                     #head
+    byte[] blockDigest # ECDSA signature with originator private key of SHA256
+                       # digest of transaction block that the validator wants
+                       # to commit
+    byte[] ValidatorID
+```
+**Description**
+
+1. `if(PreviousWaitCert.BlockNumber < LastBlockNumber) then throw
+    StaleRequestException()`
+2. Generate 256 bit random `Duration`.
+3. Convert lowest 64-bits of `Duration` into double precision number 
+	in `[0, 1]` represented by `duration'`
+4. Compute `WaitTime = minimumDuration - localMean * log(duration')`.
+5. Create WaitCertificate object `waitCertificate =
+   WaitCertificate(WaitTimer, Duration, blockDigest)`
+6. Compute ECDSA signature of waitCertificate using PSK: `signature =
+   ECDSA_{PSK} (waitCertificate)`
+7. Return `(waitCertificate, signature)`
+    
+### checkWaitCertificate(WaitCertificate)
+**Returns**
+
+```
+    boolean isValid # The WaitCertificate passed all validity checks in Enclave
+```
+**Parameters**
+
+```
+    WaitCertificate waitCertificate
+    byte[64] signature
+    byte[]   block
+    byte[32] OPK
+    byte[32] PPK
+```
+## Initialization and Sign-up
+
+A new validator joins an existing network by downloading the PoET SGX enclave 
+and a SPID certificate for the blockchain. The validator then runs the following
+sign-up procedure:
+
+1. Start PoET SGX enclave: ENC.
+2. Generate sign-up data: `(PPK, report) =
+   {ENC.generateSignUpData(OPKhash)}` The ``report_data`` (512 bits)
+   field in the report body includes the SHA256 digest of (OPKhash | PPK).
+3. Ask SGX Quoting Enclave (QE) for linkable quote on the report (using the
+   validator network's Basename).
+4. If Self Attestation is enabled in IAS API: request attestation of linkable
+   quote to IAS. The AEP sent to IAS must contain:
+
+   * isvEnclaveQuote: base64 encoded quote
+   * nonce: `WaitCertId_{n}`
+
+   The IAS sends back a signed AVR containing a copy of the input AEP and the
+   EPID Pseudonym.
+
+5. If Self Attestation is enabled in IAS API: broadcast self-attested join
+   request, (OPK, PPK, AEP, AVR) to known participants.
+
+6. If Self Attestation is NOT enabled in IAS API: broadcast join request, (OPK,
+   PPK, quote) to known participants.
+
+A validator has to wait for `c` blocks to be published on the distributed
+ledger before participating in an election.
+
+>Note: 
+>An important difference from PoET 1.0 is that PoET 2.0 does not store any 
+>persistent data in a sealed blob. Part of the reason for moving away from the 
+>sealed blob creation is the unavailability of the Monotonic counter (MTC) which 
+>provides replay protection guarantees for sealed data stored to disk.
+
+Upon receiving a join request, validators already on the network run the following
+sign-up procedure:
+
+1. Wait for a join request.
+2. Upon arrival of a join request do the verification:
+
+   If the join request is self attested (Self Attestation is enabled in IAS
+   API): (OPK, PPK, AEP, AVR)
+	* Verify AVR legitimacy using IAS Report Key and therefore quote
+          legitimacy.
+	* Verify the ``report_data`` field within the quote contains the SHA256
+          digest of (OPKhash | PPK).
+	* Verify the nonce in the AVR is equal to `WaitCertId_{n}`, namely the
+          digest of the most recently committed block. It may be that the sender
+          has not seen `WaitCertId_{n}` yet and could be sending
+          `WaitCertId_{n'}` where `n'<n`.
+          In this case the sender should be urged to updated his/her
+          view of the ledger by appending the new blocks and retry. It could
+          also happen that the receiving validator has not seen
+          `WaitCertId_{n}` in which case he/she should try to update his/her
+          view of the ledger and verify again.
+	* Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there
+      could be more than one allowed value).
+	* Verify basename in the quote is equal to distributed ledger Basename.
+	* Verify attributes field in the quote has the allowed value (normally
+          the enclave must be in initialized state and not be a debug enclave).
+ 
+   If the join request is not self attested (Self Attestation is NOT enabled in
+   IAS API): (OPK, PPK, quote)
+
+   * Create AEP with quote:
+
+      * isvEnclaveQuote: base64 encoded quote
+
+ 3. Send AEP to IAS. The IAS sends back a signed AVR.
+ 4. Verify received AVR attests to validity of both quote and
+    save EPID Pseudonym.
+ 5. Verify ``report_data`` field within the quote contains the SHA256 digest of
+	(OPKhash | PPK).
+ 6. Verify MRENCLAVE value within quote is equal to PoET\_MRENCLAVE (there could
+	be more than one allowed value).
+ 7. Verify basename in the quote is equal to distributed ledger Basename.
+ 8. Verify attributes field in the quote has the allowed value (normally the
+    enclave must be in initialized state and not be a debug enclave).
+   
+   If the verification fails, exit.
+
+   If the verification succeeds but the SGX platform identified by the EPID
+   Pseudonym in the quote has already signed up within the last 'K' blocks, ignore 
+   the join request and exit.
+
+   If the verification succeeds:
+
+   * Pass sign-up certificate of new participant (OPK, EPID Pseudonym, PPK,
+     current `WaitCertId_{n}` to upper layers for registration in
+     EndPoint registry.
+   * Goto 1
+     
+# Leader Election
+### Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the
+_Chain Clock_.
+
+The Wall Clock (`WC`) measures the time elapsed since the synchronization event.
+For the first validator, the synchronization event occurs when the genesis block
+is added as the first block in the chain.  For validators subsequently added to
+the network, the synchronization event occurs when the Synchronization block arrives 
+at the validator. See ""Bootstrapping new validator nodes"" for details. 
+The Wall Clock is maintained by the validator on the 'untrusted' portion of the
+PoET code. 
+
+The Chain Clock (`CC`) measures the cumulative `WaitTime` of all the blocks in 
+the chain (each fork will have its own `CC`).
+
+Upon receiving the Sync Block, `WC` and `CC` are initialized to 0. 
+
+>Note 1: In practice, the WC may be calculated by recording the system time
+>(`BaseTime`) at the moment of the synchronization event  and subsequently 
+>subtracting this timestamp from the current time (`WC = CurrentTime - BaseTime`). 
+
+>Note 2: Notice that the CC is a function of the WaitTime, which is computed within
+>the enclave.
+
+### Block Publishing
+
+To publish a block in the election phase a validator runs the following procedure: 
+
+1. Start the PoET SGX enclave: ENC
+2. Assemble a Proposed Block (outside the enclave)
+3. Call `(waitCertificate, signature) = ENC.createWaitCertificate(blockDigest)`
+4. Wait `WaitCertificate.WaitTime` seconds
+5. Broadcast `(waitCertificate, signature, block, OPK, PPK)` over the
+   Gossip Network.
+
+   Here `block` is the transaction block identified by `blockDigest`.
+
+>Once a `WaitCertificate` is created, the enclave stores the `BlockNumber` and 
+>`WaitCertificate` in a table. The enclave only allows one `WaitCertificate` per
+>`BlockNumber`. Further, it uses the `LastBlockNumber` to ensure a `WaitCertificate` 
+> is created only for future blocks. This prevents an attacker from 'going back 
+>in time' to request a `WaitCertificate` for a past block.
+
+### Block Verification
+Upon receiving a Block from a peer over the Gossip Network, a validator first 
+checks the block for eligibility before it can be allowed to participate in 
+consensus.
+
+#### Block Eligibility Checks
+1. Verify the PPK and OPK belong to a registered validator by checking the
+   EndPoint registry.
+
+2. Verify the signature is valid using sender's PPK.
+
+3. Verify the PPK was used by sender to commit less than `K` blocks by checking 
+   EndPoint registry (otherwise sender needs to re-sign).
+
+4. Verify the `WaitCertificate.LocalMean` is correct by comparing
+   against `LocalMean` computed locally.
+
+5. Verify the waitCertificate.blockDigest is a valid ECDSA signature of the
+   SHA256 hash of block using OPK.
+
+6. Verify the sender has been winning elections according to the expected
+   distribution (see z-test documentation).
+
+7. Verify the sender signed up at least `c` committed blocks ago, i.e.,
+   respected the `c` block start-up delay.
+
+8. Verify that the block is not an early-arriving block by computing 
+`CC' = CC + WaitCertificate.WaitTime`, then doing `WC >= CC'`, where `CC` is the 
+existing Chain Clock of the fork being extended by the new block.
+
+>Implementation Note1: While checking for early arriving blocks, implementations
+>may choose to add a constant `E`, representing average delay in seconds due to 
+>network latency. The check will then be `WC >= CC' + E`
+
+>Implementation Note2: There may be cases where a block arrives earlier than 
+>its predecessors. In such cases, the block will need to be cached until its
+>predecessors arrive before it can be checked for Eligibility.
+
+A block passing all checks is considered 'Eligible' and proceeds to participate
+in consensus. It is also then broadcast over the Gossip network to the 
+validator's peers.
+
+An early arriving block (where `WC < CC'`) is considered 'Ineligible'. The block
+is cached for `CC' - WC` seconds until it becomes 'Eligible'. It is then
+broadcast to validator's peers over the Gossip Network.
+
+>In the absence of an enclave enforced block publishing delay as in PoET 1.0,
+>the Wall Clock (`WC`) acts as a community enforced block publishing delay.
+>Validators will only forward blocks when they can independently verify that
+>sufficient time has elapsed. 
+
+>Similar to PoET 1.0, the primary aim of this mechanism is to improve efficiency
+>of leader election. It aims at having a single claim outsanding at any given
+>time in the network.
+
+### Block Commit and Fork Resolution
+Once a validator has determined that an incoming block is Eligible, it performs
+the following steps:
+
+1. If the incoming block extends the current chain head:
+   * If the validator is still working on a Proposed Block (i.e. no Claim
+     Block has been created), commit the incoming block as the Chain Head, drop
+     the current proposed block. 
+   * If the validator has published a Claim Block, compare the 
+     `WaitCertificate.Duration` of both blocks. The block having the lower
+     Duration is the winner. If the incoming block is the winner, commit the 
+     incoming block and update the Chain Head.
+   * Check the Block Cache for any blocks that may claim the new Chain Head
+     as a parent (i.e. blocks that may have arrived out of order). If such 
+     blocks are identified, start the Block Eligibility checks on those blocks.
+	
+2. If the incoming block claims an earlier block as a parent, walk the chain back 
+   until a common parent is reached. This may involve pulling previously
+   discarded blocks out of the block cache. Check the chain length of each
+   fork.
+   * If the current (active) fork is longer, discard the incoming block
+   (implementations may choose to cache discarded blocks for some time)
+   * If the new fork is longer, switch to the new fork.
+   * If the two forks are of equal length
+     * compare the Chain Clock(`CC`) of each fork. The fork with the lower `CC`
+       is the winner. 
+     * If the Chain Clocks are equal, compare the `WaitCertificate.Duration` 
+       of the topmost blocks in the forks. The block with the lower Duration is
+       the winner.
+
+3. If the Chain Head has been updated, discard any existing Proposed Block and
+   start building a new Proposed Block.
+
+## Bootstrapping new validator nodes
+
+When a new validator joins the network, it submits a registration transaction.
+It then starts synchronizing its ledger and state with its neighbors by requesting
+blocks. 
+
+It then performs the following steps to bootstrap its Wall Clock:
+
+1. Start accepting incoming blocks and requesting missing blocks from neighbors 
+	to build the chain until the genesis block is received.
+2. When a block containing the validator's registration transaction is received,
+	start the 'C' block counter. Let this block be known as the 'registration' 
+	block.
+3. Pick a random block number between the 'registration' block and the C'th block.
+	We call this block the 'Synchronization' block, since it will be used to 
+	initialize and synchronize the two clocks. 
+4. To be considered a Synchronization block, the block must extend the current 
+	chain head.
+5. Once the Synchronization block is received, the WallClock & ChainClock are 
+	initialized. This block is henceforth used for ChainClock computation.
+6. Any incoming blocks before the C'th block are considered Eligible.
+7. The start of the validator's participation in consensus (after the C-block
+	delay) is also when the validator starts enforcing the Eligibility
+	checks on incoming blocks.
+
+This mechanism bootstraps the Wall Clock to a reasonable value and allows
+the validator to effectively participate in consensus. 
+
+## Clock Drifts and Network Latency
+
+PoET 2.0 relies on the system clock in two instances: for setting a timer to
+wait for `WaitCertificate.WaitTime` seconds and for maintaining the Wall Clock.
+
+Clock drifts on a validator may cause it to broadcast a block earlier or later
+than intended. On the peer validator handling incoming blocks, clock drifts will
+impact the Wall Clock being used to determine block eligibility. Depending on 
+the drift, blocks may become eligible earlier or later than ideal.
+
+The Leader Election mechanism itself is independent of the clock and will not be
+impacted by drifts.
+
+For forward clock drifts, the primary impact on the network will be of blocks
+being broadcast earlier. This could result in situations where multiple Claim Blocks
+are active at any given time, resulting in decreased efficiency of the consensus 
+mechanism. Negative clock drifts will cause blocks to be broadcast later, potentially
+impacting SLAs for block publishing intervals.
+
+Because Leader Election is isolated from clock drifts, the the clocks need only 
+be reasonably accurate to keep the network operating at the desired level of 
+efficiency. Existing system clock synchronization mechanisms like NTP
+etc. may be sufficient for PoET 2.0 requirements.
+
+Network latencies may be exploited by malicious nodes to broadcast blocks",558,2018-10-24 13:41:55,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/227793080,https://github.com/hyperledger/sawtooth-rfcs/pull/20#discussion_r227793080,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"I believe both the prepare and commit phases require (2f+1) messages to be received, thus if f nodes are faulty, f+1 are not.",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 15:23:20,198883313,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198883313,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198883313,ineffectualproperty
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,Should this 2f be (2f+1) for new_view in case f view_changes are byzantine?,cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 15:28:47,198885347,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198885347,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198885347,ineffectualproperty
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"Should we specify the data that needs to be included in a view_change method, which I believe is current node state (seq. no?) and pending requests.",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 15:36:23,198888104,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+
+7. If *block duration* has elapsed, then try to `summarize_block` with the
+   current working block. If the working block is not ready, (`BlockNotReady`
+   or `InvalidState` occurs), then nothing happens (call `ignore_block`).
+   Otherwise, `finalize_block` is called, and a new working block is created by
+   calling `initialize_block`.
+
+Messages passed during normal operation are roughly described by the following
+diagram:
+
+![PBFT operation](../images/pbft_sawtooth.png)
+
+One additional message type that may be encountered during the event loop is
+`BlockCommit`. This message type will cause the consensus algorithm to abort its
+current working block, and initialize a new one.
+
+### View Changes
+[view-changes]: #view-changes
+Sometimes, the node currently in charge (the primary) becomes faulty. In this
+case, a view change is necessary. View changes are triggered by a timeout:
+When a secondary node receives a `BlockNew` message, a timer is started. If
+the secondary ends up receiving a `commit_final` message, the timer is
+cancelled, and the algorithm proceeds as normal. If the timer expires, the
+primary node is considered faulty and a view change is initiated. This ensures
+Byzantine fault tolerance due to the fact that each step of the algorithm will
+not proceed to the next unless it receives a certain number of matching
+messages, and due to the fact that the Validator does not pass on any messages
+that have invalid signatures [[3]](#references).
+
+The view change process is as follows:
+1. Any node who discovers the primary as faulty (whose timer timed out) sends
+   a `view_change` message to all nodes.
+",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198888104,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198888104,ineffectualproperty
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"2f+1 here also I believe, which I guess is 'receiving' 2f and the +1 is your own local node.",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 15:36:43,198888222,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+
+7. If *block duration* has elapsed, then try to `summarize_block` with the
+   current working block. If the working block is not ready, (`BlockNotReady`
+   or `InvalidState` occurs), then nothing happens (call `ignore_block`).
+   Otherwise, `finalize_block` is called, and a new working block is created by
+   calling `initialize_block`.
+
+Messages passed during normal operation are roughly described by the following
+diagram:
+
+![PBFT operation](../images/pbft_sawtooth.png)
+
+One additional message type that may be encountered during the event loop is
+`BlockCommit`. This message type will cause the consensus algorithm to abort its
+current working block, and initialize a new one.
+
+### View Changes
+[view-changes]: #view-changes
+Sometimes, the node currently in charge (the primary) becomes faulty. In this
+case, a view change is necessary. View changes are triggered by a timeout:
+When a secondary node receives a `BlockNew` message, a timer is started. If
+the secondary ends up receiving a `commit_final` message, the timer is
+cancelled, and the algorithm proceeds as normal. If the timer expires, the
+primary node is considered faulty and a view change is initiated. This ensures
+Byzantine fault tolerance due to the fact that each step of the algorithm will
+not proceed to the next unless it receives a certain number of matching
+messages, and due to the fact that the Validator does not pass on any messages
+that have invalid signatures [[3]](#references).
+
+The view change process is as follows:
+1. Any node who discovers the primary as faulty (whose timer timed out) sends
+   a `view_change` message to all nodes.
+
+2. Once a server receives `2f` `view_change` messages, it changes its own view
+   to `v + 1` and sends a `new_view` message to all nodes. The new primary
+   node's ID is `p = v mod n`.
+",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198888222,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198888222,ineffectualproperty
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"This should be 3f+1, not 3n+1.",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 15:38:14,198888750,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+
+7. If *block duration* has elapsed, then try to `summarize_block` with the
+   current working block. If the working block is not ready, (`BlockNotReady`
+   or `InvalidState` occurs), then nothing happens (call `ignore_block`).
+   Otherwise, `finalize_block` is called, and a new working block is created by
+   calling `initialize_block`.
+
+Messages passed during normal operation are roughly described by the following
+diagram:
+
+![PBFT operation](../images/pbft_sawtooth.png)
+
+One additional message type that may be encountered during the event loop is
+`BlockCommit`. This message type will cause the consensus algorithm to abort its
+current working block, and initialize a new one.
+
+### View Changes
+[view-changes]: #view-changes
+Sometimes, the node currently in charge (the primary) becomes faulty. In this
+case, a view change is necessary. View changes are triggered by a timeout:
+When a secondary node receives a `BlockNew` message, a timer is started. If
+the secondary ends up receiving a `commit_final` message, the timer is
+cancelled, and the algorithm proceeds as normal. If the timer expires, the
+primary node is considered faulty and a view change is initiated. This ensures
+Byzantine fault tolerance due to the fact that each step of the algorithm will
+not proceed to the next unless it receives a certain number of matching
+messages, and due to the fact that the Validator does not pass on any messages
+that have invalid signatures [[3]](#references).
+
+The view change process is as follows:
+1. Any node who discovers the primary as faulty (whose timer timed out) sends
+   a `view_change` message to all nodes.
+
+2. Once a server receives `2f` `view_change` messages, it changes its own view
+   to `v + 1` and sends a `new_view` message to all nodes. The new primary
+   node's ID is `p = v mod n`.
+
+3. New primary node adds all the messages from  the `new_view` field
+   `pre_prepare_messages` into its message log and re-executes the PBFT
+   multi-cast protocol for each.
+
+Messages passed during a view change operation are shown in the following
+diagram:
+
+![PBFT View Changes](../images/pbft_view_change.png)
+
+### Garbage Collection
+[garbage-collection]: #garbage-collection
+After each *checkpoint period* (around 100 successfully completed cycles of
+the algorithm), server log messages can possibly be garbage-collected. When
+each node reaches a checkpoint, it sends out a `Checkpoint` message to all of
+the other servers, with that node's current state (described by a
+`PbftBlock`).  When the current node has `2f + 1` matching `Checkpoint`
+messages from different servers, the checkpoint is considered *stable* and the
+logs can be garbage collected: All log entries with sequence number less than
+the one in the `Checkpoint` message are discarded, and all previous
+checkpoints are removed. The high and low water marks are updated to reflect
+the sequence number of the new stable checkpoint.
+
+
+# Drawbacks
+[drawbacks]: #drawbacks
+One possible downside of BFT-style consensus algorithms is that in general,
+`3n + 1` nodes are needed on the network in order to preserve Byzantine fault",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198888750,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198888750,ineffectualproperty
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"Yes, this should definitely be `2f + 1` (including the message this node sent to itself)",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 16:23:58,198904024,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198904024,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198904024,bridger-herman
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"Prepare and Commit phases do indeed require `2f + 1` messages. The `commit_final` messages were adapted from `reply` messages in the original paper, of which the ""client"" only needs `f + 1`. Do you think this is a reasonable adaptation from the original into the context of blockchain?",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 16:29:20,198905621,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198905621,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198905621,bridger-herman
https://github.com/hyperledger/sawtooth-rfcs/pull/19,https://github.com/hyperledger/sawtooth-rfcs/pull/19,"The `view_change` messages include the sequence number, current view, proof of previous checkpoint, and pending requests - this is described under the ""Data Structures"" section, but I agree it would be good to remind readers here.",cf2ee86dfcd3c4aa1dcd172a27b7e3c42df9134d,2018-06-28 16:34:20,198907046,"@@ -0,0 +1,480 @@
+- Feature Name: pbft-consensus
+- Start Date: 2018-06-18
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+This RFC describes a practical Byzantine fault tolerant (PBFT) consensus
+algorithm for Hyperledger Sawtooth. The algorithm uses the Consensus API
+described in [a pending Sawtooth RFC][consensus_engine].
+
+
+# Motivation
+[motivation]: #motivation
+The motivation for this RFC is to add a new, voting-based consensus mechanism
+with Byzantine fault tolerance to the capabilities of Hyperledger Sawtooth.
+This PBFT consensus algorithm ensures safety and liveness of a network,
+provided at most `floor((n - 1)/3)` nodes are faulty, where `n` is the total
+number of nodes [[1]](#references). The PBFT algorithm is also inherently
+crash fault tolerant. Eventually, the base PBFT algorithm can be extended to
+other PBFT-style algorithms, which can potentially decrease the number of
+nodes in the network needed for it to be Byzantine fault tolerant, and reduce
+the total number of inter-node communication steps required.
+
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## The Byzantine Generals Problem
+Malicious users and code bugs can cause nodes in a network to exhibit
+[arbitrary (Byzantine) behavior][byzantine_behavior] [[1]](#references).
+Byzantine behavior can be described by nodes that send conflicting information
+to other nodes in the network [[2]](#references).
+
+**Example:**
+Consider a scenario where there is a binary decision to be made: ""to be"" or
+""not to be."" Say that there are nine nodes in a network. Four nodes send a
+""to be"" message to all the other nodes, and four send ""not to be"" to all other
+nodes. However, the last node is faulty and sends ""to be"" to half of the
+nodes, and ""not to be"" to the other half. Because of this faulty node, the
+network ends up in an existential crisis. The goal of a Byzantine fault
+tolerant system is to resolve crises like this.
+
+## Practical Byzantine Fault Tolerance
+Algorithms that attempt to conquer the aforementioned Byzantine faults are
+called Byzantine fault tolerant algorithms. Such algorithms include: Zyzzyva,
+RBFT, MinBFT, and PBFT. This RFC focuses on the base PBFT algorithm.
+
+Practical Byzantine Fault Tolerance (PBFT) is a consensus algorithm described
+by Miguel Castro and Barbara Liskov in 1999 [[1]](#references). This consensus
+algorithm is *voting*-based, meaning [[3]](#references):
++ Only a single node (the primary) can make commits at any given time
++ One or more nodes in the network maintains a global view of the network
++ Adding and removing nodes from the network is difficult
++ There are many peer-to-peer messages passed in between nodes which
+  specifically relate to consensus (see [Message Types](#message-types))
+
+In a general sense, PBFT works as follows:
+1. A client sends a message (request) to all the nodes in the network
+2. A series of messages is sent between nodes to determine if the request is
+  valid, or has been tampered with.
+3. Once a number of nodes agree that the request is valid, then the
+  instructions (operations) in the request are executed, and a result (reply)
+  is returned to the client.
+4. The client waits for a number of replies that match, then accepts the
+  result.
+
+## Terminology
+
+| Term              | Definition                                   |
+| ----------------: | :------------------------------------------- |
+| Node              | Machine running all the components necessary for a working blockchain (including the Validator, the REST API, a transaction processor, and the PBFT algorithm itself)|
+| Server            | Synonym for node |
+| Replica           | Synonym for node |
+| Block             | A part of the [blockchain](https://en.wikipedia.org/wiki/Blockchain), containing some operations and a pointer to the previous block |
+| Primary           | Node in charge of making the final consensus decisions and committing to the blockchain |
+| Secondary         | Auxiliary node used for consensus |
+| Client            | Machine that sends requests to and receives replies from the network of nodes |
+| Checkpoint        | Point in time where logs can get garbage collected |
+| Checkpoint period | How many client requests in between each checkpoint |
+| Block duration    | How many seconds to wait in between the creation of each block |
+| Message           | Block |
+| Working block     | The block that has been initialized but not finalized, and is currently being committed to |
+| Low water mark    | The sequence number of the last stable checkpoint |
+| High water mark   | Low water mark plus the desired maximum size of nodes' message log |
+| View              | The scope of PBFT when the current primary is in charge. The view changes when the primary is deemed faulty, as described in [View Changes](#view-changes) |
+| n                 | The total number of nodes in the network |
+| f                 | The maximum number of faulty nodes |
+| v                 | The current view number |
+| p                 | The primary server number; p = v mod n |
+
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+The PBFT consensus algorithm will be written in Rust, and will implement the
+`Engine` trait described in [the Consensus API][consensus_engine]
+[[3]](#references).
+
+## Data Structures
+
+### Consensus Messages
+[consensus-messages]: #consensus-messages
+These are the messages that will be sent from node to node which specifically
+relate to consensus. The content of all consensus-related messages are
+serialized using protobuf.
+
+From the Consensus API [[3]](#references):
+
+```
+// A consensus-related message sent between peers
+message ConsensusPeerMessage {
+  // Interpretation is left to the consensus engine implementation
+  string message_type = 1;
+
+  // The opaque payload to send to other nodes
+  bytes content = 2;
+
+  // Used to identify the consensus engine that produced this message
+  string name = 3;
+  string version = 4;
+}
+```
+
+Consensus messages sent by the PBFT algorithm will have one of the following
+types (contained in the `message_type` field of `ConsensusPeerMessage`):
+
++ `pre_prepare`
++ `prepare`
++ `commit`
++ `commit_final`
++ `checkpoint`
++ `view_change`
++ `new_view`
+
+### Message Types
+[message-types]: #message-types
+In order for PBFT to work correctly, peers on the network need to send a
+significant number of messages to each other. Most messages (`pre_prepare`,
+`prepare`, `commit`, and `commit_final`, and `checkpoint`) have similar
+contents, shown by `PbftMessage`. Auxiliary messages related to view changes
+(`view_change` and `new_view`) are also shown. Furthermore, PBFT uses some of
+the message types defined in the consensus API, such as blockchain-related
+messages like `BlockNew` and `BlockCommit`, and as the system update message
+`Shutdown`.
+
+The following Protobuf-style definitions are used to represent all
+consensus-related messages in the PBFT system:
+
+```
+// PBFT-specific block information (don't need to keep sending the whole payload
+// around the network)
+message PbftBlock {
+  bytes block_id = 1;
+
+  bytes signer_id = 2;
+
+  uint64 block_num = 3;
+
+  bytes summary = 4;
+}
+
+// Represents all common information used in a PBFT message
+message PbftMessageInfo {
+  // Type of the message
+  string msg_type = 1;
+
+  // View number
+  uint64 view = 2;
+
+  // Sequence number (helps with ordering log)
+  uint64 sequence_number = 3;
+
+  // Node who signed the message
+  bytes signer_id = 4;
+}
+```
+
+```
+// A generic PBFT message (pre_prepare, prepare, commit, commit_final,
+// Checkpoint)
+message PbftMessage {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // The actual message
+  PbftBlock block = 2;
+}
+```
+
+```
+// View change message, for when a node suspects the primary node is faulty
+message PbftViewChange {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Set of `2f + 1` checkpoint messages, proving correctness of stable
+  // checkpoint mentioned in info's `sequence_number`
+  repeated PbftMessage checkpoint_messages = 2;
+
+  message PrepareMessagePair {
+    PbftMessage pre_prepare_message = 1;
+
+    repeated PbftMessage prepare_messages = 2;
+  }
+
+  // `pre_prepare` message and `2f` `prepare` messages for each `BlockNew`
+  // message with a sequence number greater than `sequence_number`
+  // Used to help construct `PbftNewView` `pre_prepare_messages`
+  PrepareMessagePair prepare_messages = 3;
+}
+
+
+// New view message, for when there is consensus that the primary is faulty
+message PbftNewView {
+  // Message information
+  PbftMessageInfo info = 1;
+
+  // Valid `view_change` messages received by new primary and the original
+  // `view_change` message sent by the new primary
+  repeated PbftViewChange view_change_messages = 2;
+
+  // New set of `pre_prepare` messages for every sequence number in between
+  // the last stable checkpoint in `view_change_messages` and the highest
+  // sequence number in `view_change_messages`.
+  repeated PbftMessage pre_prepare_messages = 3;
+}
+```
+
+## Algorithm Operation
+The PBFT algorithm will operate within the framework described by the Consensus
+API [[3]](#references). The `start` method contains an event loop which
+handles all incoming messages, in the form of `Update`s. The most important
+form of `Update` to the functionality of PBFT is `Update::PeerMessage`, but
+other updates like `BlockNew`, `BlockCommit`, `BlockValid`, and `Shutdown` are
+considered.
+
+### Peer Messages
+When a message arrives from a peer, it must be interrogated for its type, and
+then the system must create a corresponding language-specific object of that
+message type. This is made easier by the fact that all consensus messages are
+protobuf-serialized. Generally, once a message is converted into an appropriate
+object, it needs to be checked for content to make sure everything is
+legitimate. Some of these checks are performed by the validator (checking that
+a message's signature is valid), but some (making sure messages match) need to
+be handled by the PBFT consensus engine.
+
+### Node Information Storage
+Nodes will keep track of the following information:
++ Their own id.
+
++ Log of every peer message that has been sent to it (used to determine if
+  nodes have received enough matching messages to proceed to the next stage of
+  the algorithm, can be [garbage collected](#garbage-collection) every so
+  often).
+
++ List of their connected peers.
+
++ Which step of the algorithm they're on.
+
+### Predicates
+In order to keep the algorithm explanation below concise, we'll define some
+predicates here.
+
++ `prepared` is true for the current node if the following messages are
+  present in its log:
+  + The original `BlockNew` message
+  + A `pre_prepare` message matching the original message (in the current view)
+  + `2f` matching `prepare` messages from different nodes that match
+    `pre_prepare` message above
+
++ `committed` is true if for the current node:
+  + `prepared` is true
+  + This node has accepted `2f + 1` `commit` messages, including its own
+
+### Normal Case Algorithm Operation
+#### Explanation of Message Types
++ `pre_prepare`: Sent from primary node to all nodes in the network, notifying
+them that a new message (`BlockNew`) has been received from the validator.
+
++ `prepare`: Sent from every node to every other node; used as verification of
+the pre-prepare message.
+
++ `commit`: Sent from every node to every other node; used to determine if there
+is consensus that we should indeed commit the block contained in the original
+message.
+
++ `commit_final`: Sent from all nodes back to the primary node; used as
+confirmation that the block should be committed.
+
++ `checkpoint`: Sent by any node that has completed `checkpoint_period`
+`commit_final` messages.
+
++ `view_change`: Sent by any node that suspects that the primary node is faulty.
+
++ `new_view`: Sent by any node that receives `2f` `view_change` messages.
+
+#### Initialization
+At the beginning of the Engine's `start` method, some initial setup is
+required:
++ Create the node for processing messages
++ Establish timers and counters for checkpoint periods and block durations
++ Initialize the first working block
+
+#### Event Loop
+In a normal context (when the primary node is not faulty), the PBFT consensus
+algorithm operates as follows, inside the event loop of the `start` method:
+
+1. Receive a `BlockNew` message from the validator, representative of a
+   client's request. The primary node checks the legitimacy of the message and
+   assigns this message a sequence number, then broadcasts a `pre_prepare`
+   message to all nodes. Legitimacy is checked by looking at the `signer_id`
+   of the block in the `BlockNew` message, and making sure the `previous_id`
+   is valid as the current chain head. Secondary nodes ignore `BlockNew`
+   messages; only append them to their logs.
+
+2. Receive `pre_prepare` messages and check their legitimacy. `pre_prepare`
+   messages are legitimate if:
+  + `signer_id` and `summary` of block inside `pre_prepare` match the
+    corresponding fields of the original `BlockNew` block &&
+  + View in `pre_prepare` message corresponds to this server's current view &&
+  + This message hasn't been accepted already with a different `summary` &&
+  + Sequence number is within the sequential bounds of the log (low and high
+    water marks)
+
+   Once the `pre_prepare` is accepted, broadcast a `prepare` message to all nodes.
+
+3. Receive `prepare` messages, and check them all against their associated
+   `pre_prepare` message in this node's message log.
+
+4. Once the predicate `prepared` is true for this node, then call
+   `check_block`. If an error occurs (`ReceiveError` or `UnknownBlock`),
+   abort (call `cancel_block`). Otherwise, wait for a response (`BlockValid`
+   or `BlockInvalid`) from the validator. If `BlockValid`, then broadcast a
+   `commit` message to all other nodes. If `BlockInvalid`, then call
+   `cancel_block`.
+
+5. When the predicate `committed` is true for this node, then send a
+   `commit_final` message back to the primary.
+
+6. Once the primary has accumulated `f + 1` `commit_final` messages, then
+   it should commit the block using `commit_block`, and advance the chain
+   head.
+
+7. If *block duration* has elapsed, then try to `summarize_block` with the
+   current working block. If the working block is not ready, (`BlockNotReady`
+   or `InvalidState` occurs), then nothing happens (call `ignore_block`).
+   Otherwise, `finalize_block` is called, and a new working block is created by
+   calling `initialize_block`.
+
+Messages passed during normal operation are roughly described by the following
+diagram:
+
+![PBFT operation](../images/pbft_sawtooth.png)
+
+One additional message type that may be encountered during the event loop is
+`BlockCommit`. This message type will cause the consensus algorithm to abort its
+current working block, and initialize a new one.
+
+### View Changes
+[view-changes]: #view-changes
+Sometimes, the node currently in charge (the primary) becomes faulty. In this
+case, a view change is necessary. View changes are triggered by a timeout:
+When a secondary node receives a `BlockNew` message, a timer is started. If
+the secondary ends up receiving a `commit_final` message, the timer is
+cancelled, and the algorithm proceeds as normal. If the timer expires, the
+primary node is considered faulty and a view change is initiated. This ensures
+Byzantine fault tolerance due to the fact that each step of the algorithm will
+not proceed to the next unless it receives a certain number of matching
+messages, and due to the fact that the Validator does not pass on any messages
+that have invalid signatures [[3]](#references).
+
+The view change process is as follows:
+1. Any node who discovers the primary as faulty (whose timer timed out) sends
+   a `view_change` message to all nodes.
+",,2018-08-03 18:50:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/198907046,https://github.com/hyperledger/sawtooth-rfcs/pull/19#discussion_r198907046,bridger-herman
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"Isn't this a regression, in that PoET 1.0 was specifically design to avoid talking to IAS frequently? In this sequence, a conversation with IAS is necessary between consensus engine restarts because the ECDSA key pair is not stored.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 14:52:20,190617030,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190617030,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190617030,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"If this RFC requires all nodes to sync time across the network to get correct behavior, this needs to be addressed here in detail, since it is both non-trivial and probably not realistic. No such assumptions are made in the current version of PoET.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 14:59:09,190619836,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.",97,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190619836,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190619836,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"What impact will this have on attempting to keep a PoET network running? Many of the current PoET tests are problematic in testing situations, or situations with low node counts; this would seem to fall into that category, and presumably would increase these problems. The existing PoET tests (checks would be a better word here) can essentially be disabled as necessary for testing purposes, but this sounds more fundamental.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 15:07:37,190622902,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.
+The claim/candidate block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a claim block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+Duration: A random 256 bit number generated by SGX
+WaitTime: The number of seconds to wait, determined by the LocalMean
+BlockID: The BlockID passed in to the Enclave
+PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCert
+TxnHash: The hash of the transactions in the block
+BlockNumber: The length of the chain
+ValidatorID: The ID of the current Validator
+Sign: The signature of the WaitCertificate computed over all the fields using the ECDSA private key
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190622902,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190622902,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"Check if there was a previous registration by what? by the same EPID Pseudonym maybe?
If K means the same thing in PoET2 as PoET1 (maximum registration life) I don't think we want to wait k blocks, because if the process dies, that machine can't re-register for potentially a really long time.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 19:23:30,190703125,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190703125,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190703125,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"Need a better name for the keys. Consider using the PoET1 terminology, e.g. PPK. The fact that it's an ECDSA key is ancillary. ",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 19:25:24,190703677,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. ",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190703677,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190703677,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"The phrase ""SGX PSE-free machines"" could be interpreted two different ways: 1) Machines without SGX and without PSE, or 2) Machines with SGX but without PSE.  My first interpretation was 1, so I was confused until later in the paper. 

The next section uses the term ""Platform Services"".  Would that work here?  If so, this phrase would be more clear if it was something like this: 

     SGX machines without Platform Services

or 

     machines without SGX Platform Services 

Otherwise, add ""Enclave (PSE)"" to the end of the phrase.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 20:54:11,190727545,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190727545,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190727545,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"And a minor note:  Best tech-writing practice is to use the full term ""Intel® Software Guard Extensions (SGX)"" here, because it's the first time the term is used.  But if that makes the sentence confusing or too long, it's probably okay to use ""SGX"" here, because the full term is in the next section.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:01:16,190729526,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190729526,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190729526,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"Nitpick: The term ""claim block"" is written inconsistently.  At first, it's ""Claim Block"". The rest of the paper uses ""claim block"", but there's also two occurrences of ""ClaimBlock"" without the space.

Technical writers prefer lower case (claim block), because it's easier to read. Plus, fewer terms would have to change. But ""Claim Block"" is acceptable as long as it's consistent.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:06:11,190730841,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.",74,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190730841,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190730841,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"I found ""the C test"" to be confusing.  Is it the same thing as Step 6 in the previous section? (""Wait for ‘C’ blocks to be added ..."").  Could you clarify here?",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:13:43,190732722,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190732722,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190732722,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"This information is hard to read in paragraph form.  I suggest using a bulleted list.  For example: 

     The contents of the WaitCertificate for PoET 2.0 are:

     * Duration: A random 256 bit number generated by SGX
     * WaitTime: The number of seconds to wait, determined by the LocalMean
     .
     .
     .

If you don't like bullets, you could simply add a blank line between each item so that each renders as its own paragraph.

The third choice is an RST term-definition list, but I always forget the syntax for that one. ",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:16:28,190733439,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.
+The claim/candidate block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a claim block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190733439,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190733439,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"Another nitpick: I suggest spelling out ""WaitCertificate"" (here and elsewhere) instead of abbreviating it in a few places. Consistency is good. :-) ",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:27:26,190736116,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.
+The claim/candidate block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a claim block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+Duration: A random 256 bit number generated by SGX
+WaitTime: The number of seconds to wait, determined by the LocalMean
+BlockID: The BlockID passed in to the Enclave
+PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCert
+TxnHash: The hash of the transactions in the block
+BlockNumber: The length of the chain
+ValidatorID: The ID of the current Validator
+Sign: The signature of the WaitCertificate computed over all the fields using the ECDSA private key
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.
+
+## Block Publishing
+With the ClaimBlock ready and the WaitCertificate created, the validator will determine the duration for which to set the timer. The duration of the wait is determined by the ‘Duration’ field in the WaitCertificate.
+Ideally, any selected algorithm should be capable of converting the duration to a wait time distributed uniformly between 0 and LocalMean seconds.
+
+## Block Verification
+Upon receiving a claim block and a WaitCertificate, the receiving validator does the following:
+### Sanity checks
+1.	Ensure the Block and WaitCertificate signatures are valid
+2.	The Block’s ID matches the WaitCert’s blockID",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190736116,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190736116,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"As with my first comment above, I suggest using the term ""(SGX) devices with/without Platform Services"" (or something similar), instead of ""PSE-free"" or ""PSE-enabled"".",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-24 21:32:35,190737355,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.
+The claim/candidate block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a claim block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+Duration: A random 256 bit number generated by SGX
+WaitTime: The number of seconds to wait, determined by the LocalMean
+BlockID: The BlockID passed in to the Enclave
+PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCert
+TxnHash: The hash of the transactions in the block
+BlockNumber: The length of the chain
+ValidatorID: The ID of the current Validator
+Sign: The signature of the WaitCertificate computed over all the fields using the ECDSA private key
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.
+
+## Block Publishing
+With the ClaimBlock ready and the WaitCertificate created, the validator will determine the duration for which to set the timer. The duration of the wait is determined by the ‘Duration’ field in the WaitCertificate.
+Ideally, any selected algorithm should be capable of converting the duration to a wait time distributed uniformly between 0 and LocalMean seconds.
+
+## Block Verification
+Upon receiving a claim block and a WaitCertificate, the receiving validator does the following:
+### Sanity checks
+1.	Ensure the Block and WaitCertificate signatures are valid
+2.	The Block’s ID matches the WaitCert’s blockID
+3.	If the block extends the current chain head, proceed to the Block Validity check. If it is a ‘future’ block, cache it for processing later.
+4.	If the block claims a previous block as the parent, proceed to ‘Fork Resolution’
+### Validity check & block publishing
+5.	Compute the new CC. The block is valid if CC + E < WC. Here E is a network latency coefficient added to simulate the average network delay seen by the validator.
+6.	Send the block to the validator for processing the transactions
+7.	If valid, commit the block, store the blockID and WaitCert in SGX
+8.	Forward the block to the gossip network
+9.	Drop the existing block proposal and restart from the new chain head.
+### Early Arriving blocks
+10.	If the block is invalid i.e. it is an early-arriving block, compare the duration with the duration of the claim block. If the block duration is smaller, drop the claim block. Wait for the WC to catch up with the CC. If the claim block duration is less than the incoming block duration, discard the incoming block.
+11.	Proceed to step 6 once the block becomes valid.
+### Fork Resolution
+12.	If the new block claims an earlier block as a parent, check for any cached blocks claiming the new block as a parent, retrieve them. Compute the new CC and check if the chain is valid (CC + E < WC)
+13.	If valid, compare existing chain length with the new chain length. If existing chain length is more, discard new chain. If the new chain is longer, switch to the new chain (commit the new blocks)
+14.	If the two chains are equal, compare Chain Clocks. Select the chain with the smaller CC.
+15.	If the chain is switched, drop the block proposal & restart (Step 9).
+16.	Forward the new block over the gossip network.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+Poet 2.0 improves on PoET 1.0 adding support for PSE free devices. Current PoET 1.0 limits the availability of PoET consensus only to SGX PSE enabled devices.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/190737355,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r190737355,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,PoET 2.0 address`es` these challenges by making the PoET algorithm independent of the _SGX Platform Services_.,4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-29 17:16:51,191504895,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/191504895,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r191504895,rranjan3
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,Should not this be a recursive call until a terminal child is reached within the cache? There could be a fork comprising of multiple (>1 block) in the cache.,4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-05-29 17:36:59,191511101,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.
+4. Check if there was a previous ECDSA key registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added.
+5. Register the ECDSA Public key and the IAS response with the ledger. 
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus.
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C test passes), it starts the process of creating claim blocks with the aim of participating in the Consensus mechanism.
+The claim/candidate block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a claim block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+Duration: A random 256 bit number generated by SGX
+WaitTime: The number of seconds to wait, determined by the LocalMean
+BlockID: The BlockID passed in to the Enclave
+PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCert
+TxnHash: The hash of the transactions in the block
+BlockNumber: The length of the chain
+ValidatorID: The ID of the current Validator
+Sign: The signature of the WaitCertificate computed over all the fields using the ECDSA private key
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.
+
+## Block Publishing
+With the ClaimBlock ready and the WaitCertificate created, the validator will determine the duration for which to set the timer. The duration of the wait is determined by the ‘Duration’ field in the WaitCertificate.
+Ideally, any selected algorithm should be capable of converting the duration to a wait time distributed uniformly between 0 and LocalMean seconds.
+
+## Block Verification
+Upon receiving a claim block and a WaitCertificate, the receiving validator does the following:
+### Sanity checks
+1.	Ensure the Block and WaitCertificate signatures are valid
+2.	The Block’s ID matches the WaitCert’s blockID
+3.	If the block extends the current chain head, proceed to the Block Validity check. If it is a ‘future’ block, cache it for processing later.
+4.	If the block claims a previous block as the parent, proceed to ‘Fork Resolution’
+### Validity check & block publishing
+5.	Compute the new CC. The block is valid if CC + E < WC. Here E is a network latency coefficient added to simulate the average network delay seen by the validator.
+6.	Send the block to the validator for processing the transactions
+7.	If valid, commit the block, store the blockID and WaitCert in SGX
+8.	Forward the block to the gossip network
+9.	Drop the existing block proposal and restart from the new chain head.
+### Early Arriving blocks
+10.	If the block is invalid i.e. it is an early-arriving block, compare the duration with the duration of the claim block. If the block duration is smaller, drop the claim block. Wait for the WC to catch up with the CC. If the claim block duration is less than the incoming block duration, discard the incoming block.
+11.	Proceed to step 6 once the block becomes valid.
+### Fork Resolution
+12.	If the new block claims an earlier block as a parent, check for any cached blocks claiming the new block as a parent, retrieve them. Compute the new CC and check if the chain is valid (CC + E < WC)",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/191511101,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r191511101,rranjan3
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"The signup has to happen every time the process starts/system reboots based on the K-test, similar to what we have in poet 1.0.",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-06-14 09:16:21,195352889,"@@ -0,0 +1,109 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient consensus mechanisms that will scale to large networks without putting potentially crippling demands on the infrastructure required to support it. Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm that not only scales very efficiently, but also places very few demands on the infrastructure.
+This document details a new mechanism for the PoET algorithm that overcomes some of the challenges with the original algorithm.
+The intended audience of this document is architects, developers and anyone who would like to get a better understanding of the details of the PoET 2.0 algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the SGX PSE-free machines.
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The secure timer and the monotonic counters used in PoET 1.0 rely on _SGX Platform Services_ to access these services from the system hardware. _SGX Platform Services_, unfortunately, are not yet available on all SGX enabled machines. While plans are in place for introducing these services universally across the processor lineup, it will take some time to release the services and for them to become ubiquitous. PoET 2.0 address these challenges by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A claim block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently.
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX.
+3. Generate a Quote containing the ECDSA public key and perform Quote verification with the IAS.",,2018-06-14 10:20:14,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/195352889,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r195352889,askmish
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"I have a question about this fork resolution process.  Can someone explain why we only need to compare chain lengths (rather than check the total number of ""work"" done on the chain--i.e., incorporate the population estimate)?

Here's my thought process:  suppose I controlled a small group of nodes.  We could could fork off everything to a side chain and then wait for the ""difficulty"" to go down.  In the steady state, due to the population adjustment, we would be adding blocks at the same rate as the main chain.  We could let this go on for a while--maybe we would get lucky and accumulate blocks at a slightly (sublinear) rate faster than the main chain.  Then, at some point, we could add in a bunch of new members and use these to add much more blocks than expected and try to catch up to the main chain.  It's possible there's some mathematical reasoning that says this is impossible, but it's not immediate (and needs to be written up if this is becomes the spec).

Does anyone have an explanation for why this doesn't work?  Am I misunderstanding something here?  Thanks!",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-09-03 16:54:57,214735598,"@@ -0,0 +1,161 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the Intel® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+*Enclave*
+
+A protected area in an application’s address space which provides 
+confidentiality and integrity even in the presence of privileged malware.
+
+The term can also be used to refer to a specific enclave that has been 
+initialized with a specific code and data.
+
+*EPID*
+
+An anonymous credential system. 
+
+*EPID Pseudonym*
+
+Pseudonym of an SGX platform used in linkable quotes. 
+It is part of the IAS attestation response according to IAS API 
+specifications. It is computed as a function of the service Basename 
+(validator network in our case) and the device’s EPID private key.
+
+*PPK, PSK*
+
+PoET ECC public and private key created by the PoET enclave.
+
+*IAS Report Key*
+
+IAS public key used to sign attestation reports as specified in the current 
+IAS API Guide.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A Claim Block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX
+3. Generate a Quote containing the PPK and perform Quote verification with the IAS
+4. Check if there was a previous PPK registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added. If the process restarts, signup at process startup and resume the K-test since this new signup
+5. Register the PPK and the IAS response with the ledger
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus(C-test)
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C-test passes), it starts the process of creating Claim Blocks with the aim of participating in the Consensus mechanism.
+The Claim/Candidate Block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a Claim Block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+
+* Duration: A random 256 bit number generated by SGX
+* WaitTime: The number of seconds to wait, determined by the LocalMean
+* BlockID: The BlockID passed in to the Enclave
+* PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCertificate
+* TxnHash: The hash of the transactions in the block
+* BlockNumber: The length of the chain
+* ValidatorID: The ID of the current Validator
+* Sign: The signature of the WaitCertificate computed over all the fields using the PSK 
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.
+
+## Block Publishing
+With the Claim Block ready and the WaitCertificate created, the validator will determine the duration for which to set the timer. The duration of the wait is determined by the ‘Duration’ field in the WaitCertificate.
+Ideally, any selected algorithm should be capable of converting the duration to a wait time distributed uniformly between 0 and LocalMean seconds.
+
+## Block Verification
+Upon receiving a Claim Block and a WaitCertificate, the receiving validator does the following:
+### Sanity checks
+1.	Ensure the Block and WaitCertificate signatures are valid
+2.	The Block’s ID matches the WaitCertificate’s blockID
+3.	If the block extends the current chain head, proceed to the Block Validity check. If it is a ‘future’ block, cache it for processing later.
+4.	If the block claims a previous block as the parent, proceed to ‘Fork Resolution’
+### Validity check & block publishing
+5.	Compute the new CC. The block is valid if CC + E < WC. Here E is a network latency coefficient added to simulate the average network delay seen by the validator.
+6.	Send the block to the validator for processing the transactions
+7.	If valid, commit the block, store the blockID and WaitCertificate in SGX
+8.	Forward the block to the gossip network
+9.	Drop the existing block proposal and restart from the new chain head.
+### Early Arriving blocks
+10.	If the block is an early-arriving block, compare the duration with the duration of the Claim Block. If the block duration is smaller, drop the Claim Block. Wait for the WC to catch up with the CC. If the Claim Block duration is less than the incoming block duration, discard the incoming block.
+11.	Proceed to step 6 once the block becomes valid.
+### Fork Resolution
+12.	If the new block claims an earlier block as a parent, check for any cached blocks claiming the new block as a parent, retrieve them. Compute the new CC and check if the chain is valid (CC + E < WC)
+13.	If valid, compare existing chain length with the new chain length. If existing chain length is more, discard new chain. If the new chain is longer, switch to the new chain (commit the new blocks)",141,2018-09-03 16:54:57,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/214735598,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r214735598,hartm
https://github.com/hyperledger/sawtooth-rfcs/pull/12,https://github.com/hyperledger/sawtooth-rfcs/pull/12,"@hartm  The newer spec for poet 2.0 is at https://github.com/hyperledger/sawtooth-rfcs/pull/20
",4fc6c815e1641ebe22f595fc3396f1bd19d857d2,2018-09-04 08:38:09,214830678,"@@ -0,0 +1,161 @@
+- Feature Name: PoET-2.0
+- Start Date: 2018-05-21
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+As blockchain technology matures, there is a pressing need for more efficient 
+consensus mechanisms that will scale to large networks without putting 
+potentially crippling demands on the infrastructure required to support it. 
+Proof of Elapsed Time (PoET) is an attempt at providing an innovative algorithm 
+that not only scales very efficiently, but also places very few demands on the 
+infrastructure.
+
+This document details a new mechanism for the PoET algorithm that overcomes 
+some of the challenges with the original algorithm.
+
+The intended audience of this document is architects, developers and anyone who 
+would like to get a better understanding of the details of the PoET 2.0 
+algorithm.
+
+This RFC describes a new PoET 2.0 Consensus for supporting the Intel® Software 
+Guard Extensions(SGX) machines without Platform Services Enclave(PSE).
+
+# Motivation
+[motivation]: #motivation
+
+PoET 1.0 offers a very unique and efficient way to achieve consensus based on 
+secure timers set inside an Intel® Software Guard Extensions (SGX) enclave. The 
+secure timer and the monotonic counters used in PoET 1.0 rely on 
+_SGX Platform Services_ to access these services from the system hardware. 
+_SGX Platform Services_, unfortunately, are not yet available on all SGX 
+enabled machines. While plans are in place for introducing these services 
+universally across the processor lineup, it will take some time to release the 
+services and for them to become ubiquitous. PoET 2.0 addresses these challenges 
+by making the PoET algorithm independent of the _SGX Platform Services_.
+
+# Definitions
+The following terms are used throughout the PoET spec and are defined here for 
+reference.
+
+*Enclave*
+
+A protected area in an application’s address space which provides 
+confidentiality and integrity even in the presence of privileged malware.
+
+The term can also be used to refer to a specific enclave that has been 
+initialized with a specific code and data.
+
+*EPID*
+
+An anonymous credential system. 
+
+*EPID Pseudonym*
+
+Pseudonym of an SGX platform used in linkable quotes. 
+It is part of the IAS attestation response according to IAS API 
+specifications. It is computed as a function of the service Basename 
+(validator network in our case) and the device’s EPID private key.
+
+*PPK, PSK*
+
+PoET ECC public and private key created by the PoET enclave.
+
+*IAS Report Key*
+
+IAS public key used to sign attestation reports as specified in the current 
+IAS API Guide.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+In PoET 1.0, the PoET SGX enclave set a timer internally and would only release a WaitCertificate, once the timer elapsed. To be considered a candidate for consensus, a block (a.k.a Claim Block) would have to be accompanied by the corresponding WaitCertificate. This meant that peers receiving a Claim Block accompanied by the WaitCertificate, were assured that the block had not been forwarded prematurely, hence the phrase ‘Proof of Elapsed Time’.
+PoET 2.0, however, removes the reliance on internal timers and monotonic counters within SGX. This necessitates a different approach for enforcing wait times. Rather than set a timer inside SGX, PoET 2.0 relies on SGX to provide it with a duration to wait and sets the timer OUTSIDE SGX. A Claim Block is forwarded only after the timer has expired. The block is now accompanied by a WaitCertificate from SGX containing the duration that the block was expected to wait for.
+While this opens up the possibility of misuse by a malicious actor, the community of validators is engaged to enforce the wait on the block. Blocks arriving at peers earlier than suggested by their WaitCertificate are held back until the time is right to forward them. The larger the size of the network, the more resistant the protocol is to attack by a malicious actor or a cabal.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+Here we describe the PoET 2.0 algorithm in greater detail.
+
+## Initialization
+When the PoET 2.0 module is initialized, it performs the following actions:
+1. Generate an ECDSA key pair. This pair is held in SGX memory and NEVER committed to disk for reuse subsequently
+2. Create a table in SGX memory to map the Block Number to WaitCertificates. Also store the block number of the chain head in SGX
+3. Generate a Quote containing the PPK and perform Quote verification with the IAS
+4. Check if there was a previous PPK registration. If there was, verify if ‘K’ blocks have been added to the chain since the key’s registration. If K blocks have not passed, wait until K blocks have been added. If the process restarts, signup at process startup and resume the K-test since this new signup
+5. Register the PPK and the IAS response with the ledger
+6. Wait for ‘C’ blocks to be added to the chain before participating in consensus(C-test)
+
+## Wall Clock and Chain Clock
+The PoET 2.0 algorithm maintains two clocks, the _Wall Clock_ and the _Chain Clock_.
+The Wall Clock (WC) measures the time elapsed since the arrival of the ‘Sync Block’, i.e. the first block on the chain.
+The Chain Clock (CC) measures the cumulative wait duration of all the blocks in the chain (each fork will have its own CC).
+Upon receiving the Sync Block, WC and CC are initialized to 0. In practice, the WC may be calculated by recording the system time at the moment of the arrival of the Sync Block and subsequently subtracting this timestamp from the current time.
+To handle clock drifts, it may become necessary to synchronize with a standard network clock. Details are out of scope for this version of the document.
+
+## Block Creation
+Once the algorithm is in a state to publish blocks (i.e. the C-test passes), it starts the process of creating Claim Blocks with the aim of participating in the Consensus mechanism.
+The Claim/Candidate Block is assembled as usual with no special processing required for PoET 2.0
+
+## Wait Certificate Creation
+Once a Claim Block is created, the PoET algorithm requests the PoET enclave to create the WaitCertificate.
+
+The contents of the WaitCertificate for PoET 2.0 are:
+
+* Duration: A random 256 bit number generated by SGX
+* WaitTime: The number of seconds to wait, determined by the LocalMean
+* BlockID: The BlockID passed in to the Enclave
+* PrevBlockID: The BlockID of the previous block, as stored in the previousWaitCertificate
+* TxnHash: The hash of the transactions in the block
+* BlockNumber: The length of the chain
+* ValidatorID: The ID of the current Validator
+* Sign: The signature of the WaitCertificate computed over all the fields using the PSK 
+
+Once the WaitCertificate is created, the enclave stores the Last Block # and the WaitCertificate in the table created previously. The validator will only create a single wait certificate per block. This implies that if a fork containing fewer blocks becomes active, the validator will NOT be able to publish any more blocks until the new fork has added enough blocks to catch up to the previous chain.
+
+## Block Publishing
+With the Claim Block ready and the WaitCertificate created, the validator will determine the duration for which to set the timer. The duration of the wait is determined by the ‘Duration’ field in the WaitCertificate.
+Ideally, any selected algorithm should be capable of converting the duration to a wait time distributed uniformly between 0 and LocalMean seconds.
+
+## Block Verification
+Upon receiving a Claim Block and a WaitCertificate, the receiving validator does the following:
+### Sanity checks
+1.	Ensure the Block and WaitCertificate signatures are valid
+2.	The Block’s ID matches the WaitCertificate’s blockID
+3.	If the block extends the current chain head, proceed to the Block Validity check. If it is a ‘future’ block, cache it for processing later.
+4.	If the block claims a previous block as the parent, proceed to ‘Fork Resolution’
+### Validity check & block publishing
+5.	Compute the new CC. The block is valid if CC + E < WC. Here E is a network latency coefficient added to simulate the average network delay seen by the validator.
+6.	Send the block to the validator for processing the transactions
+7.	If valid, commit the block, store the blockID and WaitCertificate in SGX
+8.	Forward the block to the gossip network
+9.	Drop the existing block proposal and restart from the new chain head.
+### Early Arriving blocks
+10.	If the block is an early-arriving block, compare the duration with the duration of the Claim Block. If the block duration is smaller, drop the Claim Block. Wait for the WC to catch up with the CC. If the Claim Block duration is less than the incoming block duration, discard the incoming block.
+11.	Proceed to step 6 once the block becomes valid.
+### Fork Resolution
+12.	If the new block claims an earlier block as a parent, check for any cached blocks claiming the new block as a parent, retrieve them. Compute the new CC and check if the chain is valid (CC + E < WC)
+13.	If valid, compare existing chain length with the new chain length. If existing chain length is more, discard new chain. If the new chain is longer, switch to the new chain (commit the new blocks)",141,2018-09-04 08:38:09,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/214830678,https://github.com/hyperledger/sawtooth-rfcs/pull/12#discussion_r214830678,askmish
https://github.com/hyperledger/sawtooth-rfcs/pull/8,https://github.com/hyperledger/sawtooth-rfcs/pull/8,"typo? ""...by replaced by...""",3ad14c545ae772b53fdee0ee374eddc8e53fd68c,2018-05-02 20:04:46,185622050,"@@ -0,0 +1,363 @@
+- Feature Name: state-pruning
+- Start Date: 2018-03-21
+- RFC PR: https://github.com/hyperledger/sawtooth-rfcs/pull/8
+- Sawtooth Issue: https://jira.hyperledger.org/browse/STL-1133
+
+# Summary
+[summary]: #summary
+
+This RFC describes a method for _State Pruning_.  State Pruning refers to the
+process of discarding older roots from the Merkle-Radix trie in an effort to
+manage growth of the State Database over time.
+
+# Motivation
+[motivation]: #motivation
+
+Currently, the State Database is allowed to grow in an unconstrained manner,
+consuming a large amount of disk space over the lifetime of a given ledger. This
+presents significant challenges to maintaining a ledger over a long period of
+time. Reducing the state database size will result in longer term stability of a
+running ledger.
+
+# Guide-level Explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Terms
+
+* **Parent** - A Merkle trie root hash, from which the current trie is produced.
+* **Succesor** - A Merkle trie which used the current trie as a parent.
+* **Change Log** - A record of additions for the current trie, as well as a
+  store of successors and the nodes that they deleted.
+    * A change log entry is made up of references to the *Parent*, the
+      *Additions* made when creating the current trie, and a map of
+      *Successors* and their corresponding *Deletions* of nodes in this trie.
+* **Pruning** - The operation of removing unneeded entries from parent or
+  successor tries.
+
+## Overview
+
+The Sawtooth Merkle-Radix trie implementation is a copy-on-write data structure.
+This means that only nodes that have changed, via updates or inserts, are added
+to the trie and hashed.  Unchanged nodes are not modified or hashed, only
+referenced by the modified nodes in the trie.  Deleted nodes are no longer
+referenced, which results in the parent nodes' hashes being updated.
+
+Along side the serialized node, a change log for a particular state root is
+stored.  This change log includes the keys of values that are added, and keys of
+deleted values by replaced by future state roots. The change log is written to",,2018-05-11 02:02:29,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185622050,https://github.com/hyperledger/sawtooth-rfcs/pull/8#discussion_r185622050,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/8,https://github.com/hyperledger/sawtooth-rfcs/pull/8," ""This change log includes the keys of values that are added, and keys of deleted values by replaced by future state roots."" Im sure what you mean by ""by replace by future state roots"", is this a typo?",3ad14c545ae772b53fdee0ee374eddc8e53fd68c,2018-05-03 18:15:15,185892478,"@@ -0,0 +1,363 @@
+- Feature Name: state-pruning
+- Start Date: 2018-03-21
+- RFC PR: https://github.com/hyperledger/sawtooth-rfcs/pull/8
+- Sawtooth Issue: https://jira.hyperledger.org/browse/STL-1133
+
+# Summary
+[summary]: #summary
+
+This RFC describes a method for _State Pruning_.  State Pruning refers to the
+process of discarding older roots from the Merkle-Radix trie in an effort to
+manage growth of the State Database over time.
+
+# Motivation
+[motivation]: #motivation
+
+Currently, the State Database is allowed to grow in an unconstrained manner,
+consuming a large amount of disk space over the lifetime of a given ledger. This
+presents significant challenges to maintaining a ledger over a long period of
+time. Reducing the state database size will result in longer term stability of a
+running ledger.
+
+# Guide-level Explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Terms
+
+* **Parent** - A Merkle trie root hash, from which the current trie is produced.
+* **Succesor** - A Merkle trie which used the current trie as a parent.
+* **Change Log** - A record of additions for the current trie, as well as a
+  store of successors and the nodes that they deleted.
+    * A change log entry is made up of references to the *Parent*, the
+      *Additions* made when creating the current trie, and a map of
+      *Successors* and their corresponding *Deletions* of nodes in this trie.
+* **Pruning** - The operation of removing unneeded entries from parent or
+  successor tries.
+
+## Overview
+
+The Sawtooth Merkle-Radix trie implementation is a copy-on-write data structure.
+This means that only nodes that have changed, via updates or inserts, are added
+to the trie and hashed.  Unchanged nodes are not modified or hashed, only
+referenced by the modified nodes in the trie.  Deleted nodes are no longer
+referenced, which results in the parent nodes' hashes being updated.
+
+Along side the serialized node, a change log for a particular state root is
+stored.  This change log includes the keys of values that are added, and keys of
+deleted values by replaced by future state roots. The change log is written to",,2018-05-03 20:38:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185892478,https://github.com/hyperledger/sawtooth-rfcs/pull/8#discussion_r185892478,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/8,https://github.com/hyperledger/sawtooth-rfcs/pull/8,"This link does not work. I get a ""Page not found""",3ad14c545ae772b53fdee0ee374eddc8e53fd68c,2018-05-03 18:16:11,185892765,"@@ -0,0 +1,363 @@
+- Feature Name: state-pruning
+- Start Date: 2018-03-21
+- RFC PR: https://github.com/hyperledger/sawtooth-rfcs/pull/8
+- Sawtooth Issue: https://jira.hyperledger.org/browse/STL-1133
+
+# Summary
+[summary]: #summary
+
+This RFC describes a method for _State Pruning_.  State Pruning refers to the
+process of discarding older roots from the Merkle-Radix trie in an effort to
+manage growth of the State Database over time.
+
+# Motivation
+[motivation]: #motivation
+
+Currently, the State Database is allowed to grow in an unconstrained manner,
+consuming a large amount of disk space over the lifetime of a given ledger. This
+presents significant challenges to maintaining a ledger over a long period of
+time. Reducing the state database size will result in longer term stability of a
+running ledger.
+
+# Guide-level Explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Terms
+
+* **Parent** - A Merkle trie root hash, from which the current trie is produced.
+* **Succesor** - A Merkle trie which used the current trie as a parent.
+* **Change Log** - A record of additions for the current trie, as well as a
+  store of successors and the nodes that they deleted.
+    * A change log entry is made up of references to the *Parent*, the
+      *Additions* made when creating the current trie, and a map of
+      *Successors* and their corresponding *Deletions* of nodes in this trie.
+* **Pruning** - The operation of removing unneeded entries from parent or
+  successor tries.
+
+## Overview
+
+The Sawtooth Merkle-Radix trie implementation is a copy-on-write data structure.
+This means that only nodes that have changed, via updates or inserts, are added
+to the trie and hashed.  Unchanged nodes are not modified or hashed, only
+referenced by the modified nodes in the trie.  Deleted nodes are no longer
+referenced, which results in the parent nodes' hashes being updated.
+
+Along side the serialized node, a change log for a particular state root is
+stored.  This change log includes the keys of values that are added, and keys of
+deleted values by replaced by future state roots. The change log is written to
+the database in the same LMDB transaction of the state root itself.  The deleted
+values are added to trie's change log when a successor state root is produced,
+and replaces keys with new values. An entry for deletions is recorded for all
+successor state roots.
+
+This change-log entry also includes the parent state-root.
+
+Pruning operates on a state root and examines the change-log entry for nodes to
+delete.
+
+In the case where a parent node is deleted, for example the state root
+of a block from some number in the past, the deleted values are examined.  If the
+intersection of all successor roots delete values is empty, then all of the
+deleted nodes listed are deleted from the database.
+
+In the case where a node with no successors - that is, the state root of an
+abandoned fork -  is pruned, the added keys are removed.  In this case, pruning
+can proceed back up the parent chain until it reaches a state root that has
+multiple successors in its delete collection.
+
+**Database Compaction**
+
+After a set number of pruning phases, the database has a large amount of
+reclaimed space, which needs to be compacted.  This is an additional step
+required to keep the overall file size smaller.  It is not an automatic process
+for LMDB.
+
+This compaction operation will be available as a subcommand of `sawadm`.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Configuration
+
+Maximum block depth is a local configuration option.  It is set via the command
+line parameter `--state-pruning-block-depth` and defaults to `1000`.
+
+This is a local setting, as pruning helps manage local resource constraints.
+Network-wide settings have no knowledge of local disk size issues.
+
+## Change Log Storage
+
+Change logs are stored in a child database, `change_log`, which will record
+change-logs in a protobuf message.  Note, the hashes are stored as bytes, not
+hex representations.
+
+```
+// An Entry in the change log for a given state root
+message ChangeLogEntry {
+    // A state root that succeed this root
+    message Successor {
+        // A root hash of a merkle trie based of this root
+        bytes successor = 1;
+
+        // The keys (i.e. hashes) that were replaced (i.e. deleted) by this
+        // successor.  These may be deleted during pruning.
+        repeated bytes deletions = 2;
+    }
+
+    // A root hash of a merkle trie this tree was based off
+    bytes parent = 1;
+
+    // The hashes that were added for this root. These may be deleted during
+    // pruning, if this root is being abandoned.
+    repeated bytes additions = 2;
+
+    // The list of successors
+    repeated Successor successors = 3;
+}
+```
+
+These entries are updated in the child database during the update of the main
+state database, within the same write transaction.  This preserves the
+atomicity of the writes.
+
+## Updating Entries
+
+Collecting the hashes requires the following changes to the existing algorithm:
+
+```
+         # Rebuild the hashes to the new root
++        additions = []
++        deletions = [self._root_hash] # add the old parent
+         for path in sorted(path_map, key=len, reverse=True):
+             (key_hash, packed) = _encode_and_hash(path_map[path])
+             update_batch.append((key_hash, packed))
++            additions.append(key_hash)
+             if path != '':
+                 parent_address = path[:-TOKEN_SIZE]
+                 path_branch = path[-TOKEN_SIZE:]
++                if path_branch in path_map[parent_address]['c']:
++                    deletions.append(
++                       path_map[parent_address]['c'][path_branch])
+                 path_map[parent_address]['c'][path_branch] = key_hash
++        additions.append(key_hash) # add the new parent
+```
+
+Storing the change log requires the following additions (in pseudo code):
+
+```
+    change_log = ChangeLogEntry(
+        parent=self._root_hash,
+        addition=additions)
+    parent_change_log = read_change_log(self._root_hash)
+    parent_change_log.append(Successor(
+        successor=key_hash,
+        deletions=deletions))
+    write_change_logs(parent_change_log, change_log)
+```
+
+This operation is only needed if the state root is going to be persisted.
+
+## Pruning
+
+A state root hash selected for pruning undergoes the following operation (in
+pseudo code):
+
+```
+fn prune(state_root_hash: String, change_log: Option<ChangeLogEntry>) -> bool:
+    if not change_log:
+        change_log = read_change_log(state_root_hash)
+
+    if change_log.successors.len() > 1:
+        # cannot prune: it is the root of multiple branches
+        return False
+
+    # pruning an abandoned succesor
+    if change_log.successors.len() == 0:
+        for hash in change_log.additions:
+            delete(hash)
+
+        if change_log.parent:
+            parent_change_log = read_change_log(change_log.parent)
+            delete_successor(parent_change_log, state_root_hash)
+
+            // prune until we hit the fork
+            if parent_change_log.successor.len() == 0:
+                prune(parent, parent_change_log)
+
+        delete_change_log(state_root_hash)
+        return True
+    else:
+        # pruning an old root
+        successor = change_log.successors[0]
+        for hash in successor.deletions:
+            delete(hash)
+
+        delete_change_log(state_root_hash)
+        return True
+```
+
+If these state roots are thought of from the perspective of a time dimension,
+where time can fork based on different changes applied to the previous trie,
+either
+
+* A root is pruned by removing those deleted forward in time
+* A root is pruned by removing those added, and its parents, until the fork in
+  time is found
+* A root is untouched, as it is the root of multiple forks in time
+
+### When to Prune
+
+After committing a block, the state root for block `Head - N`, where `N =
+<maximum block depth>`, is put in a queue for pruning.  This queue is persisted,
+so as not to lose the state roots slated for removal.  Likewise, when forks are
+abandoned, and when they fall out of the cache, their state roots are put in the
+pruning queue.
+
+If a fork is re-established as part of the current chain, it hash is removed
+from the pruning queue.
+
+After the pruning queue is updated, and the queue has reached its max size, the
+oldest item in the queue is removed and it is pruned.
+
+## Example
+
+![Merkle Database Example: Before](../images/state_pruning_before.svg)
+
+The above example displays a merkle database with a single state root that
+consists of three leaf values, and two intermediate nodes.  The data is omitted
+as it is irrelevant to this discussion.
+
+The change-log entry for this state root lists all the nodes as added, no parent
+(it is the first root in the database), and no deletions (it has no successors).
+
+If a new root is produced by modifying the value stored at `Leaf3`, the database
+will now look like:
+
+![Merkle Database Example: After](../images/state_pruning_after.svg)
+
+If state pruning was to occur against the original `Root` (`123...abc`) the
+following operations would occur:
+
+- Change-log for `Root` is read
+- Since there is only one successor all keys referenced in the
+  `ChangeLogEntry.Deleted` are deleted
+    - Delete `123...abc1`, `eef...414`, `919...523`
+- Change-log for `Root` is deleted
+- Change-log for `Root'` (`774..def`) is read
+    - ChangeLogEntry.Parent is set to `None`
+
+In a running Sawtooth validator, state pruning would occur against state roots
+that are either a certain depth in the trie, or abandoned forks.  The block
+maximum block depth is configurable, and will default to `1000`.
+
+## Effect on Block Validation
+
+State pruning will have an effect on Block Validation, in that it will be
+expensive to validate forks that are against blocks whose state has been pruned.
+This is due to needing to recompute state up to the common ancestor of the
+block. This expense is roughly mitigated by the setting of maximum block depth,
+and future changes can make maximum block depth based off of a computed average
+of forks and their depth.
+
+## Database Compaction
+
+Database Compaction in LMDB is done via a copy operation. This is processed via
+the underlying LMDB api:
+
+```
+int mdb_env_copy2(MDB_env* env,
+                  const char* path,
+                  unsigned int flags)		
+```
+
+where the flag `MDB_CP_COMPACT` is set.  
+
+This operation writes the database to a new path, and returns a new LMDB
+Environment.  The environment can be swapped for the existing environment.
+
+LMDB can perform this operation to be applied while still allowing writes to
+happen against the original DB. The complexity with this operation is involved
+around when to switch from the pre-compacted to the post-compacted database.
+
+See the
+[documenation](http://www.lmdb.tech/doc/group__mdb.html#ga3bf50d7793b36aaddf6b481a44e24244)
+on the function for more detail.
+
+This operation is implemented as part of the `sawadm` tool:
+
+```
+$ sawadm state compact
+```
+
+It must be run while the validator is stopped.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+The storage of the change-log increases the amount of data per trie by
+duplicating the keys. This is the trade-off for making both updating and pruning
+the database a fast operation.
+
+Additionally, changing the LMDB database from a single database to two child
+databases requires a migration operation. This migration can be done by
+revalidating the chain and processing the new state database with pruning
+enabled.  While slow for large changes, the end result would have a much smaller
+disk size.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+**Reference Counting**
+
+Reference counting involves marking each node with a reference count for each
+parent node.  As nodes are added, reference counts are incremented for the
+remaining siblings.  During pruning, reference counts are decremented, and any
+node with a reference count of zero is deleted.
+
+While this is relatively simple, algorithmically - particularly with respect to
+the prune algorithm - it requires a large number of writes per update.  Worst
+case, this could involve a visit to every node in the trie.
+
+**Minimal Copy**
+
+An alternative to reference counting is to create a minimal copy of the database
+at intervals. As the db grows to a particular size, copy the database starting
+at a given root, and then apply the remaining deltas to the new database. At the
+end of the copy, switch the reference to the underlying database for use in
+normal operation and discard the old database.
+
+This particular solution is simple in it's implementation, and requires no
+change in the underlying data storage model.  However, it is an expensive
+operation, given that a complete copy, including a walk of every trie kept is
+required.
+
+# Prior art
+[prior-art]: #prior-art
+
+Ethereum Geth has also discussed the use of state pruning for their Merkle trie
+implemenation. It has not yet implemented this for its disk-based storage, but
+it has implemented reference counting for its in-memory storage of the trie. See
+this [blog post](https://blog.ethereum.org/2015/06/26/state-trie-pruning/) for",,2018-05-03 20:38:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185892765,https://github.com/hyperledger/sawtooth-rfcs/pull/8#discussion_r185892765,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/8,https://github.com/hyperledger/sawtooth-rfcs/pull/8,Will this be added to the validator.toml file as well? or only on the command line?,3ad14c545ae772b53fdee0ee374eddc8e53fd68c,2018-05-03 18:53:17,185902999,"@@ -0,0 +1,363 @@
+- Feature Name: state-pruning
+- Start Date: 2018-03-21
+- RFC PR: https://github.com/hyperledger/sawtooth-rfcs/pull/8
+- Sawtooth Issue: https://jira.hyperledger.org/browse/STL-1133
+
+# Summary
+[summary]: #summary
+
+This RFC describes a method for _State Pruning_.  State Pruning refers to the
+process of discarding older roots from the Merkle-Radix trie in an effort to
+manage growth of the State Database over time.
+
+# Motivation
+[motivation]: #motivation
+
+Currently, the State Database is allowed to grow in an unconstrained manner,
+consuming a large amount of disk space over the lifetime of a given ledger. This
+presents significant challenges to maintaining a ledger over a long period of
+time. Reducing the state database size will result in longer term stability of a
+running ledger.
+
+# Guide-level Explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Terms
+
+* **Parent** - A Merkle trie root hash, from which the current trie is produced.
+* **Succesor** - A Merkle trie which used the current trie as a parent.
+* **Change Log** - A record of additions for the current trie, as well as a
+  store of successors and the nodes that they deleted.
+    * A change log entry is made up of references to the *Parent*, the
+      *Additions* made when creating the current trie, and a map of
+      *Successors* and their corresponding *Deletions* of nodes in this trie.
+* **Pruning** - The operation of removing unneeded entries from parent or
+  successor tries.
+
+## Overview
+
+The Sawtooth Merkle-Radix trie implementation is a copy-on-write data structure.
+This means that only nodes that have changed, via updates or inserts, are added
+to the trie and hashed.  Unchanged nodes are not modified or hashed, only
+referenced by the modified nodes in the trie.  Deleted nodes are no longer
+referenced, which results in the parent nodes' hashes being updated.
+
+Along side the serialized node, a change log for a particular state root is
+stored.  This change log includes the keys of values that are added, and keys of
+deleted values by replaced by future state roots. The change log is written to
+the database in the same LMDB transaction of the state root itself.  The deleted
+values are added to trie's change log when a successor state root is produced,
+and replaces keys with new values. An entry for deletions is recorded for all
+successor state roots.
+
+This change-log entry also includes the parent state-root.
+
+Pruning operates on a state root and examines the change-log entry for nodes to
+delete.
+
+In the case where a parent node is deleted, for example the state root
+of a block from some number in the past, the deleted values are examined.  If the
+intersection of all successor roots delete values is empty, then all of the
+deleted nodes listed are deleted from the database.
+
+In the case where a node with no successors - that is, the state root of an
+abandoned fork -  is pruned, the added keys are removed.  In this case, pruning
+can proceed back up the parent chain until it reaches a state root that has
+multiple successors in its delete collection.
+
+**Database Compaction**
+
+After a set number of pruning phases, the database has a large amount of
+reclaimed space, which needs to be compacted.  This is an additional step
+required to keep the overall file size smaller.  It is not an automatic process
+for LMDB.
+
+This compaction operation will be available as a subcommand of `sawadm`.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Configuration
+
+Maximum block depth is a local configuration option.  It is set via the command",82,2018-05-03 20:38:44,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185902999,https://github.com/hyperledger/sawtooth-rfcs/pull/8#discussion_r185902999,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/8,https://github.com/hyperledger/sawtooth-rfcs/pull/8,Added some text for this.,3ad14c545ae772b53fdee0ee374eddc8e53fd68c,2018-05-03 20:39:13,185929463,"@@ -0,0 +1,363 @@
+- Feature Name: state-pruning
+- Start Date: 2018-03-21
+- RFC PR: https://github.com/hyperledger/sawtooth-rfcs/pull/8
+- Sawtooth Issue: https://jira.hyperledger.org/browse/STL-1133
+
+# Summary
+[summary]: #summary
+
+This RFC describes a method for _State Pruning_.  State Pruning refers to the
+process of discarding older roots from the Merkle-Radix trie in an effort to
+manage growth of the State Database over time.
+
+# Motivation
+[motivation]: #motivation
+
+Currently, the State Database is allowed to grow in an unconstrained manner,
+consuming a large amount of disk space over the lifetime of a given ledger. This
+presents significant challenges to maintaining a ledger over a long period of
+time. Reducing the state database size will result in longer term stability of a
+running ledger.
+
+# Guide-level Explanation
+[guide-level-explanation]: #guide-level-explanation
+
+## Terms
+
+* **Parent** - A Merkle trie root hash, from which the current trie is produced.
+* **Succesor** - A Merkle trie which used the current trie as a parent.
+* **Change Log** - A record of additions for the current trie, as well as a
+  store of successors and the nodes that they deleted.
+    * A change log entry is made up of references to the *Parent*, the
+      *Additions* made when creating the current trie, and a map of
+      *Successors* and their corresponding *Deletions* of nodes in this trie.
+* **Pruning** - The operation of removing unneeded entries from parent or
+  successor tries.
+
+## Overview
+
+The Sawtooth Merkle-Radix trie implementation is a copy-on-write data structure.
+This means that only nodes that have changed, via updates or inserts, are added
+to the trie and hashed.  Unchanged nodes are not modified or hashed, only
+referenced by the modified nodes in the trie.  Deleted nodes are no longer
+referenced, which results in the parent nodes' hashes being updated.
+
+Along side the serialized node, a change log for a particular state root is
+stored.  This change log includes the keys of values that are added, and keys of
+deleted values by replaced by future state roots. The change log is written to
+the database in the same LMDB transaction of the state root itself.  The deleted
+values are added to trie's change log when a successor state root is produced,
+and replaces keys with new values. An entry for deletions is recorded for all
+successor state roots.
+
+This change-log entry also includes the parent state-root.
+
+Pruning operates on a state root and examines the change-log entry for nodes to
+delete.
+
+In the case where a parent node is deleted, for example the state root
+of a block from some number in the past, the deleted values are examined.  If the
+intersection of all successor roots delete values is empty, then all of the
+deleted nodes listed are deleted from the database.
+
+In the case where a node with no successors - that is, the state root of an
+abandoned fork -  is pruned, the added keys are removed.  In this case, pruning
+can proceed back up the parent chain until it reaches a state root that has
+multiple successors in its delete collection.
+
+**Database Compaction**
+
+After a set number of pruning phases, the database has a large amount of
+reclaimed space, which needs to be compacted.  This is an additional step
+required to keep the overall file size smaller.  It is not an automatic process
+for LMDB.
+
+This compaction operation will be available as a subcommand of `sawadm`.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Configuration
+
+Maximum block depth is a local configuration option.  It is set via the command",82,2018-05-03 20:39:13,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185929463,https://github.com/hyperledger/sawtooth-rfcs/pull/8#discussion_r185929463,peterschwarz
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/couple different/couple of different/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:08:02,183505523,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183505523,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183505523,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/if we could/we could/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:08:21,183505591,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183505591,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183505591,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/A namespace registry/a namespace registry/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:08:51,183505762,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183505762,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183505762,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/namespace and add/namespace can add/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:09:13,183505882,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183505882,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183505882,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/exists/exist/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:12:44,183506858,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183506858,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183506858,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"s/entry the contract registry/entry in the contract registry,/",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:13:32,183507057,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183507057,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183507057,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/then/the/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:14:13,183507243,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183507243,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183507243,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,this sentence is garbled,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:14:50,183507396,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183507396,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183507396,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Missing space before new sentence,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:15:02,183507460,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183507460,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183507460,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,s/sheparding/shepherding/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:16:03,183507720,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: sheparding RFCs, accepting or rejecting",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183507720,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183507720,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"Runaway code isn't restricted to just malicious actors. It can be the result of inadvertent programming errors. Even if it's not initially implemented, I think there either needs to be a premise how to do the execution accounting or a means of stopping a runaway contract.",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-23 19:36:11,183512941,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: sheparding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The design presented is intended to maximize compatibility with the existing
+Sawtooth transaction processor approach. It is intended to extend the full
+capability of transaction processors into the realm of deployable on-chain
+smart contracts. To achieve this, it was clear that a permissioning system
+would be required to constrain deployments.
+
+The only existing on-chain smart contract system for Sawtooth is Seth, which
+implements Ethereum compatibility via a JSON-RPC interface and execution of
+smart contracts in the EVM. Seth does not currently have access to all of the
+features present in Sawtooth transaction processors, nor can Seth smart
+contracts access the entire global state. Is is therefore impossible today, for
+example, to write a Seth contract which interacts with the Intkey namespace.
+It would be possible to extend Seth with a set of built-in smart contracts
+which provided native access to Sawtooth features and/or non-Seth global state.
+However, the inherent complexity, both from an implementation and a user
+experience perspective, makes this solution less desirable. It would also
+provide no natural transition path from Sawtooth transaction processors.
+
+# Prior art
+[prior-art]: #prior-art
+
+Sawtooth currently relies on Transaction Processors that run in a separate
+process. The Sabre API for the smart contracts will closely match that of the
+current transaction processor API.
+
+Sawtooth Sabre follows the intent of Sawtooth [Seth][seth], being able to run
+smart contracts on Sawtooth, but removes the dependencies on an Ethereum
+Virtual Machine.
+
+This design borrows some ideas from [Ethereum][ethereum] smart contracts run on
+the Ethereum Virtual Machine as well as [Parity][parity] and
+[Parity Wasm][wasmi].
+
+[seth]: https://github.com/hyperledger/sawtooth-seth
+[ethereum]:https://www.ethereum.org/greeter
+[parity]:https://github.com/paritytech/parity/tree/27c32d3629e4b70b63ee37736147318035f48792/ethcore/wasm
+[wasmi]:https://github.com/paritytech/wasmi
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+The current design does not describe a method for limiting execution (in
+a manner similar to gas in Ethereum). It is therefore not yet suitable for
+deployment of code by potentially malicious actors. This will be addressed in
+a future RFC.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183512941,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183512941,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"We don't handle this with transaction processors, so even without this feature designed and implemented, we are on-par with current state.

One possible way to implement this is to modify the bytecode and add the equivalent of a use_gas() call at the right points. Another would be to abort the interpreter purely based on the number of instructions it has executed. Currently, we have not taken a position on which path would be better.",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-24 03:13:16,183594574,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: sheparding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The design presented is intended to maximize compatibility with the existing
+Sawtooth transaction processor approach. It is intended to extend the full
+capability of transaction processors into the realm of deployable on-chain
+smart contracts. To achieve this, it was clear that a permissioning system
+would be required to constrain deployments.
+
+The only existing on-chain smart contract system for Sawtooth is Seth, which
+implements Ethereum compatibility via a JSON-RPC interface and execution of
+smart contracts in the EVM. Seth does not currently have access to all of the
+features present in Sawtooth transaction processors, nor can Seth smart
+contracts access the entire global state. Is is therefore impossible today, for
+example, to write a Seth contract which interacts with the Intkey namespace.
+It would be possible to extend Seth with a set of built-in smart contracts
+which provided native access to Sawtooth features and/or non-Seth global state.
+However, the inherent complexity, both from an implementation and a user
+experience perspective, makes this solution less desirable. It would also
+provide no natural transition path from Sawtooth transaction processors.
+
+# Prior art
+[prior-art]: #prior-art
+
+Sawtooth currently relies on Transaction Processors that run in a separate
+process. The Sabre API for the smart contracts will closely match that of the
+current transaction processor API.
+
+Sawtooth Sabre follows the intent of Sawtooth [Seth][seth], being able to run
+smart contracts on Sawtooth, but removes the dependencies on an Ethereum
+Virtual Machine.
+
+This design borrows some ideas from [Ethereum][ethereum] smart contracts run on
+the Ethereum Virtual Machine as well as [Parity][parity] and
+[Parity Wasm][wasmi].
+
+[seth]: https://github.com/hyperledger/sawtooth-seth
+[ethereum]:https://www.ethereum.org/greeter
+[parity]:https://github.com/paritytech/parity/tree/27c32d3629e4b70b63ee37736147318035f48792/ethcore/wasm
+[wasmi]:https://github.com/paritytech/wasmi
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+The current design does not describe a method for limiting execution (in
+a manner similar to gas in Ethereum). It is therefore not yet suitable for
+deployment of code by potentially malicious actors. This will be addressed in
+a future RFC.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183594574,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183594574,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Are there any permissioning possible on who can create a contract registry?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-24 23:08:12,183905695,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+",462,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183905695,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183905695,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"I think the difference is that system admins need to take an active role in launching new transaction processors, and it's possible to gate adding TPs through the permissioning system. The concept of operations is that the consortium would review and ratify code before deploying it on the network.
Sabre would alleviate the logistical aspects of launching apps on the network but should offer some controls like the TP process to mitigate some risks. I added a question about permissioning earlier in the doc just now.
Permissioning might still be a bit less protection than status quo because a permissioned actor can still launch unreviewed code.
If the long term solution is instruction accounting, I would appreciate some dialog on the feasibility within wasm or some way to judge if we really have line of sight.",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-24 23:13:24,183906522,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: sheparding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The design presented is intended to maximize compatibility with the existing
+Sawtooth transaction processor approach. It is intended to extend the full
+capability of transaction processors into the realm of deployable on-chain
+smart contracts. To achieve this, it was clear that a permissioning system
+would be required to constrain deployments.
+
+The only existing on-chain smart contract system for Sawtooth is Seth, which
+implements Ethereum compatibility via a JSON-RPC interface and execution of
+smart contracts in the EVM. Seth does not currently have access to all of the
+features present in Sawtooth transaction processors, nor can Seth smart
+contracts access the entire global state. Is is therefore impossible today, for
+example, to write a Seth contract which interacts with the Intkey namespace.
+It would be possible to extend Seth with a set of built-in smart contracts
+which provided native access to Sawtooth features and/or non-Seth global state.
+However, the inherent complexity, both from an implementation and a user
+experience perspective, makes this solution less desirable. It would also
+provide no natural transition path from Sawtooth transaction processors.
+
+# Prior art
+[prior-art]: #prior-art
+
+Sawtooth currently relies on Transaction Processors that run in a separate
+process. The Sabre API for the smart contracts will closely match that of the
+current transaction processor API.
+
+Sawtooth Sabre follows the intent of Sawtooth [Seth][seth], being able to run
+smart contracts on Sawtooth, but removes the dependencies on an Ethereum
+Virtual Machine.
+
+This design borrows some ideas from [Ethereum][ethereum] smart contracts run on
+the Ethereum Virtual Machine as well as [Parity][parity] and
+[Parity Wasm][wasmi].
+
+[seth]: https://github.com/hyperledger/sawtooth-seth
+[ethereum]:https://www.ethereum.org/greeter
+[parity]:https://github.com/paritytech/parity/tree/27c32d3629e4b70b63ee37736147318035f48792/ethcore/wasm
+[wasmi]:https://github.com/paritytech/wasmi
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+The current design does not describe a method for limiting execution (in
+a manner similar to gas in Ethereum). It is therefore not yet suitable for
+deployment of code by potentially malicious actors. This will be addressed in
+a future RFC.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183906522,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r183906522,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Sawototh-native  -> Sawtooth-native,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-26 15:45:33,184438271,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184438271,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184438271,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,capbility -> capability ,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-26 15:45:51,184438377,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184438377,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184438377,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,I agree with @dcmiddle. It is too easy to inadvertently write runaway code.,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-26 21:09:59,184531859,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, if we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, A namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace and add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exists or does not have an entry the contract
+registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, then
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry will set in state in a the namespace
+registry list.Otherwise, the transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: sheparding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The design presented is intended to maximize compatibility with the existing
+Sawtooth transaction processor approach. It is intended to extend the full
+capability of transaction processors into the realm of deployable on-chain
+smart contracts. To achieve this, it was clear that a permissioning system
+would be required to constrain deployments.
+
+The only existing on-chain smart contract system for Sawtooth is Seth, which
+implements Ethereum compatibility via a JSON-RPC interface and execution of
+smart contracts in the EVM. Seth does not currently have access to all of the
+features present in Sawtooth transaction processors, nor can Seth smart
+contracts access the entire global state. Is is therefore impossible today, for
+example, to write a Seth contract which interacts with the Intkey namespace.
+It would be possible to extend Seth with a set of built-in smart contracts
+which provided native access to Sawtooth features and/or non-Seth global state.
+However, the inherent complexity, both from an implementation and a user
+experience perspective, makes this solution less desirable. It would also
+provide no natural transition path from Sawtooth transaction processors.
+
+# Prior art
+[prior-art]: #prior-art
+
+Sawtooth currently relies on Transaction Processors that run in a separate
+process. The Sabre API for the smart contracts will closely match that of the
+current transaction processor API.
+
+Sawtooth Sabre follows the intent of Sawtooth [Seth][seth], being able to run
+smart contracts on Sawtooth, but removes the dependencies on an Ethereum
+Virtual Machine.
+
+This design borrows some ideas from [Ethereum][ethereum] smart contracts run on
+the Ethereum Virtual Machine as well as [Parity][parity] and
+[Parity Wasm][wasmi].
+
+[seth]: https://github.com/hyperledger/sawtooth-seth
+[ethereum]:https://www.ethereum.org/greeter
+[parity]:https://github.com/paritytech/parity/tree/27c32d3629e4b70b63ee37736147318035f48792/ethcore/wasm
+[wasmi]:https://github.com/paritytech/wasmi
+
+# Unresolved questions
+[unresolved]: #unresolved-questions
+
+The current design does not describe a method for limiting execution (in
+a manner similar to gas in Ethereum). It is therefore not yet suitable for
+deployment of code by potentially malicious actors. This will be addressed in
+a future RFC.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184531859,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184531859,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,I would have liked to see a little more discussion on the decision to use mutable smart contracts as opposed to immutable contracts in Ethereum.,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-26 21:24:11,184535040,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry is set in state. Otherwise, the
+transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: shepharding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives",731,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184535040,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184535040,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,The text has been updated such that creation of a contract registry may only be done by sawtooth.swa.administrators.,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 16:01:00,184732307,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+",462,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184732307,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184732307,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"typo?
s/contact/contract ",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:10:28,184766039,"@@ -451,6 +451,11 @@ Create a contract registry with no version.
 If the contract registry for the provided contact name already exists, the",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184766039,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184766039,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,does creating a contract registry modify the sawtooth.swa.administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:34:41,184772477,"@@ -460,10 +465,12 @@ The new contract registry is set in state.
 The inputs for CreateContractRegistryAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
 
 The outputs for CreateContractRegistryAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184772477,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184772477,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,does deleting a contract modify the administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:35:42,184772754,"@@ -476,18 +483,22 @@ Deletes a contract registry if there are no versions.
 ```
 
 If the contract registry does not exist, the transaction is invalid. If the
-transaction signer is not an owner or the contract registry has any number of
+transaction signer is not an owner or does not have their public key in
+``sawtooth.swa.administrators`` or the contract registry has any number of
 versions, the transaction is invalid.
 
 The contract registry is deleted.
 
 The inputs for DeleteContractRegistryAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
 
 The outputs for DeleteContractRegistryAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184772754,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184772754,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,update contract registry modifies administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:36:43,184773066,"@@ -501,18 +512,22 @@ Update the contract registry's owners list.
 ```
 
 If the contract registry does not exist or the transaction signer is not an
-owner, the transaction is invalid.
+owner or does not have their public key in ``sawtooth.swa.administrators``, the
+transaction is invalid.
 
 The new owner list will replace the current owner list and the updated contract
 registry is set in state.
 
 The inputs for UpdateContractRegistryOwnersAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
 
 The outputs for UpdateContractRegistryOwnersAction must include:
 
 * the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184773066,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184773066,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Does this operation modify the administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:42:50,184774718,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184774718,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184774718,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Does this operation modify the administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:43:05,184774787,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or does not have their public key in
+``sawtooth.swa.administrators`` or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184774787,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184774787,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Does this operation modify the administrators?,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:43:21,184774850,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or does not have their public key in
+``sawtooth.swa.administrators`` or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner or does not have their public key in ``sawtooth.swa.administrators``, the
+transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184774850,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184774850,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,"typo?
s/contact/contract",4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:43:45,184774949,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184774949,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184774949,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,nope. This is an error. sawtooth.swa.administrators should be removed from the outputs. ,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:44:27,184775095,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184775095,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184775095,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Same as above,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:44:32,184775124,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or does not have their public key in
+``sawtooth.swa.administrators`` or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184775124,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184775124,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,same as above,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-27 18:44:38,184775138,"@@ -0,0 +1,785 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawtooth-native on-chain smart
+contract capability to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, the transaction
+is invalid.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new contract registries. If the transaction signer is an
+administrator, the new contract registry is set in state. Otherwise, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or does not have their public key in
+``sawtooth.swa.administrators`` or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner or does not have their public key in ``sawtooth.swa.administrators``, the
+transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``
+
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+* the settings address for ``sawtooth.swa.administrators``",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/184775138,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r184775138,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,typo s/intructions/instructions/,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-30 15:11:17,185015822,"@@ -783,3 +783,8 @@ The current design does not describe a method for limiting execution (in
 a manner similar to gas in Ethereum). It is therefore not yet suitable for
 deployment of code by potentially malicious actors. This will be addressed in
 a future RFC.
+
+The interpreter used by Sabre, Wasmi, does not currently offer a way to halt
+the execution of a function. However, it would be relatively simple to add
+an option to the module constructor that would specify an upper limit for the
+number of intructions that can be executed.",,2018-04-30 21:09:08,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185015822,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r185015822,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/7,https://github.com/hyperledger/sawtooth-rfcs/pull/7,Additional text related to this has been added to the Rationale and alternatives section,4dd0fd0f2090e8d27f59fbae221d57058955db2a,2018-04-30 21:16:10,185112989,"@@ -0,0 +1,770 @@
+<!--
+  Copyright 2018 Cargill Incorporated
+
+  Licensed under the Apache License, Version 2.0 (the ""License"");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+- Feature Name: wasm\_smart\_contracts
+- Start Date: 2018-04-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+Proposes Sawtooth Sabre, a transaction family which implements on-chain smart
+contracts executed in a WebAssembly virtual machine.
+
+WebAssembly (Wasm) is a stack-based virtual machine newly implemented in major
+browsers. It is well-suited for the purposes of smart contract execution due to
+its sandboxed design (executed in an environment independent from the host),
+growing popularity, and tool support.
+
+# Motivation
+[motivation]: #motivation
+
+On-chain smart contracts have excellent characteristics for ad-hoc distributed
+deployment. The code of the smart contract is deployed (or loaded) into global
+state via a transaction.  Other transactions cause that on-chain code to be
+loaded into a virtual machine and executed. The most commonly used on-chain
+smart contract system is Ethereum which uses the Ethereum Virtual Machine (EVM)
+to execute smart contracts.
+
+This differs from Sawtooth's transaction processors, which are not deployed
+on-chain; transaction processors are distributed as software in a manner
+similar to Sawtooth itself. This has various drawbacks from a distributed
+deployment perspective, since some level of network-wide coordination is
+necessary to modify transaction processors.  However, conceptually transaction
+processors can be thought of as a layer below on-chain smart contracts.  In
+fact, implementing on-chain smart contracts using transaction processors was
+one of the initial motivators behind Sawtooth's modular transaction execution
+platform design.
+
+An existing transaction family, Sawtooth Seth, implements an EVM on-chain smart
+contract solution for Sawtooth.  Seth's strength is compatibility with the
+Ethereum tool chain and existing smart contracts written for Ethereum (for
+example, using Solidity). Despite the awesome capability it provides, Seth is
+restricted by the Ethereum design and does not expose the entire feature set of
+Sawtooth transaction processors.
+
+Sawtooth Sabre fills this gap by providing a permissioned WebAssembly-based
+smart contract platform which provides full feature-parity with the Sawtooth
+SDK's transaction processor API. It further attempts to provide these
+capabilities in the most source-compatible way possible to make it easy to
+transition existing transaction processors to run as a Sabre smart contract.
+
+The goal of Sawtooth Sabre is to provide a Sawototh-native on-chain smart
+contract capbility to Sawtooth. The goal of the design and implementation
+described in this RFC is to jumpstart Sabre with a minimal but usable
+implementation, not describe every eventual feature.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Sawtooth Sabre is an on-chain smart contract platform which executes smart
+contracts in a WebAssembly virtual machine. It consists of three primary
+components:
+
+- Sabre transaction processor (sabre-tp)
+- CLI command (sabre)
+- Sabre Smart Contract SDK
+
+The design of Sabre includes three fundamental data structures:
+
+- Namespace Registry
+- Contract Registry
+- Contract
+
+## Namespace Registry
+
+Generally speaking, transaction processors can read and write arbitrary global
+state addresses. While restrictions can be placed upon the transaction
+processor via a couple of different existing mechanisms, they are not granular
+enough to be used in the context of Sabre, since all Sabre contracts would be
+subject to the same restrictions.
+
+Smart contracts and namespaces of global state are not necessarily a one-to-one
+relationship. For example, we could create separate Sabre smart contracts
+which provide multiplication and addition operations on the intkey namespace.
+Both smart contracts would operate on the same intkey namespace. Since
+this is the case, for a fully permissioned system we desire the ability to
+control access to the namespace independently from the permission to deploy and
+maintain the smart contracts.
+
+In Sabre, a namespace registry is used to control permissions related to
+a namespace. The entry in the namespace registry for a namespace also contains
+a list of owners, which can be updated through transactions. An owner of the
+namespace can add and remove permissions associated with specific contracts.
+Thus, the owner of the intkey namespace can add permissions which allow the
+multiplication and addition smart contracts read and write access. This
+delegates some authority to the smart contract owners; so while the owner of
+the namespace and owner of the contract are not necessarily the same, there is
+an implied degree of trust and coordination between them.
+
+## Contract Registry
+
+Sabre contracts are referenced by name. In the example of smart contracts which
+multiply and add values in the intkey namespace, the names of the contracts
+might be intkey-multiply and intkey-add.
+
+The contracts are also versioned. Following our example, intkey-multiply v1.0.0
+may have only allowed multiplying two values, while a more recent
+intkey-multiply v1.1.0 may allow any number of values to be multiplied
+together.  The intent is to allow developers to follow Semantic Versioning best
+practices.
+
+A contract registry stores information related to each version of the contract
+and the list of owners. Owners are allowed to load new versions of contracts.
+
+## Contract
+
+A Sabre contract consists of the compiled Wasm code and related metadata (name,
+version, input/output addresses, etc.).
+
+Writing Sabre contracts is similar in nature to writing Sawtooth transaction
+processors, and the Sabre SDK is intended to be a drop-in replacement for the
+Sawtooth SDK.  Thus, porting existing Sawtooth transaction processor code to
+Sawtooth Sabre is relatively easy - just change a few import statements to
+refer to the Sabre SDK instead of the Sawtooth SDK. The details of running
+within a Wasm interpeter are hidden from the smart contract author.
+
+It is also possible to maintain dual-target smart contracts, where the smart
+contract can be run either as a native transaction processor or as a Sabre
+contract.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## State
+
+All Sabre objects are serialized using Protocol Buffers before being stored in
+state. Theses objects include namespace registries, contract registries, and
+contracts. All objects are stored in a list to handle hash collisions.
+
+### NamespaceRegistry
+
+A namespace is a state address prefix used to identify a portion of state.
+The NamespaceRegistry stores the namespace, the owners of the namespace and the
+permissions given to that namespace. NamespaceRegistry is uniquely identified
+by its namespace.
+
+Permissions are used to control read and/or write access to the namespace. It
+includes a contract name that correlates to the name of a Sabre contract and
+whether that contract is allowed to read and/or write to that namespace. The
+permission is uniquely identified by the contract\_name. If the contract is
+executed but does not have the needed permission to read or write to state,
+the transaction is considered invalid.
+
+```protobuf
+  message NamespaceRegistry {
+    message Permission {
+      string contract_name = 1;
+      bool read = 2;
+      bool write = 3;
+    }
+
+    string namespace = 1;
+    repeated string owners = 2;
+
+    repeated Permission permissions = 3;
+  }
+```
+
+When the same address is computed for different namespace registries, a
+collision occurs; all colliding namespace registries are stored in at the
+address in a NamespaceRegistryList
+
+```protobuf
+  message NamespaceRegistryList {
+    repeated NamespaceRegistry registries = 1;
+  }
+```
+
+### ContractRegistry
+
+ContractRegistry keeps track of versions of the Sabre contract and the list of
+owners. A ContractRegistry is uniquely identified by the name of its contract.
+
+Versions represent the contract version and include the sha512 hash of the
+contract and the public key of the creator. The hash can be used by a client to
+verify this is the correct version of the contract that should be executed.
+
+```protobuf
+  message ContractRegistry {
+    message Version {
+      string version = 1;
+
+      // used to verify a contract is same as the one the client intended to
+      // invoke
+      string contract_sha512 = 2;
+
+      // for client information purposes only - the key that created this
+      // contract on the chain
+      string creator = 3;
+    }
+
+    string name = 1;
+    repeated Version versions = 2;
+    repeated string owners = 3;
+  }
+```
+
+ContractRegistry whose addresses collide are stored in a ContractRegsitryList.
+
+```protobuf
+  message ContractRegistryList {
+    repeated ContractRegistry registries = 1;
+  }
+```
+
+### Contract
+
+A Contract represents the Sabre smart contract. It is uniquely
+identified by its name and version number. The contract also contains the
+expected inputs and outputs used when executing the contract, the public
+key of the creator and the compiled wasm code of the contract.
+
+```protobuf
+    message Contract {
+      string name = 1;
+      string version = 2;
+      repeated string inputs = 3;
+      repeated string outputs = 4;
+      string creator = 5;
+      bytes contract = 6;
+    }
+```
+
+Contracts whose addresses collide are stored in a ContractList.
+
+```protobuf
+    message ContractList {
+      repeated Contract contracts = 1;
+    }
+```
+
+### Addressing
+
+Sabre objects are stored under 3 namespaces:
+
+  - ``00ec00``: Namespace for NamespaceRegistry
+  - ``00ec01``: Namespace for ContractRegistry
+  - ``00ec02``: Namespace for Contracts
+
+The remaining 64 characters of the objects address is the following:
+  - NamespaceRegistry: the first 64 characters of the hash of the first 6
+    characters of the namespaces.
+  - ContractRegistry: the first 64 characters of the hash of the name.
+  - Contract: the first 64 characters of the hash of ""name,version""
+
+For example, the address for a contract with name ""example"" and version ""1.0""
+address would be:
+
+```pycon
+  >>> '00ec02' + get_hash(""example,1.0"")
+  '00ec0248a8e00e3fbca83815668ec5eee730023e6eb61b03b54e8cae1729bf5a0bec64'
+```
+
+
+## Transaction Payload and Execution
+
+Below, the different payload actions are defined along with the inputs and
+outputs that are required in the transaction header.
+
+### SabrePayload
+
+A SabrePayload contains an action enum and the associated action payload. This
+allows for the action payload to be dispatched to the appropriate logic.
+
+Only the defined actions are available and only one action payload should be
+defined in the SabrePayload.
+
+```protobuf
+  message SabrePayload {
+    enum Action {
+      ACTION_UNSET = 0;
+      CREATE_CONTRACT = 1;
+      DELETE_CONTRACT = 2;
+      EXECUTE_CONTRACT = 3;
+      CREATE_CONTRACT_REGISTRY = 4;
+      DELETE_CONTRACT_REGISTRY = 5;
+      UPDATE_CONTRACT_REGISTRY_OWNERS = 6;
+      CREATE_NAMESPACE_REGISTRY = 7;
+      DELETE_NAMESPACE_REGISTRY = 8;
+      UPDATE_NAMESPACE_REGISTRY_OWNERS = 9;
+      CREATE_NAMESPACE_REGISTRY_PERMISSION = 10;
+      DELETE_NAMESPACE_REGISTRY_PERMISSION = 11;
+    }
+
+    Action action = 1;
+
+    CreateContractAction create_contract = 2;
+    DeleteContractAction delete_contract = 3;
+    ExecuteContractAction execute_contract = 4;
+
+    CreateContractRegistryAction create_contract_registry = 5;
+    DeleteContractRegistryAction delete_contract_registry = 6;
+    UpdateContractRegistryOwnersAction update_contract_registry_owners = 7;
+
+    CreateNamespaceRegistryAction create_namespace_registry = 8;
+    DeleteNamespaceRegistryAction delete_namespace_registry = 9;
+    UpdateNamespaceRegistryOwnersAction update_namespace_registry_owners = 10;
+    CreateNamespaceRegistryPermissionAction create_namespace_registry_permission = 11;
+    DeleteNamespaceRegistryPermissionAction delete_namespace_registry_permission = 12;
+  }
+```
+
+### CreateContractAction
+
+Creates a contract and updates the associated contract registry.
+
+```protobuf
+  message CreateContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes contract = 5;
+  }
+```
+
+If a contract with the name and version already exists the transaction is
+considered invalid.
+
+The contract registry is fetched from state and the transaction signer is
+checked against the owners. If the signer is not an owner, the transaction is
+considered invalid.
+
+If the contract registry for the contract name does not exist, a new contract
+registry is created and the transaction signer is added as an owner.
+
+Both the new contract and the updated contract registry are set in state.
+
+The inputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+The outputs for CreateContractAction must include:
+
+* the address for the new contract
+* the address for the contract registry
+
+### DeleteContractAction
+
+Delete a contract and remove its entry from the associated contract registry.
+
+```protobuf
+  message DeleteContractAction {
+    string name = 1;
+    string version = 2;
+  }
+```
+
+If the contract does not already exist or does not have an entry in the
+contract registry the transactions is invalid.
+
+If the transaction signer is not an owner, they cannot delete the contract and
+the transaction is invalid.
+
+The contract is deleted and version entry is removed from the
+contract entry.
+
+The inputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+The outputs for DeleteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+
+### ExecuteContractAction
+
+Execute the contract.
+
+```protobuf
+  message ExecuteContractAction {
+    string name = 1;
+    string version = 2;
+    repeated string inputs = 3;
+    repeated string outputs = 4;
+    bytes payload = 5;
+  }
+```
+
+The contract is fetched from state. If the contract does not exist, the
+transaction is invalid.
+
+The inputs and outputs are then checked against the namespace registry
+associated with the first 6 characters of each input or output. If the input
+or output is less then 6 characters the transaction is invalid. For every
+input, the namespace registry must have a read permission for the contract and
+for every output the namespace registry must have a write permission for the
+contract. If either are missing or the namespace registry does not exist,
+the transaction is invalid.
+
+The contract is than loaded into the wasm interpreter and run against the
+provided payload. A result is returned. If the result is ok the transaction
+is valid and the contract data is stored in state. If the result is invalid or
+another error occurs the transaction is invalid. If the result is an internal
+error, an internal error is raised.
+
+The inputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any inputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract inputs
+
+The outputs for ExecuteContractAction must include:
+
+* the address for the contract
+* the address for the contract registry
+* any outputs that are required for executing the contract
+* the addresses for every namespace registry required to check the provided
+  contract outputs
+
+### CreateContractRegistryAction
+
+Create a contract registry with no version.
+
+```protobuf
+  message CreateContractRegistryAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry for the provided contact name already exists, the
+transaction is invalid.
+
+The new contract registry is created for the name and provided owners. The
+owners should be a list of public keys of users that are allowed to add new
+contract versions, delete old versions, and delete the registry.
+
+The new contract registry is set in state.
+
+The inputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for CreateContractRegistryAction must include:
+
+* the address for the contract registry
+
+### DeleteContractRegistryAction
+
+Deletes a contract registry if there are no versions.
+
+```protobuf
+  message DeleteContractRegistryAction {
+    string name = 1;
+  }
+```
+
+If the contract registry does not exist, the transaction is invalid. If the
+transaction signer is not an owner or the contract registry has any number of
+versions, the transaction is invalid.
+
+The contract registry is deleted.
+
+The inputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+The outputs for DeleteContractRegistryAction must include:
+
+* the address for the contract registry
+
+### UpdateContractRegistryOwnersAction
+
+Update the contract registry's owners list.
+
+```protobuf
+  message UpdateContractRegistryOwnersAction {
+    string name = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the contract registry does not exist or the transaction signer is not an
+owner, the transaction is invalid.
+
+The new owner list will replace the current owner list and the updated contract
+registry is set in state.
+
+The inputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+The outputs for UpdateContractRegistryOwnersAction must include:
+
+* the address for the contract registry
+
+### CreateNamespaceRegistryAction
+
+Creates a namespace registry with no permissions.
+
+```protobuf
+  message CreateNamespaceRegistryAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+The namespace must be at least 6 characters long. If the namespace registry
+already exists, the transaction is invalid.
+
+Only those whose public keys are stored in ``sawtooth.swa.administrators`` are
+allowed to create new namespace registries. If the transaction signer is an
+administrator, the new namespace registry is set in state. Otherwise, the
+transaction is invalid.
+
+The inputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryAction
+
+Deletes a namespace registry if it does not contains any permissions.
+
+```protobuf
+  message DeleteNamespaceRegistryAction {
+    string namespace = 1;
+  }
+```
+
+If the namespace registry does not exist or contain permissions, the
+transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry
+is deleted. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### UpdateNamespaceRegistryOwnersAction
+
+Update the namespace registry's owners list.
+
+```protobuf
+  message UpdateNamespaceRegistryOwnersAction {
+    string namespace = 1;
+    repeated string owners = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, the namespace registry's
+owners are updated. Otherwise, the transaction is invalid.
+
+The updated namespace registry is set in state.
+
+The inputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for UpdateNamespaceRegistryOwnersAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### CreateNamespaceRegistryPermissionAction
+
+Adds a permission entry into a namespace registry for the associated namespace.
+
+```protobuf
+  message CreateNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+    bool read = 3;
+    bool write = 4;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid.
+
+If the transaction signer is either an owner in the namespace registry or has
+their public key in ``sawtooth.swa.administrators``, a new permission is
+added for the provided contract\_name. Otherwise, the transaction is invalid.
+
+If there is already a permission for the contract\_name in the namespace
+registry, the old permission is removed and replaced with the new
+permission.
+
+The updated namespace registry is set in state.
+
+The inputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for CreateNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+### DeleteNamespaceRegistryPermissionAction
+
+Delete a permission entry in a namespace registry for the associated
+namespace.
+
+```protobuf
+  message DeleteNamespaceRegistryPermissionAction {
+    string namespace = 1;
+    string contract_name = 2;
+  }
+```
+
+If the namespace registry does not exist, the transaction is invalid. If the
+transaction signer is either an owner in the namespace registry or has their
+public key in ``sawtooth.swa.administrators``, the permission for the provided
+contract name is removed. Otherwise, the transaction is invalid.
+
+The inputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+The outputs for DeleteNamespaceRegistryPermissionAction must include:
+
+* the address for the namespace registry
+* the settings address for ``sawtooth.swa.administrators``
+
+## Transaction Header
+
+### Inputs and Outputs
+
+The required inputs and outputs are defined for each action payload above.
+
+### Dependencies
+
+No dependencies.
+
+### Family
+
+- family_name: ""sabre""
+- family_version: ""0.1""
+
+# Sub-team Creation
+[subteam]: #subteam
+
+A Sawtooth Sabre sub-team shall be formed; this sub-team shall be responsible
+for the following related to Sabre: shepharding RFCs, accepting or rejecting
+RFCs, policy decisions (including when RFCs are required), and making decisions
+on topics which do not require an RFC.
+
+## Discussion
+
+A #sawtooth-sabre channel shall be created in Hyperledger's RocketChat for the
+purposes of discussion related to Sabre.
+
+## Membership
+
+- Shawn Amundson, team lead
+- Andi Gunderson
+- Ryan Banks
+- James Mitchell
+
+There are no strict rules for how to become a member of the Sabre sub-team, but
+in general, the following areas of participation are important:
+
+- Submit high-quality PRs to Sabre and/or RFCs concerning Sabre
+- Participate in Sabre code reviews and/or Sabre RFC discussion
+- Write and enhance Sabre documentation
+- Actively support Sabre users on RocketChat
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Only a Sabre SDK for Rust will be initially implemented.
+
+Though the goal is compatibility with the transaction processor API, it is not
+always trivial to compile commonly used Rust dependencies into Wasm. This may
+improve over time as Wasm popularity grows, or it may persist into the future.
+
+The use of named contracts is substantially different than other smart contract
+systems such as Ethereum, which instead use a hash of the contract as the
+unique identifier. The design was chosen because it provides a friendlier
+method of versioning smart contracts for permissioned networks. However, it
+would not be a suitable approach for a public non-permissioned network; thus,
+supporting such networks will require substantial design changes in the future.
+
+# Rationale and alternatives
+[alternatives]: #alternatives",731,2018-04-30 21:16:11,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/185112989,https://github.com/hyperledger/sawtooth-rfcs/pull/7#discussion_r185112989,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"Does unanimous in this context mean ""everyone votes for"" or ""no one votes against""? For the root team, this ambiguity is probably fine, but the list of maintainers might grow quite large, and it might no longer be practical to get an affirmative vote from everyone. So for maintainers in particular, I would suggest the ""no one votes against"" version, but I'd like clarification of what unanimous means either way.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-22 21:26:39,176577318,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing",308,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176577318,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176577318,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I think the implicit assumption here is that the maintainer list, which is on a per-component/per-repository basis, stays relatively small and is only comprised of the individuals with an expert knowledge and understanding of the code-base. If the maintainer list on a repo grows so large that it is impractical to get an affirmative vote from everyone, then I would interpret that as an indication that the group has grown too large. The intent is to make decisions by consensus and it is the responsibility of those in leadership positions to ""drive consensus"" by keeping people involved.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-22 22:17:56,176588916,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing",308,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176588916,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176588916,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I'd agree with the suggestion by @delventhalz since this is how an ASF PMC also operates, as long as there are no negative votes and at least one binding positive vote by a PMC member, a proposal passes.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 01:52:11,176620879,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing",308,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176620879,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176620879,grkvlt
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,Also http://www.apache.org/foundation/governance/pmcs.html which describes the ASF model.,e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 13:42:17,176737226,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing
+maintainers. Maintainers lose their status due to:
+
+- Inactivity of 12 Months
+- Unanimous vote by all other Maintainers
+
+## Code of Conduct
+
+Members of the Sawtooth community are expected to abide by the [Hyperledger Code of Conduct][hyperledger-coc].
+Violations of this code of conduct by any community member may result in
+disciplinary action at the discretion of the root team (by unanimous vote,
+excluding the offending member if applicable).
+
+[hyperledger-coc]: https://wiki.hyperledger.org/community/hyperledger-project-code-of-conduct
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This RFC makes explicit a governance model. Since this was previously implicit,
+some existing community members may feel offended by not being given a
+permission level that is consistent with their expectations or by not being
+placed on a team whose domain they feel responsible for.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The [Mozilla governance model](https://www.mozilla.org/en-US/about/governance/policies/module-ownership/ )
+was also considered, however a hierarchical model was focused on ensuring the
+project maintained a consistent vision was preferred.",336,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176737226,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176737226,grkvlt
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I'm not convinced this should be a separate group? I think there are cross-cutting concerns like release and CI that should be handled by the top-level team, with sub-teams responsible for separate projects able to modify the processes within limits.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 13:47:59,176738921,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as",277,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176738921,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176738921,grkvlt
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I would prefer if the sub-teams are focussed on software components that have their own separate existence, such as Seth and the Go SDK, where there is a definite releasable, versioned artifact they are responsible for. Other concerns like community outreach should be handled by the each team, taking their lead/direction from core.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 13:53:47,176740673,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.",260,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176740673,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176740673,grkvlt
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I'm not clear what exactly the point of the reviewer/committer/maintainer split is. It seems that a committer is basically a reviewer who is able to click the 'merge' button on github, but only once two maintainers have approved the PR. I think this is getting overly complicated, and really you only need a 'committer' role with write access privileges, plus the rule that merges can only occur once two committers have approved the change? Why would there be a special class of people trusted to review PRs but not trusted to approve or merge them? This sounds like they are just ordinary GitHub users, since the repos are public, so we don't need to specify their rights here.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 13:59:46,176742707,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.",298,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176742707,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176742707,grkvlt
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,Thanks for the link,e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 14:04:28,176744121,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing
+maintainers. Maintainers lose their status due to:
+
+- Inactivity of 12 Months
+- Unanimous vote by all other Maintainers
+
+## Code of Conduct
+
+Members of the Sawtooth community are expected to abide by the [Hyperledger Code of Conduct][hyperledger-coc].
+Violations of this code of conduct by any community member may result in
+disciplinary action at the discretion of the root team (by unanimous vote,
+excluding the offending member if applicable).
+
+[hyperledger-coc]: https://wiki.hyperledger.org/community/hyperledger-project-code-of-conduct
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+This RFC makes explicit a governance model. Since this was previously implicit,
+some existing community members may feel offended by not being given a
+permission level that is consistent with their expectations or by not being
+placed on a team whose domain they feel responsible for.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+The [Mozilla governance model](https://www.mozilla.org/en-US/about/governance/policies/module-ownership/ )
+was also considered, however a hierarchical model was focused on ensuring the
+project maintained a consistent vision was preferred.",336,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176744121,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176744121,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I am still leaning towards a ""no votes against"" version of unanimous. If nothing else, maybe people are sick, or otherwise unavailable (though in that case, we probably need to spell out what a reasonable timeframe for a response on a vote is).

More importantly, however we're using the term, I think it is important to explicitly define unanimous.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 14:10:00,176745817,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.
+
+A **maintainer** has permission to approve changes. All pull-requests must be
+approved by at least 2 maintainers before a committer may merge it.
+Maintainers are expected to:
+
+- Carefully review pull requests and leave thoughtful, constructive feedback
+- Ensure the component is up-to-date with patches and security updates
+- Participate in community discussions related to their component
+
+A committer can be promoted to a maintainer by unanimous vote by the existing",308,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176745817,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176745817,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"For the subteam breakdown, we tried to take inspiration from an existing model that seems to be working: https://www.rust-lang.org/en-US/team.html Their breakdown includes high-level, cross-cutting concerns such as a release team and a community team.

Re-reading this description again, I think it could be clarified that this subteam is responsible for the release management of the core platform and not necessarily separate components like the SDKs and Seth.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 14:33:41,176753679,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as",277,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176753679,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176753679,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"I tend to agree with @grkvlt.
Maybe there's a mix here of what constraints the github permissions allow and what permissions and roles we think are necessary.
I'd like to invite any specific user to review a PR. This seems to be a limitation on github atm.
I'd like there to be a well defined limited set of people who can merge. Those individuals have the judgement to get two diverse approvals and know when it's ok to click merge. I don't care if you call them contributors with write permissions or repo maintainers, but it really seems like just one role. Either you have write perms or you don't. 
The root group maybe defines a more classical sense of 'maintainer', but again I'm not overly concerned with the labels.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-23 20:13:13,176851782,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.",298,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176851782,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r176851782,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"There are three distinct permissions here - review, merge, and approval. These three levels are defined here to reflect those three levels.  Review and merge is controlled via github permissions and approvers are probably in MAINTAINERS.md.  If commiters == maintainers in practice, great.  However, I suspect with more repos, we will have maintainers of some repos as committers (but not maintainers) for other repos.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-27 01:25:59,177282417,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.
+
+**Core** - Responsible for the design, architecture, performance, stability,
+ and security of the core Sawtooth platform
+
+**Application SDKs** - Responsible for the design and consistency of
+user-facing SDKs for application development
+
+**Consensus** - Responsible for the supported consensus algorithms and
+the consensus interface
+
+**Seth** - Responsible for the Sawtooth integration with Ethereum
+
+**Community Outreach** - Responsible for writing and hosting documentation,
+managing the main Sawtooth website, promoting Sawtooth to the public, managing
+demos, and providing the community with training and support
+
+**Release Management** - Responsible for release related issues such as
+dependency management, license compliance, version control, and upgrade and
+backwards compatibility
+
+**Continuous Integration** - Responsible for test and build environments and
+deployment artifacts
+
+## Contributor Permission Levels
+
+The Sawtooth community encourages contributions to the project from all
+interested individuals. The project defines three levels of permissions that
+determine what actions a contributor can perform on a particular repository.
+
+A **reviewer** has ""read"" permission, meaning they can review pull requests.
+
+A **committer** has ""write"" permission, meaning they can merge pull requests
+once they have been approved.
+
+Any community member can be promoted to a reviewer or committer by a
+maintainer. Reviewers and committers lose their status when it is removed by a
+maintainer. This is usually due to inactivity, but can also be used for
+moderation.",298,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/177282417,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r177282417,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/6,https://github.com/hyperledger/sawtooth-rfcs/pull/6,"None of the Sawtooth components have a separate existence per-se, as it is one big project. We have different components but not different projects. Conceptually, something like Seth could be elevated to a proper Hyperledger project itself at some point.

Releases, publishing of artifacts, etc. need to be coordinated across the components. We could release components on their own release schedules (this makes a lot of sense for components which are not tightly coupled), but general strategy should roughly be the same (same tagging/branching approach, etc.).

For the SDKs, the sub-team will be represented by maintainers of all the SDKs (ideally) as it is important that we have similar features and design across the SDKs.",e6a4c98930fec12ab2ca05c8027e12f0c6775b96,2018-03-27 01:35:34,177283786,"@@ -0,0 +1,348 @@
+- Feature Name: governance
+- Start Date: 2018-03-16
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC proposes an explicit governance structure for the Hyperledger Sawtooth
+project and establishes the initial teams and members that makeup this
+structure. It also defines the meaning of committer and maintainer for
+individual repositories.
+
+# Motivation
+[motivation]: #motivation
+
+Many important aspects of the Sawtooth governance model have been based on
+implicit norms and expectation. As the project continues to grow, it is
+important that this governance model be made explicit with respect to decision
+making authority, repository ownership, and the establishment of the project's
+direction and vision. To this end, this RFC seeks to make explicit existing
+norms that can be carried forward as the project grows and to establish new
+policies and procedures where additional structure is needed.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+At a high-level, the Sawtooth governance model consists of two arms:
+
+1. A hierarchy of teams responsible for the continued growth and success of the
+   project
+2. A set of contributor levels used to manage component repository permissions.
+
+The team hierarchy contains a high-level root team and a set of more focused
+subteams, each of which is led by a member of the root team. Having a member of
+the root team lead each of the subteams is designed to promote communication
+and alignment of vision across the project.
+
+The contributor levels consist of three levels: reviewer, committer and
+maintainers. These levels define what actions a contributor can perform for a
+particular repository.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Project Team Hierarchy
+
+The Sawtooth governance model consists of a root team and a number of subteams.
+
+### Root Team
+
+The root team serves as leadership for the Sawtooth project as a whole. In
+particular, it:
+
+* Sets the overall direction and vision for the project. This means setting the
+  core values that are used when making decisions about technical tradeoffs. It
+  means steering the project toward specific use cases where Sawtooth can have
+  a major impact. It means leading the discussion, and writing RFCs for, major
+  initiatives in the project.
+
+* Sets the priorities and release schedule. Design bandwidth is limited, and
+  it's dangerous to try to grow the project too quickly; the root team makes
+  some difficult decisions about which areas to prioritize for new design,
+  based on the core values and target use cases.
+
+* Focuses on broad, cross-cutting concerns. The root team is specifically
+  responsible for taking a global view of the project, to make sure the pieces
+  are fitting together in a coherent way.
+
+* Spins up or shuts down subteams. Over time, it may make sense to expand the
+  set of subteams or it may make sense to have temporary ""strike teams"" that
+  focus on a particular, limited task.
+
+The root team includes stakeholders who are actively involved in the Sawtooth
+community and have expertise within the project. Subteam leaders are chosen
+from the root team.
+
+Members are added to the root team by unanimous vote of the existing root team
+members. Members are removed by unanimous vote of the existing root team,
+excluding the member being removed.
+
+### Subteams
+
+Each of the subteams has responsibility over a specific domain of the Sawtooth
+project, which is established at its inception. Specifically, subteams are
+responsible for:
+
+* Shepherding RFCs that are within the purview of the subteams domain. That
+  means (1) ensuring that stakeholders are aware of the RFC, (2) working to
+  tease out various design tradeoffs and alternatives, and (3) helping build
+  consensus.
+
+* Accepting or rejecting RFCs within the subteam's domain.
+
+* Setting policy on what changes in the subteam area require RFCs, and
+  reviewing direct PRs for changes that do not require an RFC.
+
+Subteams make it possible to involve a larger, more diverse group in the
+decision-making process. In particular, they should involve a mix of:
+
+* Sawtooth project leadership, in the form of at least one root team member
+  (the leader of the subteam).
+
+* Area experts: people who have a lot of interest and expertise in the subteam
+  area, but who may be far less engaged with other areas of the project.
+
+* Stakeholders: people who are strongly affected by decisions in the subteam
+  area, but who may not be experts in the design or implementation of that
+  area. Whenever possible, it is crucial that users of Sawtooth have a seat at
+  the table, to make sure we are actually addressing real-world needs.
+
+Members should have demonstrated a good sense for design and dealing with
+tradeoffs, an ability to work within a framework of consensus, and of course
+sufficient knowledge about or experience with the subteam area. Leaders should
+in addition have demonstrated exceptional communication, design, and people
+skills. They must be able to work with a diverse group of people and help lead
+it toward consensus and execution.
+
+Each subteam is led by a member of the root team. The leader is responsible
+for:
+
+* Setting up the subteam:
+
+    * Deciding on the initial membership of the subteam (in consultation with
+      the root team).
+
+    * Working with subteam members to determine and publish subteam policies
+      and mechanics, including the way that subteam members join or leave the
+      team (which should be based on subteam consensus).
+
+* Communicating root team vision downward to the subteam.
+
+* Alerting the root team to subteam RFCs that need global, cross-cutting
+  attention, and to RFCs that have entered the ""final comment period"".
+
+* Ensuring that RFCs and PRs are progressing at a reasonable rate and
+  re-assigning shepherds/reviewers as needed.
+
+* Making final decisions in cases of contentious RFCs that are unable to reach
+  consensus otherwise (should be rare).
+
+The way that subteams communicate internally and externally is left to each
+subteam to decide, but:
+
+* Technical discussion should take place as much as possible in public,
+  ideally on the RFC PRs and on the chat server.
+
+* Subteams should actively seek out discussion and input from stakeholders who
+  are not members of the team.
+
+* Subteams should have some kind of regular meeting or other way of making
+  decisions. The content of this meeting should be summarized with the
+  rationale for each decision -- and, as explained below, decisions should
+  generally be about weighting a set of already-known tradeoffs, not discussing
+  or discovering new rationale.
+
+* Subteams should regularly publish the status of RFCs, PRs, and other news
+  related to their area. Ideally, this should be done to the mailing list or
+  chat server.
+
+### Decision-making
+
+#### Consensus
+
+Sawtooth uses a form of [consensus decision-making][consensus]. In a nutshell
+the premise is that a successful outcome is not where one side of a debate has
+""won"", but rather where concerns from *all* sides have been addressed in some
+way. **This emphatically does not entail design by committee, nor compromised
+design**. Rather, it's a recognition that
+
+> ... every design or implementation choice carries a trade-off and numerous
+> costs. There is seldom a right answer.
+
+Breakthrough designs sometimes end up changing the playing field by eliminating
+tradeoffs altogether, but more often difficult decisions have to be made. **The
+key is to have a clear vision and set of values and priorities**, which is the
+root team's responsibility to set and communicate, and the subteam's
+responsibility to act upon.
+
+Whenever possible, we seek to reach consensus through discussion and design
+revision. Concretely, the steps are:
+
+* Initial RFC proposed, with initial analysis of tradeoffs.
+* Comments reveal additional drawbacks, problems, or tradeoffs.
+* RFC revised to address comments, often by improving the design.
+* Repeat above until ""major objections"" are fully addressed, or it's clear that
+  there is a fundamental choice to be made.
+
+Consensus is reached when most people are left with only ""minor"" objections,
+i.e., while they might choose the tradeoffs slightly differently they do not
+feel a strong need to *actively block* the RFC from progressing.
+
+One important question is: consensus among which people, exactly? Of course, the
+broader the consensus, the better. But at the very least, **consensus within the
+members of the subteam should be the norm for most decisions.** If the root team
+has done its job of communicating the values and priorities, it should be
+possible to fit the debate about the RFC into that framework and reach a fairly
+clear outcome.
+
+[consensus]: http://en.wikipedia.org/wiki/Consensus_decision-making
+
+#### Lack of consensus
+
+In some cases, though, consensus cannot be reached. These cases tend to split
+into two very different camps:
+
+* ""Trivial"" reasons, e.g., there is not widespread agreement about naming, but
+  there is consensus about the substance.
+
+* ""Deep"" reasons, e.g., the design fundamentally improves one set of concerns
+  at the expense of another, and people on both sides feel strongly about it.
+
+In either case, an alternative form of decision-making is needed.
+
+* For the ""trivial"" case, usually either the RFC shepherd or subteam leader
+  will make an executive decision.
+
+* For the ""deep"" case, the subteam leader is empowered to make a final
+  decision, but should consult with the rest of the root team before doing so.
+
+#### Final Comment Period (FCP)
+
+Each RFC has a shepherd drawn from the relevant subteam. The shepherd is
+responsible for driving the consensus process -- working with both the RFC
+author and the broader community to dig out problems, alternatives, and
+improved design, always working to reach broader consensus.
+
+At some point, the RFC comments will reach a kind of ""steady state"", where no
+new tradeoffs are being discovered, and either objections have been addressed,
+or it's clear that the design has fundamental downsides that need to be
+weighed.
+
+At that point, the shepherd will announce that the RFC is in a ""final comment
+period"" (which lasts for one week). This is a kind of ""last call"" for strong
+objections to the RFC. **The announcement of the final comment period for an
+RFC should be very visible**; it should be included in the subteam's periodic
+communications.
+
+After the final comment period, the subteam can make a decision on the RFC. The
+role of the subteam at that point is *not* to reveal any new technical issues
+or arguments; if these come up during discussion, they should be added as
+comments to the RFC, and it should undergo another final comment period.
+
+Instead, the subteam decision is based on **weighing the already-revealed
+tradeoffs against the project's priorities and values** (which the root team is
+responsible for setting, globally). In the end, these decisions are about how
+to weight tradeoffs. The decision should be communicated in these terms,
+pointing out the tradeoffs that were raised and explaining how they were
+weighted, and **never introducing new arguments**.
+
+### Initial Subteams
+
+The following is a proposed list of initial subteams which is subject to change
+at the discretion of the root team, once it has formed.",260,2018-04-06 14:52:34,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/177283786,https://github.com/hyperledger/sawtooth-rfcs/pull/6#discussion_r177283786,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,"Does present mean that the parent block is already in the block manager? It is ambiguous as to whether it is that or ""present in the List<Block>"".",c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-03-21 21:34:04,176245724,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present",112,2018-03-21 21:49:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176245724,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r176245724,boydjohnson
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,"""completer can release its hold"": Does the completer have a hold on the block or is it the Block Manager?",c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-03-21 21:40:26,176247233,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.",177,2018-03-21 21:40:26,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/176247233,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r176247233,boydjohnson
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,I read that to mean that the parent of the list[0] element is already present in tree managed by the block manager.,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 18:42:28,186828201,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present",112,2018-05-08 18:42:28,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186828201,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186828201,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,of 1 or of _at least_ 1? (viz holds placed by other objects),c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:26:59,186840495,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.
+
+### Persisting Blocks
+
+In order to support persisting blocks to an alternative not-in-memory location,
+a generic __block store__ interface is defined and the __block manager__
+supports adding any number of block stores and transferring ownership of the
+blocks to these stores.
+
+Persisting blocks is handled by the following methods:
+
+`add_store(store_name: String, store: Store)`
+
+    Take ownership of the given store and enable it for persisting blocks. The
+    store must be referenced from other methods with `store_name`.
+
+    Raises an exception if a store with `store_name` already exists.
+
+`persist(head: String, store_name: String)`
+
+    Atomically ensure that all blocks on the branch starting with head are
+    in the store.
+
+## Block Store Interface
+
+The __block store__ interface is the interface that must be implemented by an
+object in order for it to be added to the block tree as a ""store"". The
+following is a listing of the interface's methods:
+
+`put(blocks: List<Block>)`
+
+    Atomically add the given blocks to the store.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids.
+
+`iter() -> Iter<Block>`
+
+    Create an iterator over all blocks in the store.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The following provides more detail on how the API present above is implemented.
+
+## Internal Representation
+
+Internally, the block manager consists of:
+
+- A __main cache__ which, at a minimum, contains all blocks that have not been
+  persisted.
+- A map of store names to stores.
+
+Internally, the commit store consists of two components:
+
+- A database object which is responsible for persisting the committed fork to
+  disk and retrieving committed blocks from disk.
+- An __LRU cache__, which is responsible for ensuring that frequently accessed
+  blocks do not incur database reads. All writes are done directly to disk, but
+  this behavior could be changed in the future. The LRU cache keeps the last N
+  recently used blocks, where ""used"" can mean either a read or a write.
+
+## Ensuring Integrity
+
+As is heavily implied by the API, the block manager uses __reference counting__
+internally to guarantee integrity and to allow external components to place
+holds on specific blocks. For every block managed by the block manager, a
+reference count is maintained (sometimes implicitly).
+
+This one reference count is a sum of both:
+
+1. The count of all other blocks in the block manager that depend directly
+   on this block
+2. The count of all calls to `ref_block` minus the count of all calls to
+   `unref_block`
+
+In order to avoid keeping a reference count and copy of every persisted block
+in the main cache, __anchors__ are placed in the main cache to indicate that
+the rest of the branch has been persisted to some store. An __anchor__ is used
+to represent a block that is in a store and not in the __main cache__ and
+consists of:
+
+- The block id of the block that is in the store
+- A list of names of stores that the block is in
+- A reference count of blocks that depend on the anchor but are not in a store
+  the block is in
+
+A block in a store and not the __main cache__ has an implicit reference count
+of 1 if it is not the tip of the branch in the store. If it is the tip of the",266,2018-05-08 19:26:59,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186840495,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186840495,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,is main cache a block store? ,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:27:59,186840754,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.
+
+### Persisting Blocks
+
+In order to support persisting blocks to an alternative not-in-memory location,
+a generic __block store__ interface is defined and the __block manager__
+supports adding any number of block stores and transferring ownership of the
+blocks to these stores.
+
+Persisting blocks is handled by the following methods:
+
+`add_store(store_name: String, store: Store)`
+
+    Take ownership of the given store and enable it for persisting blocks. The
+    store must be referenced from other methods with `store_name`.
+
+    Raises an exception if a store with `store_name` already exists.
+
+`persist(head: String, store_name: String)`
+
+    Atomically ensure that all blocks on the branch starting with head are
+    in the store.
+
+## Block Store Interface
+
+The __block store__ interface is the interface that must be implemented by an
+object in order for it to be added to the block tree as a ""store"". The
+following is a listing of the interface's methods:
+
+`put(blocks: List<Block>)`
+
+    Atomically add the given blocks to the store.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids.
+
+`iter() -> Iter<Block>`
+
+    Create an iterator over all blocks in the store.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The following provides more detail on how the API present above is implemented.
+
+## Internal Representation
+
+Internally, the block manager consists of:
+
+- A __main cache__ which, at a minimum, contains all blocks that have not been",227,2018-05-08 19:27:59,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186840754,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186840754,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,What does it mean to say that the commit store can be added to the block tree? The block tree class has a member which which implements store? Does a tree have to be instantiated with a store implementation? I'm generally unclear on the relationship between block stores and trees. Maybe a fuller description of the block tree class would help.,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:37:58,186843290,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed",96,2018-05-08 19:37:58,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186843290,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186843290,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,Is the commit store a member of the block manager?,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:38:24,186843391,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__",95,2018-05-08 19:38:24,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186843391,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186843391,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,Do the chain controller and completer both hold references to the same block manager or does the chain controller own the block manager? I assume the former. In which case the controller and the completer must both signal the block manager before / after certain operations. ,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:46:17,186845473,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just",167,2018-05-08 19:46:17,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186845473,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186845473,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,"To clarify regarding above comment about ownership vs. signaling of the block manager...
""Instead, the completer first calls the __block manager__ to place a `ref_block` on A ~causing~ and"" ",c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:48:21,186846089,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and",172,2018-05-08 19:48:21,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186846089,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186846089,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,And what does it mean for the chain controller to take ownership of B? Would the completer actually be passing a reference to the controller for an object managed by the block manager?,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 19:49:51,186846460,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.",177,2018-05-08 19:49:51,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186846460,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186846460,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,This seems confusing to have reference counting on the anchor and on the block. Would it make sense to just have reference counting managed by the tree? I'm assuming all blocks are referenced in the tree(s)?,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-08 20:02:35,186849779,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.
+
+### Persisting Blocks
+
+In order to support persisting blocks to an alternative not-in-memory location,
+a generic __block store__ interface is defined and the __block manager__
+supports adding any number of block stores and transferring ownership of the
+blocks to these stores.
+
+Persisting blocks is handled by the following methods:
+
+`add_store(store_name: String, store: Store)`
+
+    Take ownership of the given store and enable it for persisting blocks. The
+    store must be referenced from other methods with `store_name`.
+
+    Raises an exception if a store with `store_name` already exists.
+
+`persist(head: String, store_name: String)`
+
+    Atomically ensure that all blocks on the branch starting with head are
+    in the store.
+
+## Block Store Interface
+
+The __block store__ interface is the interface that must be implemented by an
+object in order for it to be added to the block tree as a ""store"". The
+following is a listing of the interface's methods:
+
+`put(blocks: List<Block>)`
+
+    Atomically add the given blocks to the store.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids.
+
+`iter() -> Iter<Block>`
+
+    Create an iterator over all blocks in the store.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The following provides more detail on how the API present above is implemented.
+
+## Internal Representation
+
+Internally, the block manager consists of:
+
+- A __main cache__ which, at a minimum, contains all blocks that have not been
+  persisted.
+- A map of store names to stores.
+
+Internally, the commit store consists of two components:
+
+- A database object which is responsible for persisting the committed fork to
+  disk and retrieving committed blocks from disk.
+- An __LRU cache__, which is responsible for ensuring that frequently accessed
+  blocks do not incur database reads. All writes are done directly to disk, but
+  this behavior could be changed in the future. The LRU cache keeps the last N
+  recently used blocks, where ""used"" can mean either a read or a write.
+
+## Ensuring Integrity
+
+As is heavily implied by the API, the block manager uses __reference counting__
+internally to guarantee integrity and to allow external components to place
+holds on specific blocks. For every block managed by the block manager, a
+reference count is maintained (sometimes implicitly).
+
+This one reference count is a sum of both:
+
+1. The count of all other blocks in the block manager that depend directly
+   on this block
+2. The count of all calls to `ref_block` minus the count of all calls to
+   `unref_block`
+
+In order to avoid keeping a reference count and copy of every persisted block
+in the main cache, __anchors__ are placed in the main cache to indicate that
+the rest of the branch has been persisted to some store. An __anchor__ is used
+to represent a block that is in a store and not in the __main cache__ and
+consists of:
+
+- The block id of the block that is in the store
+- A list of names of stores that the block is in
+- A reference count of blocks that depend on the anchor but are not in a store
+  the block is in",263,2018-05-08 20:02:35,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/186849779,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r186849779,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,"Hi, @dcmiddle . Since I am implementing this right now, I thought I should respond. The main cache is in-memory and is not a block store. It is a logical space. Right now in my implementation it is a HashMap of block ids to Block - reference count tuples, wrapped in a lock. Specifically Rc<RwLock<HashMap<String, (Rc<Block>, i64, i64)>>>.",c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-10 14:48:04,187353045,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just
+completed a block B and would like to pass it to the chain controller. If the
+completer were to just pass the chain controller B through a queue or shared
+memory, the chain controller could decide to `unref_block` A before it receives
+B, causing B to have a missing predecessor when it arrives at the chain
+controller. Instead, the completer first places a `ref_block` on A, causing and
+then it passes B to the chain controller. If the chain controller decides to
+`unref_block` A now, the block manage will ensure it is not actually dropped,
+since there is still an open hold on the block. Finally, when the chain
+controller takes ownership of B, it can send a signal back to the completer
+that it has been received and the completer can release its hold.
+
+### Persisting Blocks
+
+In order to support persisting blocks to an alternative not-in-memory location,
+a generic __block store__ interface is defined and the __block manager__
+supports adding any number of block stores and transferring ownership of the
+blocks to these stores.
+
+Persisting blocks is handled by the following methods:
+
+`add_store(store_name: String, store: Store)`
+
+    Take ownership of the given store and enable it for persisting blocks. The
+    store must be referenced from other methods with `store_name`.
+
+    Raises an exception if a store with `store_name` already exists.
+
+`persist(head: String, store_name: String)`
+
+    Atomically ensure that all blocks on the branch starting with head are
+    in the store.
+
+## Block Store Interface
+
+The __block store__ interface is the interface that must be implemented by an
+object in order for it to be added to the block tree as a ""store"". The
+following is a listing of the interface's methods:
+
+`put(blocks: List<Block>)`
+
+    Atomically add the given blocks to the store.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids.
+
+`iter() -> Iter<Block>`
+
+    Create an iterator over all blocks in the store.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+The following provides more detail on how the API present above is implemented.
+
+## Internal Representation
+
+Internally, the block manager consists of:
+
+- A __main cache__ which, at a minimum, contains all blocks that have not been",227,2018-05-10 14:48:04,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/187353045,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r187353045,boydjohnson
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,The block manager has a map of store names to stores. The commit store would be one such member of that map.,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-15 18:40:50,188394527,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__",95,2018-05-15 18:40:50,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/188394527,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r188394527,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,"A block store is just a place to persist blocks, specifically a single branch of blocks. Multiple stores can be added to the manager and branches can be persisted to multiple stores. So, for example, you could have a faster local disk store and slower, perhaps network backed, store for archiving very old blocks.",c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-15 18:42:40,188395109,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed",96,2018-05-15 18:42:41,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/188395109,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r188395109,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/5,https://github.com/hyperledger/sawtooth-rfcs/pull/5,Both the chain controller and completer hold references. This is to allow the completer to make sure blocks arrive at the chain controller with all their predecessors and to ensure both the chain controller and completer are in sync wrt what blocks are present.,c0a9d684e02a783cb5456ff5218951e3e47fc94f,2018-05-15 18:44:07,188395532,"@@ -0,0 +1,367 @@
+- Feature Name: `block-manager`
+- Start Date: 2018-03-13
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes new architectural components of the validator that handle
+the management of blocks correctly and help remove a known race condition that
+can cause network nodes to fork.
+
+# Motivation
+[motivation]: #motivation
+
+The RFC aims to improve Sawtooth in two ways:
+
+1. By removing a race-condition  which can cause parent blocks required for
+   validation of some block to be dropped while the block is transferred from
+   the completer to the chain controller.
+2. Providing additional guarantees about the blocks being processed.
+3. Simplifying and generalizing the procedure for persisting blocks.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+This RFC aims to improve the handling of blocks by introducing some new classes
+and interfaces. The requirements for the design presented in this RFC are that
+it:
+
+- Shall store blocks for later
+- Shall ensure integrity of blocks stored
+- Shall provide a mechanism for preventing required blocks from being dropped
+- Shall provide a mechanism for persisting to a store
+
+The constraints for the design presented in this RFC are that:
+
+- The API must work for both forking vs non-forking consensus mechanisms
+- The API defined must be reusable, meaning it provides generic methods that
+  make few assumptions about how the API will be used
+
+The following new concepts are used by this design and are defined here:
+
+For a collection of blocks, __integrity__ is defined to mean that every block
+in the collection is accompanied by its parent.
+
+A __branch__ is a collection of blocks such that:
+
+1. The blocks are sorted in order of increasing block number
+2. The branch has integrity
+
+A __block tree__ is a collection of branches with a single common ancestor or
+__root__.
+
+The following is a visualization of an example block tree containing blocks
+{0, A, B, C, ..., H}.
+
+```
+0 (root)
+|
+|\
+A |
+| E
+|\
+B |
+| F
+| |\
+C | |
+| G |
+|   H
+D
+```
+
+In the above example, the block tree contains multiple branches. They are (0,
+A, B, C, D), (0, E), (0, A, F, G), and (0, A, F, H).
+
+In order to satisfy the requirements and constraints of this RFC, the
+__block manager__ class and __block store__ interface were created.
+
+A __block store__ is an interface which supports storing and retrieving blocks.
+It can be backed by any type of storage, for example disk or network. Because a
+blockchain's length grows indefinitely, part of it eventually has to be
+archived in some way. The __block store__ interface is intended to help solve
+the problem of persisting parts of the __block tree__ in a way that does not
+make assumptions about how it is persisted or what parts of the tree are
+persisted.
+
+The __block manager__ is a collection of __block tree__ s (though it usually
+contains 1) that also supports methods for:
+
+- Ensuring that blocks depended on by external objects are not removed.
+- Persisting blocks to a __block store__.
+
+In addition to the __block manager__ class and __block store__ interface, a
+__commit store__ class was created The commit store is a __block store__
+implementation that can be added to the block tree for persisting committed
+blocks to disk. It also implements an LRU cache which holds some number of most
+recently accessed blocks in memory. The __commit store__ replaces the what was
+previously called the ""block store"".
+
+## Block Manager API
+
+The following is a listing of the __block manager__ methods and their function:
+
+`put(branch: List<Block>)`
+
+Atomically adds the given blocks to the tree.
+
+The blocks passed must form a branch meaning they satisfy the following
+conditions:
+
+1. The first block's parent is present
+2. The blocks are sorted in order of increasing block num
+3. For any two sequential blocks in the list, the second block's parent is
+   the first block
+
+If any of these three conditions are not met, an exception is raised.
+
+All blocks added will have a reference count of 1 if the operation is
+successful, including the tip.
+
+`get(block_ids: List<String>) -> Iter<Block>`
+
+    Atomically get the blocks with the given ids. This passes through to any
+    registered stores as necessary.
+
+`branch(tip: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root.
+
+`branch_diff(tip: String, exclude: String) -> Iter<Block>`
+
+    Create an iterator over blocks on a given branch starting with the block
+    with the id given in `tip` and traversing from the tip to the root,
+    excluding blocks that are also in the branch starting with id in exclude.
+
+### Holding Blocks
+
+In order to avoid removing blocks that are in the process of being worked on,
+the block manager provides an operation to place a `hold` on a block. When this
+operation is called, the block manager makes a guarantee that the block will
+not be dropped until corresponding `drop` is called. Consequently, when an
+object that requests a hold is done with the block, it must call `drop` to
+signal it is done with the block.
+
+Ensuring that blocks depended on by external objects are not removed is handled
+by the following methods:
+
+`ref_block(block_id: String)`
+
+    Ensure that the block with the given block id is not dropped until a
+    corresponding `unref_block()` with the same block id is called.
+
+`unref_block(block_id: String)`
+
+    Release a previous `ref_block()` on a block. May result in the block being
+    removed from the tree.
+
+    Raises an exception if the reference count falls below 0, which indicates
+    a bug in the application code.
+
+The following example illustrates how the `ref_block` operation is used to
+prevent blocks from being dropped when they are transferred from the completer
+to the chain controller.
+
+Assume that the chain controller has a block A and that the completer has just",167,2018-05-15 18:44:07,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/188395532,https://github.com/hyperledger/sawtooth-rfcs/pull/5#discussion_r188395532,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"typo?
:s/as/as well as",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-03-15 20:44:22,174926294,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_Nakamoto-style_ consensus algorithms as _elected leader_ algorithms such as",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/174926294,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r174926294,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,PBFT is the general case. rBFT is probably the first PBFT-like consensus we'll add.,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-03-15 20:45:40,174926672,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_Nakamoto-style_ consensus algorithms as _elected leader_ algorithms such as
+rBFT.",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/174926672,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r174926672,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"The hyperledger architecture paper on consensus breaks these methods into _lottery_ (PoET, POW, etc) and _voting_ (PBFT, RBFT, RAFT, etc) based elections. The use of _elected leader_ could be misunderstood. (see https://www.hyperledger.org/wp-content/uploads/2017/08/Hyperledger_Arch_WG_Paper_1_Consensus.pdf)",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-03-15 20:52:10,174928628,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_Nakamoto-style_ consensus algorithms as _elected leader_ algorithms such as
+rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_elected leader_ algorithm or a _Nakamoto-style_ algorithm.",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/174928628,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r174928628,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,This is too vague for me. Can you give an example of why and what non-blockchain app needs this?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 16:26:39,179809413,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179809413,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179809413,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Is this a convenience method wrapping GetState(address) or does it imply settings outside of global state?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 16:52:04,179815641,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |",235,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179815641,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179815641,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"Does ""and validated"" imply that validator will validate transactions before consensus considers whether the block is valid or has precedence?
",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 16:54:29,179816177,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179816177,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179816177,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Do you envision sending the entire block or just the block header?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 16:54:51,179816256,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179816256,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179816256,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Will this return the entire block or just the block header?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 16:55:14,179816344,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |",237,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179816344,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179816344,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Block Store and Block Cache from the diagram are not defined here. No need for unnecessary work if the definitions remain the same as here: https://sawtooth.hyperledger.org/docs/core/releases/latest/architecture/journal.html. ,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-06 17:01:08,179817718,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |
+
+While the consensus engine should maintain a cache of blocks internally, this method is provided to:
+1. Allow cache entries to expire without losing the block forever
+2. Allow the engine to handle getting a block before its parent
+
+### Engine Methods
+The following methods will be implemented by Consensus Engines. They are used
+to initialize and shutdown the engine, when a new validator starts up, or when
+the consensus engine changes.
+
+| Method | Description |
+| --- | --- |
+| Startup() | Startup and synchronize with any peers if necessary. |
+| Shutdown() | Shutdown and notify peers if necessary. |
+
+## Consensus Engine Architecture Integration
+The following describes the interactions between the consensus engine and the
+other major architectural components of the validator.
+
+![consensus engine architecture](../images/consensus-engine-architecture.svg ""Consensus Engine Architecture"")
+
+**Consensus Engine**
+
+The Consensus Engine is a separate process that implements a consensus
+algorithm and communicates with the Consensus Proxy through the Dispatcher. It
+can also communicate with peers using consensus messages. Its role is to direct
+block creation and chain management. It must implement the interface described
+above.
+
+**Consensus Proxy**
+
+The Consensus Proxy mediates interactions between the Consensus Engine and the
+rest of the Validator. This includes marshaling and unmarshaling messages from
+the Dispatcher, passing block notifications to the Consensus Engine, and
+passing commands from the Consensus Engine to the Chain Controller and Block
+Publisher.
+
+**Block Validator**
+
+The Block Validator validates blocks except from the perspective of consensus.
+
+**Chain Controller**
+
+The Chain Controller manages both committed and uncommitted chain heads. The
+Chain Controller is driven by the Consensus Engine and does not know about
+block validity or consensus.
+
+**Block Publisher**
+
+The Block Publisher handles the creation of blocks. It is directed by the
+Consensus Engine on when to do this. When a block is finalized, it is forwarded
+to the Block Validator.
+",328,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/179817718,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r179817718,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"For example, it would be nice if we can re-use consensus engines to implement a distributed oracle.",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-04-21 11:51:25,183207991,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/183207991,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r183207991,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,How does one register a consensus engine with the validator?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-05-13 22:57:15,187815946,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |
+
+While the consensus engine should maintain a cache of blocks internally, this method is provided to:
+1. Allow cache entries to expire without losing the block forever
+2. Allow the engine to handle getting a block before its parent
+
+### Engine Methods
+The following methods will be implemented by Consensus Engines. They are used
+to initialize and shutdown the engine, when a new validator starts up, or when
+the consensus engine changes.
+
+| Method | Description |
+| --- | --- |
+| Startup() | Startup and synchronize with any peers if necessary. |
+| Shutdown() | Shutdown and notify peers if necessary. |
+",284,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/187815946,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r187815946,kulkarniamol
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"That was what the original design called for. The API was changed to include a `CheckBlocks()` method which signals to the validator that it should ""check that the block can be committed"" which allows the validator to send blocks to consensus before they are validated. At this time however, blocks are still validated prior to sending them to consensus. The alternative requires two new feautres: 1. the ability to process all consensus-specific transactions separately from regular transactions and 2. the ability to read, write, and verify state sub-trees.",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:23:36,219617412,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617412,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617412,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Addressed,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:23:44,219617453,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617453,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617453,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"In the end, everything but batches are sent.",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:24:10,219617573,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617573,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617573,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,yes,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:24:18,219617609,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |",235,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617609,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617609,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"In the end, everything but batches are sent.",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:24:31,219617658,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |",237,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617658,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617658,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,This is more detail than I think belongs in the RFC and I will include it in the documentation.,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:25:03,219617772,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |
+
+While the consensus engine should maintain a cache of blocks internally, this method is provided to:
+1. Allow cache entries to expire without losing the block forever
+2. Allow the engine to handle getting a block before its parent
+
+### Engine Methods
+The following methods will be implemented by Consensus Engines. They are used
+to initialize and shutdown the engine, when a new validator starts up, or when
+the consensus engine changes.
+
+| Method | Description |
+| --- | --- |
+| Startup() | Startup and synchronize with any peers if necessary. |
+| Shutdown() | Shutdown and notify peers if necessary. |
+",284,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617772,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617772,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,These are not very relevant to the discussion and are superseded by the block manager.,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 20:25:43,219617914,"@@ -0,0 +1,320 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as rBFT, while continuing to support
+PoET/SGX, PoET/Simulator, and devmode consensus modules.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either an
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string
+        Previous Id: string
+        Index: unsigned integer
+        Consensus: bytes
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API below is presented as a set of methods in order to
+simplify their presentation and explanation. Each method is implemented with a
+pair of (Request, Response) messages and the Consensus Proxy is responsible for
+handling this transformation within the Validator.
+
+The methods presented below are only meant to describe roughly what the
+high-level interactions between the Consensus Engine and the Validator will be.
+The names, parameters, and exact behavior of these methods are subject to
+change based on additional constraints and requirements discovered during
+implementation.
+
+The methods defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Event Handlers
+3. Block Creation
+4. Block Management
+5. Queries
+6. Engine
+
+### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+### Event Handlers
+The following methods will be implemented by Consensus Engines. They are used
+to update the Consensus Engine on external events. These methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| OnMessageReceived(message) | Called when a new consensus message is received |
+| OnNewBlockReceived(block) | Called when a new block is received and validated |
+| OnAddPeer(peer) | Called when a new peer is added |
+| OnDropPeer(peer) | Called when a peer is dropped |
+
+### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, OnNewBlockReceived() will be called with the block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. |
+| DropBlock(block_id) | Remove the given block from the system. The main purpose of this method is to allow the Consensus Engine to notify the rest of the system that a block is invalid from the perspective of consensus. |
+
+### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |
+
+While the consensus engine should maintain a cache of blocks internally, this method is provided to:
+1. Allow cache entries to expire without losing the block forever
+2. Allow the engine to handle getting a block before its parent
+
+### Engine Methods
+The following methods will be implemented by Consensus Engines. They are used
+to initialize and shutdown the engine, when a new validator starts up, or when
+the consensus engine changes.
+
+| Method | Description |
+| --- | --- |
+| Startup() | Startup and synchronize with any peers if necessary. |
+| Shutdown() | Shutdown and notify peers if necessary. |
+
+## Consensus Engine Architecture Integration
+The following describes the interactions between the consensus engine and the
+other major architectural components of the validator.
+
+![consensus engine architecture](../images/consensus-engine-architecture.svg ""Consensus Engine Architecture"")
+
+**Consensus Engine**
+
+The Consensus Engine is a separate process that implements a consensus
+algorithm and communicates with the Consensus Proxy through the Dispatcher. It
+can also communicate with peers using consensus messages. Its role is to direct
+block creation and chain management. It must implement the interface described
+above.
+
+**Consensus Proxy**
+
+The Consensus Proxy mediates interactions between the Consensus Engine and the
+rest of the Validator. This includes marshaling and unmarshaling messages from
+the Dispatcher, passing block notifications to the Consensus Engine, and
+passing commands from the Consensus Engine to the Chain Controller and Block
+Publisher.
+
+**Block Validator**
+
+The Block Validator validates blocks except from the perspective of consensus.
+
+**Chain Controller**
+
+The Chain Controller manages both committed and uncommitted chain heads. The
+Chain Controller is driven by the Consensus Engine and does not know about
+block validity or consensus.
+
+**Block Publisher**
+
+The Block Publisher handles the creation of blocks. It is directed by the
+Consensus Engine on when to do this. When a block is finalized, it is forwarded
+to the Block Validator.
+",328,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219617914,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219617914,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"Just a suggestion--I would add ""Also known as Nakamoto-style consensus""",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 21:54:22,219636812,"@@ -0,0 +1,356 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).",66,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219636812,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219636812,danintel
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"Just a suggestion--I would add ""Also known as classical consensus""",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 21:54:42,219636876,"@@ -0,0 +1,356 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.",55,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219636876,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219636876,danintel
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,"Where does the ""Write"" (from ""Read/Write Access"") come in here.  I think you mean ""read/write"" access on line 128.",7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-21 21:57:56,219637496,"@@ -0,0 +1,356 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.",128,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219637496,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219637496,danintel
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,Dead link; https://tendermint.com/docs/spec/abci/ maybe?,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-09-23 18:03:38,219706560,"@@ -0,0 +1,356 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.
+
+**R6 - Reusability**
+
+The Consensus Engine interface and implementations of the interface should be
+reusable in other applications. Specifically, the interface should support
+non-blockchain applications, such as distributed oracles and distributed
+ledgers not focused around blocks.
+
+## Data Structures
+
+**Blocks**
+
+In order to support R6, ""block"" is defined to mean the following data structure
+within the context of the consensus engine. When used in a blockchain context,
+this block definition is equivalent to those parts of the block that are
+relevant to consensus. When used outside the blockchain context, the
+interpretation of this data structure is context dependent.
+
+    Block {
+        Id: string               // This block's identifier
+        Previous Id: string      // This block's parent's identifier
+        Index: unsigned integer  // The ""height"" of this block
+        Consensus: bytes         // The opaque consensus payload for this block
+        Signer: bytes            // The signer of this block
+        Summary: bytes           // A digest of the contents of the block
+    }
+
+**Consensus Messages**
+
+In order to support arbitrary communication between nodes, a generic message
+type is defined with a payload that is opaque to the validator. The following
+data structure is used for this:
+
+    ConsensusMessage {
+        MessageType: string
+        Payload: bytes
+    }
+
+Payload is the opaque payload to send to other nodes. The interpretation of
+MessageType is left to the Consensus Engine implementation.
+
+## API
+
+The Consensus Engine API is split into two types of interactions between the
+Validator and the Consensus Engine implementation: services and updates. The
+Validator provides a set of _services_ to the Consensus Engine which are pairs
+of (Request, Response) messages that allow the Engine to get information from
+the Validator and send commands to the Validator. These service calls are
+synchronous and on-demand. The Validator also provides _updates_ to the Engine,
+which alert the Engine of new events that have occurred. Updates are sent
+asynchronously as they occur.
+
+Below is a listing of the services and updates defined by the Consensus Engine
+API. Each service is implemented with a pair of (Request, Response) messages
+and the Consensus Proxy is responsible for handling this transformation within
+the Validator. Each update is implemented as a single Notification message and
+the Consensus Notifier is responsible for creating and sending these messages
+at the request of the Validator internals.
+
+### Services
+
+The services defined by the Consensus Engine interface are split into the
+following groups:
+
+1. P2P Messaging
+2. Block Creation
+3. Block Management
+4. Queries
+
+#### P2P Messaging Methods
+The following methods are provided to Consensus Engines for sending consensus
+messages to other nodes on the network. These methods support R1.
+
+| Method | Description |
+| --- | --- |
+| SendTo(peer, message) | Send a consensus message to a specific, connected peer |
+| Broadcast(message) | Broadcast a message to all peers |
+
+#### Block Creation Methods
+The following methods will be provided to Consensus Engines for controlling
+block creation. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| InitializeBlock() |  Initialize a new block based on the current chain head and start adding batches to it. |
+| SummarizeBlock() -> digest |  Stop adding to the current block and summarize the contents of the block with a digest.  |
+| FinalizeBlock(data) -> block_id | Stop adding batches to the current block and finalize it. Include the given consensus data in the block. If this call is successful, a BlockNew update will be received with the new block. |
+| CancelBlock() |  Stop adding batches to the current block and abandon it. |
+
+#### Block Management Methods
+The following methods will be provided to Consensus Engines for controlling
+chain updates. These methods support R3.
+
+| Method | Description |
+| --- | --- |
+| FailBlock(block_id) | Mark the block as failed by consensus. This will also fail all descendants. |
+| IgnoreBlock(block_id) | Mark the block as considered, but do not fail or commit the block. |
+| CheckBlocks(blocks) | Check that the blocks can be successfully committed. The results of all checks will be send as BlockValid and BlockInvalid updates. |
+| CommitBlock(block_id) | Set the chain head to the given block id. The Chain Controller would handle the details here. This block must already have been checked. |
+
+#### Query Methods
+The following methods will be provided to Consensus Engines for getting
+information from the validator.
+
+| Method | Description |
+| --- | --- |
+| GetSetting(setting) -> data | Read the current value of the setting. Supports R4. |
+| GetState(address) -> data | This is needed to read values from arbitrary addresses used by consensus (eg., validator registry). Supports R5. |
+| GetBlock(block_id) -> block | Retrieve consensus-related information about a block. |
+| GetChainHead() -> block | Get the current committed chain head. |
+
+While the consensus engine should maintain a cache of blocks internally, this method is provided to:
+1. Allow cache entries to expire without losing the block forever
+2. Allow the engine to handle getting a block before its parent
+
+### Updates
+
+The following updates are sent to Consensus Engines by the validator as they
+happen. They are used to update the Consensus Engine on external events. These
+methods support R1-R3.
+
+| Method | Description |
+| --- | --- |
+| PeerConnected(peer_info) | Called when a new peer is added |
+| PeerDisconnected(peer_id) | Called when a peer is dropped |
+| PeerMessage(message) | Called when a new consensus message is received |
+| BlockNew(block) | Called when a new block is received and validated |
+| BlockValid(block_id) | Called when a block check succeeds |
+| BlockInvalid(block_id) | Called when a block check fails |
+| BlockInvalid(block_id) | Called when a block commit completes |
+
+### API Contracts
+
+A few expectations, or _contracts_, are part of this API and must be upheld by
+both sides of the API in order for it to function correctly. These _contracts_
+are defined below:
+
+1. Following successful registration, `BlockNew` updates will only be sent to
+   the Consensus Engine by the Validator if the block's parent has already been
+   sent.
+
+2. The Consensus Engine must ""render an opinion"" on every block sent to it by the
+   validator by calling either `FailBlock()`, `IgnoreBlock()`, or
+   `CommitBlock()`. If a Consensus Engine implementation does not uphold this
+   contract, the validator will hold onto blocks in memory indefinitely,
+   causing a memory leak.
+
+3. Before a block can be committed with `CommitBlock()` it must first be
+   checked with `CheckBlocks()`. Failing to check a block before committing
+   will result in an error. This is to enable internal optimizations within the
+   Validator such as lazily evaluating blocks after consensus has rendered an
+   opinion.
+
+4. The state of block construction within the Validator will not change without
+   a request from the Consensus Engine.
+
+## Consensus Engine Architecture Integration
+The following describes the interactions between the consensus engine and the
+other major architectural components of the validator.
+
+![consensus engine architecture](../images/consensus-engine-architecture.svg ""Consensus Engine Architecture"")
+
+**Consensus Engine**
+
+The Consensus Engine is a separate process that implements a consensus
+algorithm and communicates with the Consensus Proxy through the Dispatcher. It
+can also communicate with peers using consensus messages. Its role is to direct
+block creation and chain management. It must implement the interface described
+above.
+
+**Consensus Proxy**
+
+The Consensus Proxy mediates interactions between the Consensus Engine and the
+rest of the Validator. This includes marshaling and unmarshaling messages from
+the Dispatcher, passing block notifications to the Consensus Engine, and
+passing commands from the Consensus Engine to the Chain Controller and Block
+Publisher.
+
+**Consensus Notifier**
+
+The Consensus Notifier handles marshaling and sending notifications to the
+Validator. It is passed to validator components that are responsible for
+sending notifications.
+
+**Block Validator**
+
+The Block Validator validates blocks except from the perspective of consensus.
+
+**Chain Controller**
+
+The Chain Controller manages both committed and uncommitted chain heads. The
+Chain Controller is driven by the Consensus Engine and does not know about
+block validity or consensus.
+
+**Block Publisher**
+
+The Block Publisher handles the creation of blocks. It is directed by the
+Consensus Engine on when to do this. When a block is finalized, it is forwarded
+to the Block Validator.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+1. Requires additional work to maintain existing consensus modules.
+2. Requires work to change existing architecture.
+3. Moving consensus to a separate process and requiring additional
+   (de)serialization may degrade performance.
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+This design balances the existing architectural constraints of the validator
+against the desire for an abstract, uniform consensus interface.
+
+# Prior art
+[prior-art]: #prior-art
+
+This design borrows some ideas from the [Tendermint ABCI][tendermint] and the
+[Parity Consensus Engine][parity] interface.
+
+[tendermint]: https://tendermint.readthedocs.io/en/master/app-development.html#abci-design)",,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/219706560,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r219706560,kirkwood
https://github.com/hyperledger/sawtooth-rfcs/pull/4,https://github.com/hyperledger/sawtooth-rfcs/pull/4,This should just say Read,7feec961d2b7e2ddac0206e8e3b13f8783a9de31,2018-10-03 16:02:05,222369535,"@@ -0,0 +1,356 @@
+- Feature Name: consensus-api
+- Start Date: 2018-03-15
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+This RFC describes a new Consensus API for supporting both the existing
+_lottery_ consensus algorithms as well as _voting_ algorithms
+such as rBFT.
+
+# Motivation
+[motivation]: #motivation
+
+The motivation for this RFC is to support integrating new non-lottery-based
+consensus mechanisms with Sawtooth, such as raft and rBFT, while continuing to
+support PoET/SGX, PoET/Simulator, and devmode consensus algorithms.
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+The Consensus API was developed after researching a variety of consensus
+algorithms, consensus interfaces, and the consensus problem in general. Out of
+this research came the following observations:
+
+1. All consensus algorithms describe a state machine.
+2. All consensus algorithm state transitions occur as a result of:
+
+    a. Receiving a consensus message from a peer
+    b. Receiving a message from a client
+    c. An internal interrupt
+
+These observations led to the creation of the _Consensus Engine_ abstraction,
+which can be used to implement any consensus algorithm that is either a
+_voting_ algorithm or a _lottery_ algorithm.
+
+## Comparison of Algorithm Types
+
+The following describes the differences between what we refer to as _voting_
+and _lottery_ algorithms for the purpose of understanding how both are handled
+by the consensus interface.
+
+In _voting_ consensus algorithms:
+
+- A single node is authorized to make commits at any given time.
+- One or more nodes in the network is required to maintain a ""global view"" of
+  the network.
+- Adding and removing nodes from the network is non-trivial and requires
+  coordination and network-wide agreement.
+- Many ""consensus-specific"" messages are passed between nodes on the network to
+  coordinate consensus.
+
+Examples of _voting_ algorithms include PBFT, rBFT, Tendermint, and
+Raft.
+
+In _lottery_ consensus algorithms:
+
+- All nodes are authorized to make commits at any given time.
+- Nodes need only be aware of their peers on the network (not all nodes).
+- Adding and removing nodes is trivial; the network supports ""open-enrollment"".
+- There are no ""consensus-specific"" messages; consensus is an emergent property
+  of the fork-resolution logic.
+
+Examples of _lottery_ consensus algorithms include Proof-of-Work (PoW)
+and Proof-of-Elapsed-Time (PoET).
+
+## Overview of the Consensus Engine
+
+The Consensus Engine lives in a separate process and interacts with the
+Validator using protobuf messages.
+
+The Consensus Engine is responsible for:
+
+- Determining messages to send to peers
+- Sending commands to progress the blockchain
+- Reacting to internal interrupts and a limited set of external messages
+
+The Consensus Engine is not responsible for:
+
+- Validating the integrity of blocks, batches, transactions
+- Validating block, batch, transaction, or message signatures
+- Gossiping blocks, batches, or transactions
+- Block storage, block creation, and (direct) management of the chain head
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+## Requirements
+
+The following is a list of necessary requirements for the Sawtooth
+architecture to be able to support both classes of consensus algorithms:
+
+**R1 - Consensus-Specific Messaging**
+
+At a minimum, an efficient communication channel for sending consensus messages
+between network nodes is needed. This channel must support both broadcast and
+peer-to-peer messaging.
+
+**R2 - Separate Consensus Process**
+
+Implementing consensus algorithms is expensive. It must be possible to reuse
+existing implementations to the extent possible. For this reason, it must be
+possible to run consensus in a separate process.
+
+**R3 - Consensus-Driven Block Publishing and Chain Updates**
+
+In order to be efficient, the consensus algorithm must be responsible for
+determining when to publish blocks and when to update the chain head.
+
+For example, in voting algorithms, only one node is authorized to
+publish blocks at any given time and the authorized node often does not change
+until it is deemed faulty. In a stable environment, this could be an indefinite
+amount of time. If consensus is not responsible for determining when to publish
+and update the chain head, all but one nodes will be polling consensus
+for long periods of time.
+
+**R4 - Read Access to Settings**
+
+Consensus engines need to read settings to support on-chain consensus engine
+configuration using the Settings Transaction Family. PoET depends on Settings
+to configure critical values such as target wait time and enclave measurements
+and these values must be consistent across the network.
+
+**R5 - Read/Write Access to Global State**
+
+PoET depends on tracking authorized block publishers in global state and
+therefore requires read access to a PoET specific namespace.",128,2018-10-03 16:32:48,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/222369535,https://github.com/hyperledger/sawtooth-rfcs/pull/4#discussion_r222369535,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Reference to other programming languages, cargo, etc. seem rust specific.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-05 22:59:38,172358436,"@@ -0,0 +1,74 @@
+- Feature Name: (fill me in with a unique ident, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how it should impact the way they use Sawtooth. It should explain the impact as concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or migration guidance.
+- If applicable, describe the differences between teaching this to existing Sawtooth programmers and new Sawtooth programmers.
+
+For implementation-oriented RFCs (e.g. for compiler internals), this section should focus on how compiler contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+This is the technical portion of the RFC. Explain the design in sufficient detail that:
+
+- Its interaction with other features is clear.
+- It is reasonably clear how the feature would be implemented.
+- Corner cases are dissected by example.
+
+The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Why should we *not* do this?
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+- Why is this design the best in the space of possible designs?
+- What other designs have been considered and what is the rationale for not choosing them?
+- What is the impact of not doing this?
+
+# Prior art
+[prior-art]: #prior-art
+
+Discuss prior art, both the good and the bad, in relation to this proposal.
+A few examples of what this can include are:
+
+- For language, library, cargo, tools, and compiler proposals: Does this feature exists in other programming languages and what experience have their community had?",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172358436,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172358436,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,rust specific,9434205cd45399ea3cfca9868529c04d27156287,2018-03-05 22:59:53,172358494,"@@ -0,0 +1,74 @@
+- Feature Name: (fill me in with a unique ident, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how it should impact the way they use Sawtooth. It should explain the impact as concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or migration guidance.
+- If applicable, describe the differences between teaching this to existing Sawtooth programmers and new Sawtooth programmers.
+
+For implementation-oriented RFCs (e.g. for compiler internals), this section should focus on how compiler contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+This is the technical portion of the RFC. Explain the design in sufficient detail that:
+
+- Its interaction with other features is clear.
+- It is reasonably clear how the feature would be implemented.
+- Corner cases are dissected by example.
+
+The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Why should we *not* do this?
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+- Why is this design the best in the space of possible designs?
+- What other designs have been considered and what is the rationale for not choosing them?
+- What is the impact of not doing this?
+
+# Prior art
+[prior-art]: #prior-art
+
+Discuss prior art, both the good and the bad, in relation to this proposal.
+A few examples of what this can include are:
+
+- For language, library, cargo, tools, and compiler proposals: Does this feature exists in other programming languages and what experience have their community had?
+- For community proposals: Is this done by some other community and what were their experiences with it?
+- For other teams: What lessons can we learn from what other communities have done here?
+- Papers: Are there any published papers or great posts that discuss this? If you have some relevant papers to refer to, this can serve as a more detailed theoretical background.
+
+This section is intended to encourage you as an author to think about the lessons from other languages, provide readers of your RFC with a fuller picture.
+If there is no prior art, that is fine - your ideas are interesting to us whether they are brand new or if it is an adaptation from other languages.
+
+Note that while precedent set by other languages is some motivation, it does not on its own motivate an RFC.
+Please also take into consideration that rust sometimes intentionally diverges from common language features.",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172358494,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172358494,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,don't abbreviate ident,9434205cd45399ea3cfca9868529c04d27156287,2018-03-05 23:00:15,172358573,"@@ -0,0 +1,74 @@
+- Feature Name: (fill me in with a unique ident, my_awesome_feature)",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172358573,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172358573,jsmitchell
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Mentions language, left over from Rust?",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:26:30,172575539,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter the language and standard",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172575539,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172575539,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Same as above,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:26:34,172575575,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter the language and standard
+libraries, so that all stakeholders can be confident about the direction the
+language is evolving in.",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172575575,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172575575,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,When is the number assigned?,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:32:09,172577867,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter the language and standard
+libraries, so that all stakeholders can be confident about the direction the
+language is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).",108,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172577867,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172577867,agunde406
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Do we need subteams at all if we only have the one? Is this referencing a community infrastructure we aren't actually going to build out?,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:44:35,172582259,"@@ -63,12 +66,10 @@ the RFC process, it may be closed with a polite request to submit an RFC first.
 [Sub-team specific guidelines]: #sub-team-specific-guidelines
 
 For more details on when an RFC is required for the following areas, please see
-the Rust community's [sub-team] specific guidelines for:
+the Sawtooth community's [sub-team] specific guidelines for:
 
-  - [language changes](lang_changes.md),
-  - [library changes](libs_changes.md),
-  - [compiler changes](compiler_changes.md).
 
+  - [core changes](core_changes.md)",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172582259,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172582259,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Does this link to Rocket Chat somehow? If not, should we include that link?",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:45:40,172582597,"@@ -85,9 +86,8 @@ beforehand, to ascertain that the RFC may be desirable; having a consistent
 impact on the project requires concerted effort toward consensus-building.
 
 The most common preparations for writing and submitting an RFC include talking
-the idea over on #rust-internals, discussing the topic on our [developer discussion forum],
-and occasionally posting ""pre-RFCs"" on the developer forum. You may file issues
-on this repo for discussion, but these are not actively looked at by the teams.
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172582597,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172582597,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Is this the best way to check if other members of the team are working on a feature for Sawtooth? I personally don't check JIRA comments often (read: at all).,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:50:34,172584202,"@@ -219,25 +216,6 @@ cannot determine if someone else is already working on it, feel free to ask
 (e.g. by leaving a comment on the associated issue).",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172584202,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172584202,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"""in the language""",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:52:29,172584825,"@@ -16,13 +16,13 @@ Why are we doing this? What use cases does it support? What is the expected outc
 # Guide-level explanation
 [guide-level-explanation]: #guide-level-explanation
 
-Explain the proposal as if it was already included in the language and you were teaching it to another Rust programmer. That generally means:
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172584825,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172584825,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"My current assumption is prior to merging, at the end of the FCP.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:52:59,172584983,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter the language and standard
+libraries, so that all stakeholders can be confident about the direction the
+language is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).",108,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172584983,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172584983,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,The line references compilers.,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:53:30,172585165,"@@ -16,13 +16,13 @@ Why are we doing this? What use cases does it support? What is the expected outc
 # Guide-level explanation
 [guide-level-explanation]: #guide-level-explanation
 
-Explain the proposal as if it was already included in the language and you were teaching it to another Rust programmer. That generally means:
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:
 
 - Introducing new named concepts.
 - Explaining the feature largely in terms of examples.
-- Explaining how Rust programmers should *think* about the feature, and how it should impact the way they use Rust. It should explain the impact as concretely as possible.
+- Explaining how Sawtooth programmers should *think* about the feature, and how it should impact the way they use Sawtooth. It should explain the impact as concretely as possible.
 - If applicable, provide sample error messages, deprecation warnings, or migration guidance.
-- If applicable, describe the differences between teaching this to existing Rust programmers and new Rust programmers.
+- If applicable, describe the differences between teaching this to existing Sawtooth programmers and new Sawtooth programmers.
 
 For implementation-oriented RFCs (e.g. for compiler internals), this section should focus on how compiler contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172585165,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172585165,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"The _Prior art_ section below also heavily references ""other languages"".",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:54:39,172585574,"@@ -16,13 +16,13 @@ Why are we doing this? What use cases does it support? What is the expected outc
 # Guide-level explanation
 [guide-level-explanation]: #guide-level-explanation
 
-Explain the proposal as if it was already included in the language and you were teaching it to another Rust programmer. That generally means:
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:
 
 - Introducing new named concepts.
 - Explaining the feature largely in terms of examples.
-- Explaining how Rust programmers should *think* about the feature, and how it should impact the way they use Rust. It should explain the impact as concretely as possible.
+- Explaining how Sawtooth programmers should *think* about the feature, and how it should impact the way they use Sawtooth. It should explain the impact as concretely as possible.
 - If applicable, provide sample error messages, deprecation warnings, or migration guidance.
-- If applicable, describe the differences between teaching this to existing Rust programmers and new Rust programmers.
+- If applicable, describe the differences between teaching this to existing Sawtooth programmers and new Sawtooth programmers.
 
 For implementation-oriented RFCs (e.g. for compiler internals), this section should focus on how compiler contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172585574,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172585574,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,have their **communities** had,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 16:56:35,172586283,"@@ -55,16 +55,16 @@ Why should we *not* do this?
 Discuss prior art, both the good and the bad, in relation to this proposal.
 A few examples of what this can include are:
 
-- For language, library, cargo, tools, and compiler proposals: Does this feature exists in other programming languages and what experience have their community had?
+- For consensus, global state, transaction processors, and smart contracts implementation proposals: Does this feature exists in other distributed ledgers and what experience have their community had?",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172586283,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172586283,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Might be worth spelling it out, even though the Rust RFCS docs don't.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:01:54,172588163,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter the language and standard
+libraries, so that all stakeholders can be confident about the direction the
+language is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).",108,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172588163,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172588163,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Do we need subteams at all if we only have the one? Is this referencing a community infrastructure we aren't actually going to build out?,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:04:29,172588987,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)",73,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172588987,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172588987,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Does`#sawtooth` link to Rocket Chat somehow? If not, should we include that link?",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:06:15,172589559,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172589559,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172589559,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Is this the best way to check if other members of the team are working on a feature for Sawtooth? I personally don't check JIRA comments often (read: at all).,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:06:51,172589759,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).
+  - Fill in the RFC. Put care into the details: RFCs that do not present
+    convincing motivation, demonstrate understanding of the impact of the
+    design, or are disingenuous about the drawbacks or alternatives tend to be
+    poorly-received.
+  - Submit a pull request. As a pull request the RFC will receive design
+    feedback from the larger community, and the author should be prepared to
+    revise it in response.
+  - Build consensus and integrate feedback. RFCs that have broad support are
+    much more likely to make progress than those that don't receive any
+    comments. Feel free to reach out to the RFC assignee in particular to get
+    help identifying stakeholders and obstacles.
+  - The sub-team will discuss the RFC pull request, as much as possible in the
+    comment thread of the pull request itself. Offline discussion will be
+    summarized on the pull request comment thread.
+  - RFCs rarely go through this process unchanged, especially as alternatives
+    and drawbacks are shown. You can make edits, big and small, to the RFC to
+    clarify or change the design, but make changes as new commits to the pull
+    request, and leave a comment on the pull request explaining your changes.
+    Specifically, do not squash or rebase commits after they are visible on the
+    pull request.
+  - At some point, a member of the subteam will propose a ""motion for final
+    comment period"" (FCP), along with a *disposition* for the RFC (merge, close,
+    or postpone).
+    - This step is taken when enough of the tradeoffs have been discussed that
+    the subteam is in a position to make a decision. That does not require
+    consensus amongst all participants in the RFC thread (which is usually
+    impossible). However, the argument supporting the disposition on the RFC
+    needs to have already been clearly articulated, and there should not be a
+    strong consensus *against* that position outside of the subteam. Subteam
+    members use their best judgment in taking this step, and the FCP itself
+    ensures there is ample time and notification for stakeholders to push back
+    if it is made prematurely.
+    - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
+      a *summary comment* trying to lay out the current state of the discussion
+      and major trade-offs/points of disagreement.
+    - Before actually entering FCP, *all* members of the subteam must sign off;
+    this is often the point at which many subteam members first review the RFC
+    in full depth.
+  - The FCP lasts ten calendar days, so that it is open for at least 5 business
+    days. It is also advertised widely, e.g. in [Sawtooth Mailing
+    List](https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl). This
+    way all stakeholders have a chance to lodge any final objections before
+    a decision is reached.
+  - In most cases, the FCP period is quiet, and the RFC is either merged or
+    closed. However, sometimes substantial new arguments or ideas are raised,
+    the FCP is canceled, and the RFC goes back into development mode.
+
+## The RFC life-cycle
+[The RFC life-cycle]: #the-rfc-life-cycle
+
+Once an RFC becomes ""active"" then authors may implement it and submit the
+change as a pull request to the corresponding Sawtooth repo. Being ""active"" is not a rubber
+stamp, and in particular still does not mean the change will ultimately be
+merged; it does mean that in principle all the major stakeholders have agreed
+to the change and are amenable to merging it.
+
+Furthermore, the fact that a given RFC has been accepted and is ""active""
+implies nothing about what priority is assigned to its implementation, nor does
+it imply anything about whether a Sawtooth developer has been assigned the task of
+implementing the feature. While it is not *necessary* that the author of the
+RFC also write the implementation, it is by far the most effective way to see
+an RFC through to completion: authors should not expect that other project
+developers will take on responsibility for implementing their accepted feature.
+
+Modifications to ""active"" RFCs can be done in follow-up pull requests. We
+strive to write each RFC in a manner that it will reflect the final design of
+the feature; but the nature of the process means that we cannot expect every
+merged RFC to actually reflect what the end result will be at the time of the
+next major release.
+
+In general, once accepted, RFCs should not be substantially changed. Only very
+minor changes should be submitted as amendments. More substantial changes
+should be new RFCs, with a note added to the original RFC. Exactly what counts
+as a ""very minor change"" is up to the sub-team to decide; check
+[Sub-team specific guidelines] for more details.
+
+
+## Reviewing RFCs
+[Reviewing RFCs]: #reviewing-rfcs
+
+While the RFC pull request is up, the sub-team may schedule meetings with the
+author and/or relevant stakeholders to discuss the issues in greater detail,
+and in some cases the topic may be discussed at a sub-team meeting. In either
+case a summary from the meeting will be posted back to the RFC pull request.
+
+A sub-team makes final decisions about RFCs after the benefits and drawbacks
+are well understood. These decisions can be made at any time, but the sub-team
+will regularly issue decisions. When a decision is made, the RFC pull request
+will either be merged or closed. In either case, if the reasoning is not clear
+from the discussion in thread, the sub-team will add a comment describing the
+rationale for the decision.
+
+
+## Implementing an RFC
+[Implementing an RFC]: #implementing-an-rfc
+
+Some accepted RFCs represent vital features that need to be implemented right
+away. Other accepted RFCs can represent features that can wait until some
+arbitrary developer feels like doing the work. Every accepted RFC has an
+associated issue tracking its implementation in the Sawtooth JIRA issue tracker; thus that
+associated issue can be assigned a priority via the triage process that the
+team uses for all issues related to Sawtooth.
+
+The author of an RFC is not obligated to implement it. Of course, the RFC
+author (like any other developer) is welcome to post an implementation for
+review after the RFC has been accepted.
+
+If you are interested in working on the implementation for an ""active"" RFC, but
+cannot determine if someone else is already working on it, feel free to ask
+(e.g. by leaving a comment on the associated issue).",218,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172589759,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172589759,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Yes, see other comment (sub-teams will be specified in an RFC).",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:07:21,172589906,"@@ -63,12 +66,10 @@ the RFC process, it may be closed with a polite request to submit an RFC first.
 [Sub-team specific guidelines]: #sub-team-specific-guidelines
 
 For more details on when an RFC is required for the following areas, please see
-the Rust community's [sub-team] specific guidelines for:
+the Sawtooth community's [sub-team] specific guidelines for:
 
-  - [language changes](lang_changes.md),
-  - [library changes](libs_changes.md),
-  - [compiler changes](compiler_changes.md).
 
+  - [core changes](core_changes.md)",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172589906,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172589906,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,have their **communities** had,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:08:11,172590143,"@@ -0,0 +1,101 @@
+- Feature Name: (fill me in with a unique identifier, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected
+outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in Sawtooth and you were
+teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how
+  it should impact the way they use Sawtooth. It should explain the impact as
+  concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or
+  migration guidance.
+- If applicable, describe the differences between teaching this to existing
+  Sawtooth programmers and new Sawtooth programmers.
+
+For implementation-oriented RFCs (e.g. for validator internals), this section
+should focus on how contributors should think about the change, and give
+examples of its concrete impact. For policy RFCs, this section should provide
+an example-driven introduction to the policy, and explain its impact in
+concrete terms.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+This is the technical portion of the RFC. Explain the design in sufficient
+detail that:
+
+- Its interaction with other features is clear.
+- It is reasonably clear how the feature would be implemented.
+- Corner cases are dissected by example.
+
+The section should return to the examples given in the previous section, and
+explain more fully how the detailed proposal makes those examples work.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Why should we *not* do this?
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+- Why is this design the best in the space of possible designs?
+- What other designs have been considered and what is the rationale for not
+  choosing them?
+- What is the impact of not doing this?
+
+# Prior art
+[prior-art]: #prior-art
+
+Discuss prior art, both the good and the bad, in relation to this proposal.
+A few examples of what this can include are:
+
+- For consensus, global state, transaction processors, and smart contracts
+  implementation proposals: Does this feature exists in other distributed
+  ledgers and what experience have their community had?",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172590143,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172590143,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"In theory... yes. In practice, only to the extent we use JIRA.  I think for the purposes of this, it is reasonable to use JIRA as the point of assignment for implementation.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:10:28,172590848,"@@ -219,25 +216,6 @@ cannot determine if someone else is already working on it, feel free to ask
 (e.g. by leaving a comment on the associated issue).",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172590848,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172590848,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Addressed in a subsequent commit.,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:11:24,172591133,"@@ -16,13 +16,13 @@ Why are we doing this? What use cases does it support? What is the expected outc
 # Guide-level explanation
 [guide-level-explanation]: #guide-level-explanation
 
-Explain the proposal as if it was already included in the language and you were teaching it to another Rust programmer. That generally means:
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172591133,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172591133,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Addressed in a subsequent commit.,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:11:45,172591261,"@@ -16,13 +16,13 @@ Why are we doing this? What use cases does it support? What is the expected outc
 # Guide-level explanation
 [guide-level-explanation]: #guide-level-explanation
 
-Explain the proposal as if it was already included in the language and you were teaching it to another Rust programmer. That generally means:
+Explain the proposal as if it was already included in the language and you were teaching it to another Sawtooth programmer. That generally means:
 
 - Introducing new named concepts.
 - Explaining the feature largely in terms of examples.
-- Explaining how Rust programmers should *think* about the feature, and how it should impact the way they use Rust. It should explain the impact as concretely as possible.
+- Explaining how Sawtooth programmers should *think* about the feature, and how it should impact the way they use Sawtooth. It should explain the impact as concretely as possible.
 - If applicable, provide sample error messages, deprecation warnings, or migration guidance.
-- If applicable, describe the differences between teaching this to existing Rust programmers and new Rust programmers.
+- If applicable, describe the differences between teaching this to existing Sawtooth programmers and new Sawtooth programmers.
 
 For implementation-oriented RFCs (e.g. for compiler internals), this section should focus on how compiler contributors should think about the change, and give examples of its concrete impact. For policy RFCs, this section should provide an example-driven introduction to the policy, and explain its impact in concrete terms.",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172591261,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172591261,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Yes, see other comment (sub-teams will be specified in an RFC).",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:16:14,172592589,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)",73,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172592589,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172592589,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"In theory... yes. In practice, only to the extent we use JIRA. I think for the purposes of this, it is reasonable to use JIRA as the point of assignment for implementation.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:16:48,172592766,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).
+  - Fill in the RFC. Put care into the details: RFCs that do not present
+    convincing motivation, demonstrate understanding of the impact of the
+    design, or are disingenuous about the drawbacks or alternatives tend to be
+    poorly-received.
+  - Submit a pull request. As a pull request the RFC will receive design
+    feedback from the larger community, and the author should be prepared to
+    revise it in response.
+  - Build consensus and integrate feedback. RFCs that have broad support are
+    much more likely to make progress than those that don't receive any
+    comments. Feel free to reach out to the RFC assignee in particular to get
+    help identifying stakeholders and obstacles.
+  - The sub-team will discuss the RFC pull request, as much as possible in the
+    comment thread of the pull request itself. Offline discussion will be
+    summarized on the pull request comment thread.
+  - RFCs rarely go through this process unchanged, especially as alternatives
+    and drawbacks are shown. You can make edits, big and small, to the RFC to
+    clarify or change the design, but make changes as new commits to the pull
+    request, and leave a comment on the pull request explaining your changes.
+    Specifically, do not squash or rebase commits after they are visible on the
+    pull request.
+  - At some point, a member of the subteam will propose a ""motion for final
+    comment period"" (FCP), along with a *disposition* for the RFC (merge, close,
+    or postpone).
+    - This step is taken when enough of the tradeoffs have been discussed that
+    the subteam is in a position to make a decision. That does not require
+    consensus amongst all participants in the RFC thread (which is usually
+    impossible). However, the argument supporting the disposition on the RFC
+    needs to have already been clearly articulated, and there should not be a
+    strong consensus *against* that position outside of the subteam. Subteam
+    members use their best judgment in taking this step, and the FCP itself
+    ensures there is ample time and notification for stakeholders to push back
+    if it is made prematurely.
+    - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
+      a *summary comment* trying to lay out the current state of the discussion
+      and major trade-offs/points of disagreement.
+    - Before actually entering FCP, *all* members of the subteam must sign off;
+    this is often the point at which many subteam members first review the RFC
+    in full depth.
+  - The FCP lasts ten calendar days, so that it is open for at least 5 business
+    days. It is also advertised widely, e.g. in [Sawtooth Mailing
+    List](https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl). This
+    way all stakeholders have a chance to lodge any final objections before
+    a decision is reached.
+  - In most cases, the FCP period is quiet, and the RFC is either merged or
+    closed. However, sometimes substantial new arguments or ideas are raised,
+    the FCP is canceled, and the RFC goes back into development mode.
+
+## The RFC life-cycle
+[The RFC life-cycle]: #the-rfc-life-cycle
+
+Once an RFC becomes ""active"" then authors may implement it and submit the
+change as a pull request to the corresponding Sawtooth repo. Being ""active"" is not a rubber
+stamp, and in particular still does not mean the change will ultimately be
+merged; it does mean that in principle all the major stakeholders have agreed
+to the change and are amenable to merging it.
+
+Furthermore, the fact that a given RFC has been accepted and is ""active""
+implies nothing about what priority is assigned to its implementation, nor does
+it imply anything about whether a Sawtooth developer has been assigned the task of
+implementing the feature. While it is not *necessary* that the author of the
+RFC also write the implementation, it is by far the most effective way to see
+an RFC through to completion: authors should not expect that other project
+developers will take on responsibility for implementing their accepted feature.
+
+Modifications to ""active"" RFCs can be done in follow-up pull requests. We
+strive to write each RFC in a manner that it will reflect the final design of
+the feature; but the nature of the process means that we cannot expect every
+merged RFC to actually reflect what the end result will be at the time of the
+next major release.
+
+In general, once accepted, RFCs should not be substantially changed. Only very
+minor changes should be submitted as amendments. More substantial changes
+should be new RFCs, with a note added to the original RFC. Exactly what counts
+as a ""very minor change"" is up to the sub-team to decide; check
+[Sub-team specific guidelines] for more details.
+
+
+## Reviewing RFCs
+[Reviewing RFCs]: #reviewing-rfcs
+
+While the RFC pull request is up, the sub-team may schedule meetings with the
+author and/or relevant stakeholders to discuss the issues in greater detail,
+and in some cases the topic may be discussed at a sub-team meeting. In either
+case a summary from the meeting will be posted back to the RFC pull request.
+
+A sub-team makes final decisions about RFCs after the benefits and drawbacks
+are well understood. These decisions can be made at any time, but the sub-team
+will regularly issue decisions. When a decision is made, the RFC pull request
+will either be merged or closed. In either case, if the reasoning is not clear
+from the discussion in thread, the sub-team will add a comment describing the
+rationale for the decision.
+
+
+## Implementing an RFC
+[Implementing an RFC]: #implementing-an-rfc
+
+Some accepted RFCs represent vital features that need to be implemented right
+away. Other accepted RFCs can represent features that can wait until some
+arbitrary developer feels like doing the work. Every accepted RFC has an
+associated issue tracking its implementation in the Sawtooth JIRA issue tracker; thus that
+associated issue can be assigned a priority via the triage process that the
+team uses for all issues related to Sawtooth.
+
+The author of an RFC is not obligated to implement it. Of course, the RFC
+author (like any other developer) is welcome to post an implementation for
+review after the RFC has been accepted.
+
+If you are interested in working on the implementation for an ""active"" RFC, but
+cannot determine if someone else is already working on it, feel free to ask
+(e.g. by leaving a comment on the associated issue).",218,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172592766,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172592766,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Fixed,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:20:54,172594087,"@@ -0,0 +1,101 @@
+- Feature Name: (fill me in with a unique identifier, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected
+outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in Sawtooth and you were
+teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how
+  it should impact the way they use Sawtooth. It should explain the impact as
+  concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or
+  migration guidance.
+- If applicable, describe the differences between teaching this to existing
+  Sawtooth programmers and new Sawtooth programmers.
+
+For implementation-oriented RFCs (e.g. for validator internals), this section
+should focus on how contributors should think about the change, and give
+examples of its concrete impact. For policy RFCs, this section should provide
+an example-driven introduction to the policy, and explain its impact in
+concrete terms.
+
+# Reference-level explanation
+[reference-level-explanation]: #reference-level-explanation
+
+This is the technical portion of the RFC. Explain the design in sufficient
+detail that:
+
+- Its interaction with other features is clear.
+- It is reasonably clear how the feature would be implemented.
+- Corner cases are dissected by example.
+
+The section should return to the examples given in the previous section, and
+explain more fully how the detailed proposal makes those examples work.
+
+# Drawbacks
+[drawbacks]: #drawbacks
+
+Why should we *not* do this?
+
+# Rationale and alternatives
+[alternatives]: #alternatives
+
+- Why is this design the best in the space of possible designs?
+- What other designs have been considered and what is the rationale for not
+  choosing them?
+- What is the impact of not doing this?
+
+# Prior art
+[prior-art]: #prior-art
+
+Discuss prior art, both the good and the bad, in relation to this proposal.
+A few examples of what this can include are:
+
+- For consensus, global state, transaction processors, and smart contracts
+  implementation proposals: Does this feature exists in other distributed
+  ledgers and what experience have their community had?",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172594087,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172594087,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,I would put the link to Rocket Chat here in addition to mentioning JIRA then.,9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 17:54:14,172603973,"@@ -0,0 +1,242 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on #sawtooth and proposing ideas to the Hyperledger Sawtooth
+mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).
+  - Fill in the RFC. Put care into the details: RFCs that do not present
+    convincing motivation, demonstrate understanding of the impact of the
+    design, or are disingenuous about the drawbacks or alternatives tend to be
+    poorly-received.
+  - Submit a pull request. As a pull request the RFC will receive design
+    feedback from the larger community, and the author should be prepared to
+    revise it in response.
+  - Build consensus and integrate feedback. RFCs that have broad support are
+    much more likely to make progress than those that don't receive any
+    comments. Feel free to reach out to the RFC assignee in particular to get
+    help identifying stakeholders and obstacles.
+  - The sub-team will discuss the RFC pull request, as much as possible in the
+    comment thread of the pull request itself. Offline discussion will be
+    summarized on the pull request comment thread.
+  - RFCs rarely go through this process unchanged, especially as alternatives
+    and drawbacks are shown. You can make edits, big and small, to the RFC to
+    clarify or change the design, but make changes as new commits to the pull
+    request, and leave a comment on the pull request explaining your changes.
+    Specifically, do not squash or rebase commits after they are visible on the
+    pull request.
+  - At some point, a member of the subteam will propose a ""motion for final
+    comment period"" (FCP), along with a *disposition* for the RFC (merge, close,
+    or postpone).
+    - This step is taken when enough of the tradeoffs have been discussed that
+    the subteam is in a position to make a decision. That does not require
+    consensus amongst all participants in the RFC thread (which is usually
+    impossible). However, the argument supporting the disposition on the RFC
+    needs to have already been clearly articulated, and there should not be a
+    strong consensus *against* that position outside of the subteam. Subteam
+    members use their best judgment in taking this step, and the FCP itself
+    ensures there is ample time and notification for stakeholders to push back
+    if it is made prematurely.
+    - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
+      a *summary comment* trying to lay out the current state of the discussion
+      and major trade-offs/points of disagreement.
+    - Before actually entering FCP, *all* members of the subteam must sign off;
+    this is often the point at which many subteam members first review the RFC
+    in full depth.
+  - The FCP lasts ten calendar days, so that it is open for at least 5 business
+    days. It is also advertised widely, e.g. in [Sawtooth Mailing
+    List](https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl). This
+    way all stakeholders have a chance to lodge any final objections before
+    a decision is reached.
+  - In most cases, the FCP period is quiet, and the RFC is either merged or
+    closed. However, sometimes substantial new arguments or ideas are raised,
+    the FCP is canceled, and the RFC goes back into development mode.
+
+## The RFC life-cycle
+[The RFC life-cycle]: #the-rfc-life-cycle
+
+Once an RFC becomes ""active"" then authors may implement it and submit the
+change as a pull request to the corresponding Sawtooth repo. Being ""active"" is not a rubber
+stamp, and in particular still does not mean the change will ultimately be
+merged; it does mean that in principle all the major stakeholders have agreed
+to the change and are amenable to merging it.
+
+Furthermore, the fact that a given RFC has been accepted and is ""active""
+implies nothing about what priority is assigned to its implementation, nor does
+it imply anything about whether a Sawtooth developer has been assigned the task of
+implementing the feature. While it is not *necessary* that the author of the
+RFC also write the implementation, it is by far the most effective way to see
+an RFC through to completion: authors should not expect that other project
+developers will take on responsibility for implementing their accepted feature.
+
+Modifications to ""active"" RFCs can be done in follow-up pull requests. We
+strive to write each RFC in a manner that it will reflect the final design of
+the feature; but the nature of the process means that we cannot expect every
+merged RFC to actually reflect what the end result will be at the time of the
+next major release.
+
+In general, once accepted, RFCs should not be substantially changed. Only very
+minor changes should be submitted as amendments. More substantial changes
+should be new RFCs, with a note added to the original RFC. Exactly what counts
+as a ""very minor change"" is up to the sub-team to decide; check
+[Sub-team specific guidelines] for more details.
+
+
+## Reviewing RFCs
+[Reviewing RFCs]: #reviewing-rfcs
+
+While the RFC pull request is up, the sub-team may schedule meetings with the
+author and/or relevant stakeholders to discuss the issues in greater detail,
+and in some cases the topic may be discussed at a sub-team meeting. In either
+case a summary from the meeting will be posted back to the RFC pull request.
+
+A sub-team makes final decisions about RFCs after the benefits and drawbacks
+are well understood. These decisions can be made at any time, but the sub-team
+will regularly issue decisions. When a decision is made, the RFC pull request
+will either be merged or closed. In either case, if the reasoning is not clear
+from the discussion in thread, the sub-team will add a comment describing the
+rationale for the decision.
+
+
+## Implementing an RFC
+[Implementing an RFC]: #implementing-an-rfc
+
+Some accepted RFCs represent vital features that need to be implemented right
+away. Other accepted RFCs can represent features that can wait until some
+arbitrary developer feels like doing the work. Every accepted RFC has an
+associated issue tracking its implementation in the Sawtooth JIRA issue tracker; thus that
+associated issue can be assigned a priority via the triage process that the
+team uses for all issues related to Sawtooth.
+
+The author of an RFC is not obligated to implement it. Of course, the RFC
+author (like any other developer) is welcome to post an implementation for
+review after the RFC has been accepted.
+
+If you are interested in working on the implementation for an ""active"" RFC, but
+cannot determine if someone else is already working on it, feel free to ask
+(e.g. by leaving a comment on the associated issue).",218,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172603973,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172603973,delventhalz
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"You changed ""feature"" to ""change"" in this paragraph, but not in the following content. See lines 166 and 203 for examples.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 19:11:16,172627211,"@@ -158,14 +155,14 @@ merged into the RFC repository as a markdown file. At that point the RFC is
 [The RFC life-cycle]: #the-rfc-life-cycle
 
 Once an RFC becomes ""active"" then authors may implement it and submit the
-feature as a pull request to the Rust repo. Being ""active"" is not a rubber
-stamp, and in particular still does not mean the feature will ultimately be
+change as a pull request to the corresponding Sawtooth repo. Being ""active"" is not a rubber",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172627211,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172627211,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Links are inconsistent.  Some are hidden (the first and this one) and some are shown in the text (the second link, above). 

Also, I suggest using the full name for this mailing list: ""[Hyperledger Sawtooth mailing list]""

P.S. Capitalizing ""mailing list"" makes me :cry: ",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 19:13:14,172627753,"@@ -141,13 +138,13 @@ merged into the RFC repository as a markdown file. At that point the RFC is
     if it is made prematurely.
     - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
       a *summary comment* trying to lay out the current state of the discussion
-      and major tradeoffs/points of disagreement.
+      and major trade-offs/points of disagreement.
     - Before actually entering FCP, *all* members of the subteam must sign off;
     this is often the point at which many subteam members first review the RFC
     in full depth.
   - The FCP lasts ten calendar days, so that it is open for at least 5 business
     days. It is also advertised widely,
-    e.g. in [This Week in Rust](https://this-week-in-rust.org/). This way all
+    e.g. in [Sawtooth Mailing List](hyperledger-stl@lists.hyperledger.org). This way all",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172627753,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172627753,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Does ""core changes"" mean ""Changes to sawtooth-core""?  If so, I would change this to ""sawtooth-core changes"". (The term ""core"" is vague.)",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 19:28:27,172632311,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)",73,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172632311,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172632311,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Echoing Zac's comment (yes, I saw the reply).  It would be helpful to list the other Sawtooth components asap, because the document is confusing (especially the sub-team stuff) if there's only one thing listed here.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 19:30:02,172632792,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)",73,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172632792,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172632792,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Links are inconsistent.  Most, like these, are ""hidden"", but the one for the Hyperledger Sawtooth mailing list appears in formatted contents.  In general, I prefer showing the links explicitly (along with making them active links).  But mostly, I just like consistency.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 19:36:33,172634868,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on [#sawtooth](https://chat.hyperledger.org/channel/sawtooth) and",90,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172634868,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172634868,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"It means ""Sawtooth Core"" abstractly, or maybe components managed by the core sub-team. It is not necessarily 1:1 with the repository.  This will be flushed out when we define sub-teams.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 20:33:44,172650657,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)",73,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172650657,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172650657,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"I don't want to churn on this too much, because I think we should actually link to a community page for all of these things instead of having them directly in this repository.  I don't want to link to the core docs for this though, but rather a page we maintain in a sawtooth-website repository in the near future (which helps manage sawtooth.hyperledger.org content).",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 20:41:28,172652739,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on [#sawtooth](https://chat.hyperledger.org/channel/sawtooth) and",90,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172652739,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172652739,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"This has been changed, the remaining 'feature' is intentional.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 20:42:43,172653076,"@@ -158,14 +155,14 @@ merged into the RFC repository as a markdown file. At that point the RFC is
 [The RFC life-cycle]: #the-rfc-life-cycle
 
 Once an RFC becomes ""active"" then authors may implement it and submit the
-feature as a pull request to the Rust repo. Being ""active"" is not a rubber
-stamp, and in particular still does not mean the feature will ultimately be
+change as a pull request to the corresponding Sawtooth repo. Being ""active"" is not a rubber",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172653076,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172653076,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"My comment was simply on how links appear when you view the readme in a browser.  Sometimes a link looks like this:

>  #sawtooth

And one link looks like this: 

> Hyperledger Sawtooth mailing list (https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl)

My comment is only about the inconsistency of mostly ""hiding"" the URL but showing it in one link.
",9434205cd45399ea3cfca9868529c04d27156287,2018-03-06 20:48:17,172654653,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for new features to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected features, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on [#sawtooth](https://chat.hyperledger.org/channel/sawtooth) and",90,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172654653,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172654653,chenette
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,what constitutes sign-off?,9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 08:22:36,172767456,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected changes, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on [#sawtooth](https://chat.hyperledger.org/channel/sawtooth) and
+proposing ideas to the Hyperledger Sawtooth mailing list
+(https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).
+  - Fill in the RFC. Put care into the details: RFCs that do not present
+    convincing motivation, demonstrate understanding of the impact of the
+    design, or are disingenuous about the drawbacks or alternatives tend to be
+    poorly-received.
+  - Submit a pull request. As a pull request the RFC will receive design
+    feedback from the larger community, and the author should be prepared to
+    revise it in response.
+  - Build consensus and integrate feedback. RFCs that have broad support are
+    much more likely to make progress than those that don't receive any
+    comments. Feel free to reach out to the RFC assignee in particular to get
+    help identifying stakeholders and obstacles.
+  - The sub-team will discuss the RFC pull request, as much as possible in the
+    comment thread of the pull request itself. Offline discussion will be
+    summarized on the pull request comment thread.
+  - RFCs rarely go through this process unchanged, especially as alternatives
+    and drawbacks are shown. You can make edits, big and small, to the RFC to
+    clarify or change the design, but make changes as new commits to the pull
+    request, and leave a comment on the pull request explaining your changes.
+    Specifically, do not squash or rebase commits after they are visible on the
+    pull request.
+  - At some point, a member of the subteam will propose a ""motion for final
+    comment period"" (FCP), along with a *disposition* for the RFC (merge, close,
+    or postpone).
+    - This step is taken when enough of the tradeoffs have been discussed that
+    the subteam is in a position to make a decision. That does not require
+    consensus amongst all participants in the RFC thread (which is usually
+    impossible). However, the argument supporting the disposition on the RFC
+    needs to have already been clearly articulated, and there should not be a
+    strong consensus *against* that position outside of the subteam. Subteam
+    members use their best judgment in taking this step, and the FCP itself
+    ensures there is ample time and notification for stakeholders to push back
+    if it is made prematurely.
+    - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
+      a *summary comment* trying to lay out the current state of the discussion
+      and major trade-offs/points of disagreement.
+    - Before actually entering FCP, *all* members of the subteam must sign off;",144,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172767456,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172767456,dcmiddle
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"typo, should be `0000-template.md`",9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 15:16:40,172874774,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172874774,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172874774,aludvik
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Probably creating a checklist of sign-offs within the PR thread. After we go through the process a couple times, we will probably have opinions on what works and doesn't work.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 17:37:29,172923744,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+
+Some changes do not require an RFC:
+
+  - Rephrasing, reorganizing, refactoring, or otherwise ""changing shape does
+    not change meaning"".
+  - Additions that strictly improve objective, numerical quality criteria
+    (warning removal, speedup, better platform coverage, more parallelism, trap
+    more errors, etc.)
+
+If you submit a pull request to implement a new feature without going through
+the RFC process, it may be closed with a polite request to submit an RFC first.
+
+
+### Sub-team specific guidelines
+[Sub-team specific guidelines]: #sub-team-specific-guidelines
+
+For more details on when an RFC is required for the following areas, please see
+the Sawtooth community's [sub-team] specific guidelines for:
+
+
+  - [core changes](core_changes.md)
+
+## Before creating an RFC
+[Before creating an RFC]: #before-creating-an-rfc
+
+A hastily-proposed RFC can hurt its chances of acceptance. Low quality
+proposals, proposals for previously-rejected changes, or those that don't fit
+into the near-term roadmap, may be quickly rejected, which can be demotivating
+for the unprepared contributor. Laying some groundwork ahead of the RFC can
+make the process smoother.
+
+Although there is no single way to prepare for submitting an RFC, it is
+generally a good idea to pursue feedback from other project developers
+beforehand, to ascertain that the RFC may be desirable; having a consistent
+impact on the project requires concerted effort toward consensus-building.
+
+The most common preparations for writing and submitting an RFC include talking
+the idea over on [#sawtooth](https://chat.hyperledger.org/channel/sawtooth) and
+proposing ideas to the Hyperledger Sawtooth mailing list
+(https://lists.hyperledger.org/mailman/listinfo/hyperledger-stl).
+
+As a rule of thumb, receiving encouraging feedback from long-standing project
+developers, and particularly members of the relevant [sub-team] is a good
+indication that the RFC is worth pursuing.
+
+
+## What the process is
+[What the process is]: #what-the-process-is
+
+In short, to get a major feature added to Sawtooth, one must first get the RFC
+merged into the RFC repository as a markdown file. At that point the RFC is
+""active"" and may be implemented with the goal of eventual inclusion into Sawtooth.
+
+  - Fork the RFC repo [RFC repository]
+  - Copy `0000-template.md` to `text/0000-my-feature.md` (where ""my-feature"" is
+    descriptive. don't assign an RFC number yet).
+  - Fill in the RFC. Put care into the details: RFCs that do not present
+    convincing motivation, demonstrate understanding of the impact of the
+    design, or are disingenuous about the drawbacks or alternatives tend to be
+    poorly-received.
+  - Submit a pull request. As a pull request the RFC will receive design
+    feedback from the larger community, and the author should be prepared to
+    revise it in response.
+  - Build consensus and integrate feedback. RFCs that have broad support are
+    much more likely to make progress than those that don't receive any
+    comments. Feel free to reach out to the RFC assignee in particular to get
+    help identifying stakeholders and obstacles.
+  - The sub-team will discuss the RFC pull request, as much as possible in the
+    comment thread of the pull request itself. Offline discussion will be
+    summarized on the pull request comment thread.
+  - RFCs rarely go through this process unchanged, especially as alternatives
+    and drawbacks are shown. You can make edits, big and small, to the RFC to
+    clarify or change the design, but make changes as new commits to the pull
+    request, and leave a comment on the pull request explaining your changes.
+    Specifically, do not squash or rebase commits after they are visible on the
+    pull request.
+  - At some point, a member of the subteam will propose a ""motion for final
+    comment period"" (FCP), along with a *disposition* for the RFC (merge, close,
+    or postpone).
+    - This step is taken when enough of the tradeoffs have been discussed that
+    the subteam is in a position to make a decision. That does not require
+    consensus amongst all participants in the RFC thread (which is usually
+    impossible). However, the argument supporting the disposition on the RFC
+    needs to have already been clearly articulated, and there should not be a
+    strong consensus *against* that position outside of the subteam. Subteam
+    members use their best judgment in taking this step, and the FCP itself
+    ensures there is ample time and notification for stakeholders to push back
+    if it is made prematurely.
+    - For RFCs with lengthy discussion, the motion to FCP is usually preceded by
+      a *summary comment* trying to lay out the current state of the discussion
+      and major trade-offs/points of disagreement.
+    - Before actually entering FCP, *all* members of the subteam must sign off;",144,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172923744,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172923744,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Fixed,9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 17:39:04,172924331,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+000-template.md were initially forked from [Rust",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172924331,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172924331,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Consider adding:
- Changes that affect the security of communications or administration",9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 18:47:09,172945320,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+0000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172945320,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172945320,TomBarnes
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,"Consider adding:
- If applicable, describe any changes that may affect the security of communications or administration.",9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 18:48:08,172945657,"@@ -0,0 +1,101 @@
+- Feature Name: (fill me in with a unique identifier, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected
+outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in Sawtooth and you were
+teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how
+  it should impact the way they use Sawtooth. It should explain the impact as
+  concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or
+  migration guidance.
+- If applicable, describe the differences between teaching this to existing
+  Sawtooth programmers and new Sawtooth programmers.
+",,2018-03-07 23:18:33,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/172945657,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r172945657,TomBarnes
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Added,9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 23:18:40,173017100,"@@ -0,0 +1,243 @@
+# Sawtooth RFCs
+[Sawtooth RFCs]: #sawtooth-rfcs
+
+Many changes, including bug fixes and documentation improvements can be
+implemented and reviewed via the normal GitHub pull request workflow.
+
+Some changes though are ""substantial"", and we ask that these be put through a
+bit of a design process and produce a consensus among the Sawtooth community and
+the [sub-team]s.
+
+The ""RFC"" (request for comments) process is intended to provide a consistent
+and controlled path for major changes to enter Sawtooth Core and other official
+project components, so that all stakeholders can be confident about the
+direction Sawtooth is evolving in.
+
+This process is intended to be substantially similar to the Rust RFCs process,
+customized as necessary for use with Sawtooth. The README.md and
+0000-template.md were initially forked from [Rust
+RFCs](https://github.com/rust-lang/rfcs).
+
+
+## Table of Contents
+[Table of Contents]: #table-of-contents
+
+  - [Opening](#sawtooth-rfcs)
+  - [Table of Contents]
+  - [When you need to follow this process]
+  - [Before creating an RFC]
+  - [What the process is]
+  - [The RFC life-cycle]
+  - [Reviewing RFCs]
+  - [Implementing an RFC]
+  - [Help this is all too informal!]
+  - [License]
+
+
+## When you need to follow this process
+[When you need to follow this process]: #when-you-need-to-follow-this-process
+
+You need to follow this process if you intend to make ""substantial"" changes to
+Sawtooth or any of its sub-components including but not limited to Sawtooth
+Core, Sawtooth Supply Chain, Sawtooth Seth, the various Sawtooth SDKs, or the
+RFC process itself. What constitutes a ""substantial"" change is evolving based
+on community norms and varies depending
+on what part of the ecosystem you are proposing to change, but may include the
+following.
+
+  - Architectural changes
+  - Substantial changes to component interfaces
+  - New core features
+  - Backward incompatible changes
+",,2018-03-07 23:18:40,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/173017100,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r173017100,vaporos
https://github.com/hyperledger/sawtooth-rfcs/pull/3,https://github.com/hyperledger/sawtooth-rfcs/pull/3,Added,9434205cd45399ea3cfca9868529c04d27156287,2018-03-07 23:18:47,173017117,"@@ -0,0 +1,101 @@
+- Feature Name: (fill me in with a unique identifier, my_awesome_feature)
+- Start Date: (fill me in with today's date, YYYY-MM-DD)
+- RFC PR: (leave this empty)
+- Sawtooth Issue: (leave this empty)
+
+# Summary
+[summary]: #summary
+
+One paragraph explanation of the feature.
+
+# Motivation
+[motivation]: #motivation
+
+Why are we doing this? What use cases does it support? What is the expected
+outcome?
+
+# Guide-level explanation
+[guide-level-explanation]: #guide-level-explanation
+
+Explain the proposal as if it was already included in Sawtooth and you were
+teaching it to another Sawtooth programmer. That generally means:
+
+- Introducing new named concepts.
+- Explaining the feature largely in terms of examples.
+- Explaining how Sawtooth programmers should *think* about the feature, and how
+  it should impact the way they use Sawtooth. It should explain the impact as
+  concretely as possible.
+- If applicable, provide sample error messages, deprecation warnings, or
+  migration guidance.
+- If applicable, describe the differences between teaching this to existing
+  Sawtooth programmers and new Sawtooth programmers.
+",,2018-03-07 23:18:47,https://api.github.com/repos/hyperledger/sawtooth-rfcs/pulls/comments/173017117,https://github.com/hyperledger/sawtooth-rfcs/pull/3#discussion_r173017117,vaporos
